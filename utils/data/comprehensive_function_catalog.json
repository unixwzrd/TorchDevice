{
  "CORE_TORCH": [
    {
      "function": "torch.align_tensors",
      "signature": "torch.align_tensors(*tensors)",
      "doc": "",
      "arguments": [
        "tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.are_deterministic_algorithms_enabled",
      "signature": "torch.are_deterministic_algorithms_enabled() -> bool",
      "doc": "Returns True if the global deterministic flag is turned on. Refer to\n    :func:`torch.use_deterministic_algorithms` documentation for more details.\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns True if the global deterministic flag is turned on. Refer to\n    :func:`torch.use_deterministic_algorithms` documentation for more details.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.atleast_1d",
      "signature": "torch.atleast_1d(*tensors)",
      "doc": "\n    Returns a 1-dimensional view of each input tensor with zero dimensions.\n    Input tensors with one or more dimensions are returned as-is.\n\n    Args:\n        input (Tensor or list of Tensors)\n\n    Returns:\n        output (Tensor or tuple of Tensors)\n\n    Example::\n\n        >>> x = torch.arange(2)\n        >>> x\n        tensor([0, 1])\n        >>> torch.atleast_1d(x)\n        tensor([0, 1])\n        >>> x = torch.tensor(1.)\n        >>> x\n        tensor(1.)\n        >>> torch.atleast_1d(x)\n        tensor([1.])\n        >>> x = torch.tensor(0.5)\n        >>> y = torch.tensor(1.)\n        >>> torch.atleast_1d((x, y))\n        (tensor([0.5000]), tensor([1.]))\n    ",
      "arguments": [
        "tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns a 1-dimensional view of each input tensor with zero dimensions.\n    Input tensors with one or more dimensions are returned as-is.\n\n    Args:\n        input (Tensor or list of Tensors)\n\n    Returns:\n        output (Tensor or tuple of Tensors)\n\n    Example::\n\n        >>> x = torch.arange(2)\n        >>> x\n        tensor([0, 1])\n        >>> torch.atleast_1d(x)\n        tensor([0, 1])\n        >>> x = torch.tensor(1.)\n        >>> x\n        tensor(1.)\n        >>> torch.atleast_1d(x)\n        tensor([1.])\n        >>> x = torch.tensor(0.5)\n        >>> y = torch.tensor(1.)\n        >>> torch.atleast_1d((x, y))\n        (tensor([0.5000]), tensor([1.]))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.atleast_2d",
      "signature": "torch.atleast_2d(*tensors)",
      "doc": "\n    Returns a 2-dimensional view of each input tensor with zero dimensions.\n    Input tensors with two or more dimensions are returned as-is.\n\n    Args:\n        input (Tensor or list of Tensors)\n\n    Returns:\n        output (Tensor or tuple of Tensors)\n\n    Example::\n\n        >>> x = torch.tensor(1.)\n        >>> x\n        tensor(1.)\n        >>> torch.atleast_2d(x)\n        tensor([[1.]])\n        >>> x = torch.arange(4).view(2, 2)\n        >>> x\n        tensor([[0, 1],\n                [2, 3]])\n        >>> torch.atleast_2d(x)\n        tensor([[0, 1],\n                [2, 3]])\n        >>> x = torch.tensor(0.5)\n        >>> y = torch.tensor(1.)\n        >>> torch.atleast_2d((x, y))\n        (tensor([[0.5000]]), tensor([[1.]]))\n    ",
      "arguments": [
        "tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns a 2-dimensional view of each input tensor with zero dimensions.\n    Input tensors with two or more dimensions are returned as-is.\n\n    Args:\n        input (Tensor or list of Tensors)\n\n    Returns:\n        output (Tensor or tuple of Tensors)\n\n    Example::\n\n        >>> x = torch.tensor(1.)\n        >>> x\n        tensor(1.)\n        >>> torch.atleast_2d(x)\n        tensor([[1.]])\n        >>> x = torch.arange(4).view(2, 2)\n        >>> x\n        tensor([[0, 1],\n                [2, 3]])\n        >>> torch.atleast_2d(x)\n        tensor([[0, 1],\n                [2, 3]])\n        >>> x = torch.tensor(0.5)\n        >>> y = torch.tensor(1.)\n        >>> torch.atleast_2d((x, y))\n        (tensor([[0.5000]]), tensor([[1.]]))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.atleast_3d",
      "signature": "torch.atleast_3d(*tensors)",
      "doc": "\n    Returns a 3-dimensional view of each input tensor with zero dimensions.\n    Input tensors with three or more dimensions are returned as-is.\n\n    Args:\n        input (Tensor or list of Tensors)\n\n    Returns:\n        output (Tensor or tuple of Tensors)\n\n    Example:\n\n        >>> x = torch.tensor(0.5)\n        >>> x\n        tensor(0.5000)\n        >>> torch.atleast_3d(x)\n        tensor([[[0.5000]]])\n        >>> y = torch.arange(4).view(2, 2)\n        >>> y\n        tensor([[0, 1],\n                [2, 3]])\n        >>> torch.atleast_3d(y)\n        tensor([[[0],\n                 [1]],\n                <BLANKLINE>\n                [[2],\n                 [3]]])\n        >>> x = torch.tensor(1).view(1, 1, 1)\n        >>> x\n        tensor([[[1]]])\n        >>> torch.atleast_3d(x)\n        tensor([[[1]]])\n        >>> x = torch.tensor(0.5)\n        >>> y = torch.tensor(1.0)\n        >>> torch.atleast_3d((x, y))\n        (tensor([[[0.5000]]]), tensor([[[1.]]]))\n    ",
      "arguments": [
        "tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns a 3-dimensional view of each input tensor with zero dimensions.\n    Input tensors with three or more dimensions are returned as-is.\n\n    Args:\n        input (Tensor or list of Tensors)\n\n    Returns:\n        output (Tensor or tuple of Tensors)\n\n    Example:\n\n        >>> x = torch.tensor(0.5)\n        >>> x\n        tensor(0.5000)\n        >>> torch.atleast_3d(x)\n        tensor([[[0.5000]]])\n        >>> y = torch.arange(4).view(2, 2)\n        >>> y\n        tensor([[0, 1],\n                [2, 3]])\n        >>> torch.atleast_3d(y)\n        tensor([[[0],\n                 [1]],\n                <BLANKLINE>\n                [[2],\n                 [3]]])\n        >>> x = torch.tensor(1).view(1, 1, 1)\n        >>> x\n        tensor([[[1]]])\n        >>> torch.atleast_3d(x)\n        tensor([[[1]]])\n        >>> x = torch.tensor(0.5)\n        >>> y = torch.tensor(1.0)\n        >>> torch.atleast_3d((x, y))\n        (tensor([[[0.5000]]]), tensor([[[1.]]]))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.block_diag",
      "signature": "torch.block_diag(*tensors)",
      "doc": "Create a block diagonal matrix from provided tensors.\n\n    Args:\n        *tensors: One or more tensors with 0, 1, or 2 dimensions.\n\n    Returns:\n        Tensor: A 2 dimensional tensor with all the input tensors arranged in\n        order such that their upper left and lower right corners are\n        diagonally adjacent. All other elements are set to 0.\n\n    Example::\n\n        >>> import torch\n        >>> A = torch.tensor([[0, 1], [1, 0]])\n        >>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n        >>> C = torch.tensor(7)\n        >>> D = torch.tensor([1, 2, 3])\n        >>> E = torch.tensor([[4], [5], [6]])\n        >>> torch.block_diag(A, B, C, D, E)\n        tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n                [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n    ",
      "arguments": [
        "tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Create a block diagonal matrix from provided tensors.\n\n    Args:\n        *tensors: One or more tensors with 0, 1, or 2 dimensions.\n\n    Returns:\n        Tensor: A 2 dimensional tensor with all the input tensors arranged in\n        order such that their upper left and lower right corners are\n        diagonally adjacent. All other elements are set to 0.\n\n    Example::\n\n        >>> import torch\n        >>> A = torch.tensor([[0, 1], [1, 0]])\n        >>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n        >>> C = torch.tensor(7)\n        >>> D = torch.tensor([1, 2, 3])\n        >>> E = torch.tensor([[4], [5], [6]])\n        >>> torch.block_diag(A, B, C, D, E)\n        tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n                [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.broadcast_shapes",
      "signature": "torch.broadcast_shapes(*shapes)",
      "doc": "broadcast_shapes(*shapes) -> Size\n\n    Similar to :func:`broadcast_tensors` but for shapes.\n\n    This is equivalent to\n    ``torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape``\n    but avoids the need create to intermediate tensors. This is useful for\n    broadcasting tensors of common batch shape but different rightmost shape,\n    e.g. to broadcast mean vectors with covariance matrices.\n\n    Example::\n\n        >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\n        torch.Size([1, 3, 2])\n\n    Args:\n        \\*shapes (torch.Size): Shapes of tensors.\n\n    Returns:\n        shape (torch.Size): A shape compatible with all input shapes.\n\n    Raises:\n        RuntimeError: If shapes are incompatible.\n    ",
      "arguments": [
        "shapes"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "broadcast_shapes(*shapes) -> Size\n\n    Similar to :func:`broadcast_tensors` but for shapes.\n\n    This is equivalent to\n    ``torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape``\n    but avoids the need create to intermediate tensors. This is useful for\n    broadcasting tensors of common batch shape but different rightmost shape,\n    e.g. to broadcast mean vectors with covariance matrices.\n\n    Example::\n\n        >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\n        torch.Size([1, 3, 2])\n\n    Args:\n        \\*shapes (torch.Size): Shapes of tensors.\n\n    Returns:\n        shape (torch.Size): A shape compatible with all input shapes.\n\n    Raises:\n        RuntimeError: If shapes are incompatible.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.broadcast_tensors",
      "signature": "torch.broadcast_tensors(*tensors)",
      "doc": "broadcast_tensors(*tensors) -> List of Tensors\n\n    Broadcasts the given tensors according to :ref:`broadcasting-semantics`.\n\n    Args:\n        *tensors: any number of tensors of the same type\n\n    .. warning::\n\n        More than one element of a broadcasted tensor may refer to a single\n        memory location. As a result, in-place operations (especially ones that\n        are vectorized) may result in incorrect behavior. If you need to write\n        to the tensors, please clone them first.\n\n    Example::\n\n        >>> x = torch.arange(3).view(1, 3)\n        >>> y = torch.arange(2).view(2, 1)\n        >>> a, b = torch.broadcast_tensors(x, y)\n        >>> a.size()\n        torch.Size([2, 3])\n        >>> a\n        tensor([[0, 1, 2],\n                [0, 1, 2]])\n    ",
      "arguments": [
        "tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "broadcast_tensors(*tensors) -> List of Tensors\n\n    Broadcasts the given tensors according to :ref:`broadcasting-semantics`.\n\n    Args:\n        *tensors: any number of tensors of the same type\n\n    .. warning::\n\n        More than one element of a broadcasted tensor may refer to a single\n        memory location. As a result, in-place operations (especially ones that\n        are vectorized) may result in incorrect behavior. If you need to write\n        to the tensors, please clone them first.\n\n    Example::\n\n        >>> x = torch.arange(3).view(1, 3)\n        >>> y = torch.arange(2).view(2, 1)\n        >>> a, b = torch.broadcast_tensors(x, y)\n        >>> a.size()\n        torch.Size([2, 3])\n        >>> a\n        tensor([[0, 1, 2],\n                [0, 1, 2]])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cartesian_prod",
      "signature": "torch.cartesian_prod(*tensors: torch.Tensor) -> torch.Tensor",
      "doc": "Do cartesian product of the given sequence of tensors. The behavior is similar to\n    python's `itertools.product`.\n\n    Args:\n        *tensors: any number of 1 dimensional tensors.\n\n    Returns:\n        Tensor: A tensor equivalent to converting all the input tensors into lists,\n        do `itertools.product` on these lists, and finally convert the resulting list\n        into tensor.\n\n    Example::\n\n        >>> import itertools\n        >>> a = [1, 2, 3]\n        >>> b = [4, 5]\n        >>> list(itertools.product(a, b))\n        [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n        >>> tensor_a = torch.tensor(a)\n        >>> tensor_b = torch.tensor(b)\n        >>> torch.cartesian_prod(tensor_a, tensor_b)\n        tensor([[1, 4],\n                [1, 5],\n                [2, 4],\n                [2, 5],\n                [3, 4],\n                [3, 5]])\n    ",
      "arguments": [
        "tensors"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Do cartesian product of the given sequence of tensors. The behavior is similar to\n    python's `itertools.product`.\n\n    Args:\n        *tensors: any number of 1 dimensional tensors.\n\n    Returns:\n        Tensor: A tensor equivalent to converting all the input tensors into lists,\n        do `itertools.product` on these lists, and finally convert the resulting list\n        into tensor.\n\n    Example::\n\n        >>> import itertools\n        >>> a = [1, 2, 3]\n        >>> b = [4, 5]\n        >>> list(itertools.product(a, b))\n        [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n        >>> tensor_a = torch.tensor(a)\n        >>> tensor_b = torch.tensor(b)\n        >>> torch.cartesian_prod(tensor_a, tensor_b)\n        tensor([[1, 4],\n                [1, 5],\n                [2, 4],\n                [2, 5],\n                [3, 4],\n                [3, 5]])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cdist",
      "signature": "torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')",
      "doc": "Computes batched the p-norm distance between each pair of the two collections of row vectors.\n\n    Args:\n        x1 (Tensor): input tensor where the last two dimensions represent the points and the feature dimension respectively.\n            The shape can be :math:`D_1 \\times D_2 \\times \\cdots \\times D_n \\times P \\times M`,\n            where :math:`P` is the number of points and :math:`M` is the feature dimension.\n        x2 (Tensor): input tensor where the last two dimensions also represent the points and the feature dimension respectively.\n            The shape can be :math:`D_1' \\times D_2' \\times \\cdots \\times D_m' \\times R \\times M`,\n            where :math:`R` is the number of points and :math:`M` is the feature dimension,\n            which should match the feature dimension of `x1`.\n        p: p value for the p-norm distance to calculate between each vector pair\n            :math:`\\in [0, \\infty]`.\n        compute_mode:\n            'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate\n            euclidean distance (p = 2) if P > 25 or R > 25\n            'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate\n            euclidean distance (p = 2)\n            'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate\n            euclidean distance (p = 2)\n            Default: use_mm_for_euclid_dist_if_necessary.\n\n    If x1 has shape :math:`B \\times P \\times M` and x2 has shape :math:`B \\times R \\times M` then the\n    output will have shape :math:`B \\times P \\times R`.\n\n    This function is equivalent to `scipy.spatial.distance.cdist(input,'minkowski', p=p)`\n    if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is equivalent to\n    `scipy.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p = \\infty`, the closest\n    scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())`.\n\n    Example:\n\n        >>> a = torch.tensor([[0.9041, 0.0196], [-0.3108, -2.4423], [-0.4821, 1.059]])\n        >>> a\n        tensor([[ 0.9041,  0.0196],\n                [-0.3108, -2.4423],\n                [-0.4821,  1.0590]])\n        >>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986, 1.3702]])\n        >>> b\n        tensor([[-2.1763, -0.4713],\n                [-0.6986,  1.3702]])\n        >>> torch.cdist(a, b, p=2)\n        tensor([[3.1193, 2.0959],\n                [2.7138, 3.8322],\n                [2.2830, 0.3791]])\n    ",
      "arguments": [
        "x1",
        "x2",
        "p",
        "compute_mode"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Computes batched the p-norm distance between each pair of the two collections of row vectors.\n\n    Args:\n        x1 (Tensor): input tensor where the last two dimensions represent the points and the feature dimension respectively.\n            The shape can be :math:`D_1 \\times D_2 \\times \\cdots \\times D_n \\times P \\times M`,\n            where :math:`P` is the number of points and :math:`M` is the feature dimension.\n        x2 (Tensor): input tensor where the last two dimensions also represent the points and the feature dimension respectively.\n            The shape can be :math:`D_1' \\times D_2' \\times \\cdots \\times D_m' \\times R \\times M`,\n            where :math:`R` is the number of points and :math:`M` is the feature dimension,\n            which should match the feature dimension of `x1`.\n        p: p value for the p-norm distance to calculate between each vector pair\n            :math:`\\in [0, \\infty]`.\n        compute_mode:\n            'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate\n            euclidean distance (p = 2) if P > 25 or R > 25\n            'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate\n            euclidean distance (p = 2)\n            'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate\n            euclidean distance (p = 2)\n            Default: use_mm_for_euclid_dist_if_necessary.\n\n    If x1 has shape :math:`B \\times P \\times M` and x2 has shape :math:`B \\times R \\times M` then the\n    output will have shape :math:`B \\times P \\times R`.\n\n    This function is equivalent to `scipy.spatial.distance.cdist(input,'minkowski', p=p)`\n    if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is equivalent to\n    `scipy.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p = \\infty`, the closest\n    scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())`.\n\n    Example:\n\n        >>> a = torch.tensor([[0.9041, 0.0196], [-0.3108, -2.4423], [-0.4821, 1.059]])\n        >>> a\n        tensor([[ 0.9041,  0.0196],\n                [-0.3108, -2.4423],\n                [-0.4821,  1.0590]])\n        >>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986, 1.3702]])\n        >>> b\n        tensor([[-2.1763, -0.4713],\n                [-0.6986,  1.3702]])\n        >>> torch.cdist(a, b, p=2)\n        tensor([[3.1193, 2.0959],\n                [2.7138, 3.8322],\n                [2.2830, 0.3791]])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.chain_matmul",
      "signature": "torch.chain_matmul(*matrices, out=None)",
      "doc": "Returns the matrix product of the :math:`N` 2-D tensors. This product is efficiently computed\n    using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\n    of arithmetic operations (`[CLRS]`_). Note that since this is a function to compute the product, :math:`N`\n    needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\n    If :math:`N` is 1, then this is a no-op - the original matrix is returned as is.\n\n    .. warning::\n\n        :func:`torch.chain_matmul` is deprecated and will be removed in a future PyTorch release.\n        Use :func:`torch.linalg.multi_dot` instead, which accepts a list of two or more tensors\n        rather than multiple arguments.\n\n    Args:\n        matrices (Tensors...): a sequence of 2 or more 2-D tensors whose product is to be determined.\n        out (Tensor, optional): the output tensor. Ignored if :attr:`out` = ``None``.\n\n    Returns:\n        Tensor: if the :math:`i^{th}` tensor was of dimensions :math:`p_{i} \\times p_{i + 1}`, then the product\n        would be of dimensions :math:`p_{1} \\times p_{N + 1}`.\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> a = torch.randn(3, 4)\n        >>> b = torch.randn(4, 5)\n        >>> c = torch.randn(5, 6)\n        >>> d = torch.randn(6, 7)\n        >>> # will raise a deprecation warning\n        >>> torch.chain_matmul(a, b, c, d)\n        tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n                [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n                [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n\n    .. _`[CLRS]`: https://mitpress.mit.edu/books/introduction-algorithms-third-edition\n    ",
      "arguments": [
        "matrices",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the matrix product of the :math:`N` 2-D tensors. This product is efficiently computed\n    using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\n    of arithmetic operations (`[CLRS]`_). Note that since this is a function to compute the product, :math:`N`\n    needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\n    If :math:`N` is 1, then this is a no-op - the original matrix is returned as is.\n\n    .. warning::\n\n        :func:`torch.chain_matmul` is deprecated and will be removed in a future PyTorch release.\n        Use :func:`torch.linalg.multi_dot` instead, which accepts a list of two or more tensors\n        rather than multiple arguments.\n\n    Args:\n        matrices (Tensors...): a sequence of 2 or more 2-D tensors whose product is to be determined.\n        out (Tensor, optional): the output tensor. Ignored if :attr:`out` = ``None``.\n\n    Returns:\n        Tensor: if the :math:`i^{th}` tensor was of dimensions :math:`p_{i} \\times p_{i + 1}`, then the product\n        would be of dimensions :math:`p_{1} \\times p_{N + 1}`.\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> a = torch.randn(3, 4)\n        >>> b = torch.randn(4, 5)\n        >>> c = torch.randn(5, 6)\n        >>> d = torch.randn(6, 7)\n        >>> # will raise a deprecation warning\n        >>> torch.chain_matmul(a, b, c, d)\n        tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n                [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n                [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n\n    .. _`[CLRS]`: https://mitpress.mit.edu/books/introduction-algorithms-third-edition\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.classproperty",
      "signature": "torch.classproperty(func)",
      "doc": "",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compile",
      "signature": "torch.compile(model: Optional[Callable] = None, *, fullgraph: bool = False, dynamic: Optional[bool] = None, backend: Union[str, Callable] = 'inductor', mode: Optional[str] = None, options: Optional[dict[str, Union[str, int, bool, Callable]]] = None, disable: bool = False) -> Union[Callable[[Callable[~_InputT, ~_RetT]], Callable[~_InputT, ~_RetT]], Callable[~_InputT, ~_RetT]]",
      "doc": "\n    Optimizes given model/function using TorchDynamo and specified backend.\n    If you are compiling an :class:`torch.nn.Module`, you can also use :meth:`torch.nn.Module.compile`\n    to compile the module inplace without changing its structure.\n\n    Concretely, for every frame executed within the compiled region, we will attempt\n    to compile it and cache the compiled result on the code object for future\n    use.  A single frame may be compiled multiple times if previous compiled\n    results are not applicable for subsequent calls (this is called a \"guard\n    failure), you can use TORCH_LOGS=guards to debug these situations.\n    Multiple compiled results can be associated with a frame up to\n    ``torch._dynamo.config.recompile_limit``, which defaults to 8; at which\n    point we will fall back to eager.  Note that compile caches are per\n    *code object*, not frame; if you dynamically create multiple copies of a\n    function, they will all share the same code cache.\n\n    Args:\n       model (Callable): Module/function to optimize\n       fullgraph (bool): If False (default), torch.compile attempts to discover compileable regions\n        in the function that it will optimize. If True, then we require that the entire function be\n        capturable into a single graph. If this is not possible (that is, if there are graph breaks),\n        then this will raise an error.\n       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt\n        to generate a kernel that is as dynamic as possible to avoid recompilations when\n        sizes change.  This may not always work as some operations/optimizations will\n        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.\n        When this is False, we will NEVER generate dynamic kernels, we will always specialize.\n        By default (None), we automatically detect if dynamism has occurred and compile a more\n        dynamic kernel upon recompile.\n       backend (str or Callable): backend to be used\n\n        - \"inductor\" is the default backend, which is a good balance between performance and overhead\n\n        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`\n\n        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`\n\n        - To register an out-of-tree custom backend:\n          https://pytorch.org/docs/main/torch.compiler_custom_backends.html#registering-custom-backends\n       mode (str): Can be either \"default\", \"reduce-overhead\", \"max-autotune\" or \"max-autotune-no-cudagraphs\"\n\n        - \"default\" is the default mode, which is a good balance between performance and overhead\n\n        - \"reduce-overhead\" is a mode that reduces the overhead of python with CUDA graphs,\n          useful for small batches.  Reduction of overhead can come at the cost of more memory\n          usage, as we will cache the workspace memory required for the invocation so that we\n          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\n          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\n          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\n          to debug.\n\n        - \"max-autotune\" is a mode that leverages Triton or template based matrix multiplications\n          on supported devices and Triton based convolutions on GPU.\n          It enables CUDA graphs by default on GPU.\n\n        - \"max-autotune-no-cudagraphs\" is a mode similar to \"max-autotune\" but without CUDA graphs\n\n        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`\n\n       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are\n\n        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set\n\n        - `max_autotune` which will profile to pick the best matmul configuration\n\n        - `fallback_random` which is useful when debugging accuracy issues\n\n        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores\n\n        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs\n\n        - `trace.enabled` which is the most useful debugging flag to turn on\n\n        - `trace.graph_diagram` which will show you a picture of your graph after fusion\n\n        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`\n       disable (bool): Turn torch.compile() into a no-op for testing\n\n    Example::\n\n        @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n        def foo(x):\n            return torch.sin(x) + torch.cos(x)\n\n    ",
      "arguments": [
        "model",
        "fullgraph",
        "dynamic",
        "backend",
        "mode",
        "options",
        "disable"
      ],
      "return_type": "typing.Union[typing.Callable[[typing.Callable[~_InputT, ~_RetT]], typing.Callable[~_InputT, ~_RetT]], typing.Callable[~_InputT, ~_RetT]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Optimizes given model/function using TorchDynamo and specified backend.\n    If you are compiling an :class:`torch.nn.Module`, you can also use :meth:`torch.nn.Module.compile`\n    to compile the module inplace without changing its structure.\n\n    Concretely, for every frame executed within the compiled region, we will attempt\n    to compile it and cache the compiled result on the code object for future\n    use.  A single frame may be compiled multiple times if previous compiled\n    results are not applicable for subsequent calls (this is called a \"guard\n    failure), you can use TORCH_LOGS=guards to debug these situations.\n    Multiple compiled results can be associated with a frame up to\n    ``torch._dynamo.config.recompile_limit``, which defaults to 8; at which\n    point we will fall back to eager.  Note that compile caches are per\n    *code object*, not frame; if you dynamically create multiple copies of a\n    function, they will all share the same code cache.\n\n    Args:\n       model (Callable): Module/function to optimize\n       fullgraph (bool): If False (default), torch.compile attempts to discover compileable regions\n        in the function that it will optimize. If True, then we require that the entire function be\n        capturable into a single graph. If this is not possible (that is, if there are graph breaks),\n        then this will raise an error.\n       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt\n        to generate a kernel that is as dynamic as possible to avoid recompilations when\n        sizes change.  This may not always work as some operations/optimizations will\n        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.\n        When this is False, we will NEVER generate dynamic kernels, we will always specialize.\n        By default (None), we automatically detect if dynamism has occurred and compile a more\n        dynamic kernel upon recompile.\n       backend (str or Callable): backend to be used\n\n        - \"inductor\" is the default backend, which is a good balance between performance and overhead\n\n        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`\n\n        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`\n\n        - To register an out-of-tree custom backend:\n          https://pytorch.org/docs/main/torch.compiler_custom_backends.html#registering-custom-backends\n       mode (str): Can be either \"default\", \"reduce-overhead\", \"max-autotune\" or \"max-autotune-no-cudagraphs\"\n\n        - \"default\" is the default mode, which is a good balance between performance and overhead\n\n        - \"reduce-overhead\" is a mode that reduces the overhead of python with CUDA graphs,\n          useful for small batches.  Reduction of overhead can come at the cost of more memory\n          usage, as we will cache the workspace memory required for the invocation so that we\n          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\n          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\n          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\n          to debug.\n\n        - \"max-autotune\" is a mode that leverages Triton or template based matrix multiplications\n          on supported devices and Triton based convolutions on GPU.\n          It enables CUDA graphs by default on GPU.\n\n        - \"max-autotune-no-cudagraphs\" is a mode similar to \"max-autotune\" but without CUDA graphs\n\n        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`\n\n       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are\n\n        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set\n\n        - `max_autotune` which will profile to pick the best matmul configuration\n\n        - `fallback_random` which is useful when debugging accuracy issues\n\n        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores\n\n        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs\n\n        - `trace.enabled` which is the most useful debugging flag to turn on\n\n        - `trace.graph_diagram` which will show you a picture of your graph after fusion\n\n        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`\n       disable (bool): Turn torch.compile() into a no-op for testing\n\n    Example::\n\n        @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n        def foo(x):\n            return torch.sin(x) + torch.cos(x)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiled_with_cxx11_abi",
      "signature": "torch.compiled_with_cxx11_abi() -> bool",
      "doc": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cond",
      "signature": "torch.cond(pred: Union[bool, int, float, torch.Tensor], true_fn: Callable, false_fn: Callable, operands: Union[tuple, list] = ()) -> Any",
      "doc": "\n    Conditionally applies `true_fn` or `false_fn`.\n\n    .. warning::\n        `torch.cond` is a prototype feature in PyTorch. It has limited support for input and output types and\n        doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\n        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    `cond` is structured control flow operator. That is, it is like a Python if-statement,\n    but has restrictions on `true_fn`, `false_fn`, and `operands` that enable it to be\n    capturable using torch.compile and torch.export.\n\n    Assuming the constraints on `cond`'s arguments are met, `cond` is equivalent to the following::\n\n        def cond(pred, true_branch, false_branch, operands):\n            if pred:\n                return true_branch(*operands)\n            else:\n                return false_branch(*operands)\n\n    Args:\n        pred (Union[bool, torch.Tensor]): A boolean expression or a tensor with one element,\n          indicating which branch function to apply.\n\n        true_fn (Callable): A callable function (a -> b) that is within the\n          scope that is being traced.\n\n        false_fn (Callable): A callable function (a -> b) that is within the\n          scope that is being traced. The true branch and false branch must\n          have consistent input and outputs, meaning the inputs have to be\n          the same, and the outputs have to be the same type and shape.\n\n        operands (Tuple of possibly nested dict/list/tuple of torch.Tensor): A tuple of inputs to the\n          true/false functions. It can be empty if true_fn/false_fn doesn't require input. Defaults to ().\n\n    Example::\n\n        def true_fn(x: torch.Tensor):\n            return x.cos()\n        def false_fn(x: torch.Tensor):\n            return x.sin()\n        return cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n    Restrictions:\n        - The conditional statement (aka `pred`) must meet one of the following constraints:\n\n          - It's a `torch.Tensor` with only one element, and torch.bool dtype\n\n          - It's a boolean expression, e.g. `x.shape[0] > 10` or `x.dim() > 1 and x.shape[1] > 10`\n\n        - The branch function (aka `true_fn`/`false_fn`) must meet all of the following constraints:\n\n          - The function signature must match with operands.\n\n          - The function must return a tensor with the same metadata, e.g. shape,\n            dtype, etc.\n\n          - The function cannot have in-place mutations on inputs or global variables.\n            (Note: in-place tensor operations such as `add_` for intermediate results\n            are allowed in a branch)\n\n    ",
      "arguments": [
        "pred",
        "true_fn",
        "false_fn",
        "operands"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Conditionally applies `true_fn` or `false_fn`.\n\n    .. warning::\n        `torch.cond` is a prototype feature in PyTorch. It has limited support for input and output types and\n        doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\n        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    `cond` is structured control flow operator. That is, it is like a Python if-statement,\n    but has restrictions on `true_fn`, `false_fn`, and `operands` that enable it to be\n    capturable using torch.compile and torch.export.\n\n    Assuming the constraints on `cond`'s arguments are met, `cond` is equivalent to the following::\n\n        def cond(pred, true_branch, false_branch, operands):\n            if pred:\n                return true_branch(*operands)\n            else:\n                return false_branch(*operands)\n\n    Args:\n        pred (Union[bool, torch.Tensor]): A boolean expression or a tensor with one element,\n          indicating which branch function to apply.\n\n        true_fn (Callable): A callable function (a -> b) that is within the\n          scope that is being traced.\n\n        false_fn (Callable): A callable function (a -> b) that is within the\n          scope that is being traced. The true branch and false branch must\n          have consistent input and outputs, meaning the inputs have to be\n          the same, and the outputs have to be the same type and shape.\n\n        operands (Tuple of possibly nested dict/list/tuple of torch.Tensor): A tuple of inputs to the\n          true/false functions. It can be empty if true_fn/false_fn doesn't require input. Defaults to ().\n\n    Example::\n\n        def true_fn(x: torch.Tensor):\n            return x.cos()\n        def false_fn(x: torch.Tensor):\n            return x.sin()\n        return cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n    Restrictions:\n        - The conditional statement (aka `pred`) must meet one of the following constraints:\n\n          - It's a `torch.Tensor` with only one element, and torch.bool dtype\n\n          - It's a boolean expression, e.g. `x.shape[0] > 10` or `x.dim() > 1 and x.shape[1] > 10`\n\n        - The branch function (aka `true_fn`/`false_fn`) must meet all of the following constraints:\n\n          - The function signature must match with operands.\n\n          - The function must return a tensor with the same metadata, e.g. shape,\n            dtype, etc.\n\n          - The function cannot have in-place mutations on inputs or global variables.\n            (Note: in-place tensor operations such as `add_` for intermediate results\n            are allowed in a branch)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.eig",
      "signature": "torch.eig(self: torch.Tensor, eigenvectors: bool = False, *, e=None, v=None) -> tuple[torch.Tensor, torch.Tensor]",
      "doc": "",
      "arguments": [
        "self",
        "eigenvectors",
        "e",
        "v"
      ],
      "return_type": "tuple[torch.Tensor, torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.einsum",
      "signature": "torch.einsum(*args: Any) -> torch.Tensor",
      "doc": "einsum(equation, *operands) -> Tensor\n\n    Sums the product of the elements of the input :attr:`operands` along dimensions specified using a notation\n    based on the Einstein summation convention.\n\n    Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\n    in a short-hand format based on the Einstein summation convention, given by :attr:`equation`. The details of\n    this format are described below, but the general idea is to label every dimension of the input :attr:`operands`\n    with some subscript and define which subscripts are part of the output. The output is then computed by summing\n    the product of the elements of the :attr:`operands` along the dimensions whose subscripts are not part of the\n    output. For example, matrix multiplication can be computed using einsum as `torch.einsum(\"ij,jk->ik\", A, B)`.\n    Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).\n\n    Equation:\n\n        The :attr:`equation` string specifies the subscripts (letters in `[a-zA-Z]`) for each dimension of\n        the input :attr:`operands` in the same order as the dimensions, separating subscripts for each operand by a\n        comma (','), e.g. `'ij,jk'` specify subscripts for two 2D operands. The dimensions labeled with the same subscript\n        must be broadcastable, that is, their size must either match or be `1`. The exception is if a subscript is\n        repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\n        must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\n        appear exactly once in the :attr:`equation` will be part of the output, sorted in increasing alphabetical order.\n        The output is computed by multiplying the input :attr:`operands` element-wise, with their dimensions aligned based\n        on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.\n\n        Optionally, the output subscripts can be explicitly defined by adding an arrow ('->') at the end of the equation\n        followed by the subscripts for the output. For instance, the following equation computes the transpose of a\n        matrix multiplication: 'ij,jk->ki'. The output subscripts must appear at least once for some input operand and\n        at most once for the output.\n\n        Ellipsis ('...') can be used in place of subscripts to broadcast the dimensions covered by the ellipsis.\n        Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts,\n        e.g. for an input operand with 5 dimensions, the ellipsis in the equation `'ab...c'` cover the third and fourth\n        dimensions. The ellipsis does not need to cover the same number of dimensions across the :attr:`operands` but the\n        'shape' of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not\n        explicitly defined with the arrow ('->') notation, the ellipsis will come first in the output (left-most dimensions),\n        before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements\n        batch matrix multiplication `'...ij,...jk'`.\n\n        A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis,\n        arrow and comma) but something like `'. . .'` is not valid. An empty string `''` is valid for scalar operands.\n\n    .. note::\n\n        ``torch.einsum`` handles ellipsis ('...') differently from NumPy in that it allows dimensions\n        covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.\n\n    .. note::\n\n        Please install opt-einsum (https://optimized-einsum.readthedocs.io/en/stable/) in order to enroll into a more\n        performant einsum. You can install when installing torch like so: `pip install torch[opt-einsum]` or by itself\n        with `pip install opt-einsum`.\n\n        If opt-einsum is available, this function will automatically speed up computation and/or consume less memory\n        by optimizing contraction order through our opt_einsum backend :mod:`torch.backends.opt_einsum` (The _ vs - is\n        confusing, I know). This optimization occurs when there are at least three inputs, since the order does not matter\n        otherwise. Note that finding `the` optimal path is an NP-hard problem, thus, opt-einsum relies on different\n        heuristics to achieve near-optimal results. If opt-einsum is not available, the default order is to contract\n        from left to right.\n\n        To bypass this default behavior, add the following to disable opt_einsum and skip path calculation:\n        ``torch.backends.opt_einsum.enabled = False``\n\n        To specify which strategy you'd like for opt_einsum to compute the contraction path, add the following line:\n        ``torch.backends.opt_einsum.strategy = 'auto'``. The default strategy is 'auto', and we also support 'greedy' and\n        'optimal'. Disclaimer that the runtime of 'optimal' is factorial in the number of inputs! See more details in\n        the opt_einsum documentation (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).\n\n    .. note::\n\n        As of PyTorch 1.10 :func:`torch.einsum` also supports the sublist format (see examples below). In this format,\n        subscripts for each operand are specified by sublists, list of integers in the range [0, 52). These sublists\n        follow their operands, and an extra sublist can appear at the end of the input to specify the output's\n        subscripts., e.g. `torch.einsum(op1, sublist1, op2, sublist2, ..., [subslist_out])`. Python's `Ellipsis` object\n        may be provided in a sublist to enable broadcasting as described in the Equation section above.\n\n    Args:\n        equation (str): The subscripts for the Einstein summation.\n        operands (List[Tensor]): The tensors to compute the Einstein summation of.\n\n    Examples::\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # trace\n        >>> torch.einsum('ii', torch.randn(4, 4))\n        tensor(-1.2104)\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # diagonal\n        >>> torch.einsum('ii->i', torch.randn(4, 4))\n        tensor([-0.1034,  0.7952, -0.2433,  0.4545])\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # outer product\n        >>> x = torch.randn(5)\n        >>> y = torch.randn(4)\n        >>> torch.einsum('i,j->ij', x, y)\n        tensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n                [-0.3744,  0.9381,  1.2685, -1.6070],\n                [ 0.7208, -1.8058, -2.4419,  3.0936],\n                [ 0.1713, -0.4291, -0.5802,  0.7350],\n                [ 0.5704, -1.4290, -1.9323,  2.4480]])\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # batch matrix multiplication\n        >>> As = torch.randn(3, 2, 5)\n        >>> Bs = torch.randn(3, 5, 4)\n        >>> torch.einsum('bij,bjk->bik', As, Bs)\n        tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n                [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n                [[ 4.2239,  0.3107, -0.5756, -0.2354],\n                [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n                [[ 2.8153,  1.8787, -4.3839, -1.2112],\n                [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # with sublist format and ellipsis\n        >>> torch.einsum(As, [..., 0, 1], Bs, [..., 1, 2], [..., 0, 2])\n        tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n                [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n                [[ 4.2239,  0.3107, -0.5756, -0.2354],\n                [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n                [[ 2.8153,  1.8787, -4.3839, -1.2112],\n                [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n        >>> # batch permute\n        >>> A = torch.randn(2, 3, 4, 5)\n        >>> torch.einsum('...ij->...ji', A).shape\n        torch.Size([2, 3, 5, 4])\n\n        >>> # equivalent to torch.nn.functional.bilinear\n        >>> A = torch.randn(3, 5, 4)\n        >>> l = torch.randn(2, 5)\n        >>> r = torch.randn(2, 4)\n        >>> torch.einsum('bn,anm,bm->ba', l, A, r)\n        tensor([[-0.3430, -5.2405,  0.4494],\n                [ 0.3311,  5.5201, -3.0356]])\n    ",
      "arguments": [
        "args"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "einsum(equation, *operands) -> Tensor\n\n    Sums the product of the elements of the input :attr:`operands` along dimensions specified using a notation\n    based on the Einstein summation convention.\n\n    Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\n    in a short-hand format based on the Einstein summation convention, given by :attr:`equation`. The details of\n    this format are described below, but the general idea is to label every dimension of the input :attr:`operands`\n    with some subscript and define which subscripts are part of the output. The output is then computed by summing\n    the product of the elements of the :attr:`operands` along the dimensions whose subscripts are not part of the\n    output. For example, matrix multiplication can be computed using einsum as `torch.einsum(\"ij,jk->ik\", A, B)`.\n    Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).\n\n    Equation:\n\n        The :attr:`equation` string specifies the subscripts (letters in `[a-zA-Z]`) for each dimension of\n        the input :attr:`operands` in the same order as the dimensions, separating subscripts for each operand by a\n        comma (','), e.g. `'ij,jk'` specify subscripts for two 2D operands. The dimensions labeled with the same subscript\n        must be broadcastable, that is, their size must either match or be `1`. The exception is if a subscript is\n        repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\n        must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\n        appear exactly once in the :attr:`equation` will be part of the output, sorted in increasing alphabetical order.\n        The output is computed by multiplying the input :attr:`operands` element-wise, with their dimensions aligned based\n        on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.\n\n        Optionally, the output subscripts can be explicitly defined by adding an arrow ('->') at the end of the equation\n        followed by the subscripts for the output. For instance, the following equation computes the transpose of a\n        matrix multiplication: 'ij,jk->ki'. The output subscripts must appear at least once for some input operand and\n        at most once for the output.\n\n        Ellipsis ('...') can be used in place of subscripts to broadcast the dimensions covered by the ellipsis.\n        Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts,\n        e.g. for an input operand with 5 dimensions, the ellipsis in the equation `'ab...c'` cover the third and fourth\n        dimensions. The ellipsis does not need to cover the same number of dimensions across the :attr:`operands` but the\n        'shape' of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not\n        explicitly defined with the arrow ('->') notation, the ellipsis will come first in the output (left-most dimensions),\n        before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements\n        batch matrix multiplication `'...ij,...jk'`.\n\n        A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis,\n        arrow and comma) but something like `'. . .'` is not valid. An empty string `''` is valid for scalar operands.\n\n    .. note::\n\n        ``torch.einsum`` handles ellipsis ('...') differently from NumPy in that it allows dimensions\n        covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.\n\n    .. note::\n\n        Please install opt-einsum (https://optimized-einsum.readthedocs.io/en/stable/) in order to enroll into a more\n        performant einsum. You can install when installing torch like so: `pip install torch[opt-einsum]` or by itself\n        with `pip install opt-einsum`.\n\n        If opt-einsum is available, this function will automatically speed up computation and/or consume less memory\n        by optimizing contraction order through our opt_einsum backend :mod:`torch.backends.opt_einsum` (The _ vs - is\n        confusing, I know). This optimization occurs when there are at least three inputs, since the order does not matter\n        otherwise. Note that finding `the` optimal path is an NP-hard problem, thus, opt-einsum relies on different\n        heuristics to achieve near-optimal results. If opt-einsum is not available, the default order is to contract\n        from left to right.\n\n        To bypass this default behavior, add the following to disable opt_einsum and skip path calculation:\n        ``torch.backends.opt_einsum.enabled = False``\n\n        To specify which strategy you'd like for opt_einsum to compute the contraction path, add the following line:\n        ``torch.backends.opt_einsum.strategy = 'auto'``. The default strategy is 'auto', and we also support 'greedy' and\n        'optimal'. Disclaimer that the runtime of 'optimal' is factorial in the number of inputs! See more details in\n        the opt_einsum documentation (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).\n\n    .. note::\n\n        As of PyTorch 1.10 :func:`torch.einsum` also supports the sublist format (see examples below). In this format,\n        subscripts for each operand are specified by sublists, list of integers in the range [0, 52). These sublists\n        follow their operands, and an extra sublist can appear at the end of the input to specify the output's\n        subscripts., e.g. `torch.einsum(op1, sublist1, op2, sublist2, ..., [subslist_out])`. Python's `Ellipsis` object\n        may be provided in a sublist to enable broadcasting as described in the Equation section above.\n\n    Args:\n        equation (str): The subscripts for the Einstein summation.\n        operands (List[Tensor]): The tensors to compute the Einstein summation of.\n\n    Examples::\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # trace\n        >>> torch.einsum('ii', torch.randn(4, 4))\n        tensor(-1.2104)\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # diagonal\n        >>> torch.einsum('ii->i', torch.randn(4, 4))\n        tensor([-0.1034,  0.7952, -0.2433,  0.4545])\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # outer product\n        >>> x = torch.randn(5)\n        >>> y = torch.randn(4)\n        >>> torch.einsum('i,j->ij', x, y)\n        tensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n                [-0.3744,  0.9381,  1.2685, -1.6070],\n                [ 0.7208, -1.8058, -2.4419,  3.0936],\n                [ 0.1713, -0.4291, -0.5802,  0.7350],\n                [ 0.5704, -1.4290, -1.9323,  2.4480]])\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # batch matrix multiplication\n        >>> As = torch.randn(3, 2, 5)\n        >>> Bs = torch.randn(3, 5, 4)\n        >>> torch.einsum('bij,bjk->bik', As, Bs)\n        tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n                [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n                [[ 4.2239,  0.3107, -0.5756, -0.2354],\n                [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n                [[ 2.8153,  1.8787, -4.3839, -1.2112],\n                [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> # with sublist format and ellipsis\n        >>> torch.einsum(As, [..., 0, 1], Bs, [..., 1, 2], [..., 0, 2])\n        tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n                [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n                [[ 4.2239,  0.3107, -0.5756, -0.2354],\n                [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n                [[ 2.8153,  1.8787, -4.3839, -1.2112],\n                [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n        >>> # batch permute\n        >>> A = torch.randn(2, 3, 4, 5)\n        >>> torch.einsum('...ij->...ji', A).shape\n        torch.Size([2, 3, 5, 4])\n\n        >>> # equivalent to torch.nn.functional.bilinear\n        >>> A = torch.randn(3, 5, 4)\n        >>> l = torch.randn(2, 5)\n        >>> r = torch.randn(2, 4)\n        >>> torch.einsum('bn,anm,bm->ba', l, A, r)\n        tensor([[-0.3430, -5.2405,  0.4494],\n                [ 0.3311,  5.5201, -3.0356]])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.from_dlpack",
      "signature": "torch.from_dlpack(ext_tensor: Any) -> 'torch.Tensor'",
      "doc": "from_dlpack(ext_tensor) -> Tensor\n\n    Converts a tensor from an external library into a ``torch.Tensor``.\n\n    The returned PyTorch tensor will share the memory with the input tensor\n    (which may have come from another library). Note that in-place operations\n    will therefore also affect the data of the input tensor. This may lead to\n    unexpected issues (e.g., other libraries may have read-only flags or\n    immutable data structures), so the user should only do this if they know\n    for sure that this is fine.\n\n    Args:\n        ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule):\n            The tensor or DLPack capsule to convert.\n\n            If ``ext_tensor`` is a tensor (or ndarray) object, it must support\n            the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__``\n            method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is\n            an opaque ``PyCapsule`` instance, typically produced by a\n            ``to_dlpack`` function or method.\n\n    Examples::\n\n        >>> import torch.utils.dlpack\n        >>> t = torch.arange(4)\n\n        # Convert a tensor directly (supported in PyTorch >= 1.10)\n        >>> t2 = torch.from_dlpack(t)\n        >>> t2[:2] = -1  # show that memory is shared\n        >>> t2\n        tensor([-1, -1,  2,  3])\n        >>> t\n        tensor([-1, -1,  2,  3])\n\n        # The old-style DLPack usage, with an intermediate capsule object\n        >>> capsule = torch.utils.dlpack.to_dlpack(t)\n        >>> capsule\n        <capsule object \"dltensor\" at ...>\n        >>> t3 = torch.from_dlpack(capsule)\n        >>> t3\n        tensor([-1, -1,  2,  3])\n        >>> t3[0] = -9  # now we're sharing memory between 3 tensors\n        >>> t3\n        tensor([-9, -1,  2,  3])\n        >>> t2\n        tensor([-9, -1,  2,  3])\n        >>> t\n        tensor([-9, -1,  2,  3])\n\n    ",
      "arguments": [
        "ext_tensor"
      ],
      "return_type": "torch.Tensor",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "from_dlpack(ext_tensor) -> Tensor\n\n    Converts a tensor from an external library into a ``torch.Tensor``.\n\n    The returned PyTorch tensor will share the memory with the input tensor\n    (which may have come from another library). Note that in-place operations\n    will therefore also affect the data of the input tensor. This may lead to\n    unexpected issues (e.g., other libraries may have read-only flags or\n    immutable data structures), so the user should only do this if they know\n    for sure that this is fine.\n\n    Args:\n        ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule):\n            The tensor or DLPack capsule to convert.\n\n            If ``ext_tensor`` is a tensor (or ndarray) object, it must support\n            the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__``\n            method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is\n            an opaque ``PyCapsule`` instance, typically produced by a\n            ``to_dlpack`` function or method.\n\n    Examples::\n\n        >>> import torch.utils.dlpack\n        >>> t = torch.arange(4)\n\n        # Convert a tensor directly (supported in PyTorch >= 1.10)\n        >>> t2 = torch.from_dlpack(t)\n        >>> t2[:2] = -1  # show that memory is shared\n        >>> t2\n        tensor([-1, -1,  2,  3])\n        >>> t\n        tensor([-1, -1,  2,  3])\n\n        # The old-style DLPack usage, with an intermediate capsule object\n        >>> capsule = torch.utils.dlpack.to_dlpack(t)\n        >>> capsule\n        <capsule object \"dltensor\" at ...>\n        >>> t3 = torch.from_dlpack(capsule)\n        >>> t3\n        tensor([-1, -1,  2,  3])\n        >>> t3[0] = -9  # now we're sharing memory between 3 tensors\n        >>> t3\n        tensor([-9, -1,  2,  3])\n        >>> t2\n        tensor([-9, -1,  2,  3])\n        >>> t\n        tensor([-9, -1,  2,  3])\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.get_default_device",
      "signature": "torch.get_default_device() -> 'torch.device'",
      "doc": "Gets the default ``torch.Tensor`` to be allocated on ``device``",
      "arguments": [],
      "return_type": "torch.device",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Gets the default ``torch.Tensor`` to be allocated on ``device``",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.get_deterministic_debug_mode",
      "signature": "torch.get_deterministic_debug_mode() -> int",
      "doc": "Returns the current value of the debug mode for deterministic\n    operations. Refer to :func:`torch.set_deterministic_debug_mode`\n    documentation for more details.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the current value of the debug mode for deterministic\n    operations. Refer to :func:`torch.set_deterministic_debug_mode`\n    documentation for more details.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.get_file_path",
      "signature": "torch.get_file_path(*path_components: str) -> str",
      "doc": "",
      "arguments": [
        "path_components"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.get_float32_matmul_precision",
      "signature": "torch.get_float32_matmul_precision() -> str",
      "doc": "Returns the current value of float32 matrix multiplication precision. Refer to\n    :func:`torch.set_float32_matmul_precision` documentation for more details.\n    ",
      "arguments": [],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the current value of float32 matrix multiplication precision. Refer to\n    :func:`torch.set_float32_matmul_precision` documentation for more details.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.get_rng_state",
      "signature": "torch.get_rng_state() -> torch.Tensor",
      "doc": "Returns the random number generator state as a `torch.ByteTensor`.\n\n    .. note:: The returned state is for the default generator on CPU only.\n\n    See also: :func:`torch.random.fork_rng`.\n    ",
      "arguments": [],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the random number generator state as a `torch.ByteTensor`.\n\n    .. note:: The returned state is for the default generator on CPU only.\n\n    See also: :func:`torch.random.fork_rng`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.initial_seed",
      "signature": "torch.initial_seed() -> int",
      "doc": "Returns the initial seed for generating random numbers as a\n    Python `long`.\n\n    .. note:: The returned seed is for the default generator on CPU only.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the initial seed for generating random numbers as a\n    Python `long`.\n\n    .. note:: The returned seed is for the default generator on CPU only.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.is_deterministic_algorithms_warn_only_enabled",
      "signature": "torch.is_deterministic_algorithms_warn_only_enabled() -> bool",
      "doc": "Returns True if the global deterministic flag is set to warn only.\n    Refer to :func:`torch.use_deterministic_algorithms` documentation for more\n    details.\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns True if the global deterministic flag is set to warn only.\n    Refer to :func:`torch.use_deterministic_algorithms` documentation for more\n    details.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.is_storage",
      "signature": "torch.is_storage(obj: Any, /) -> typing_extensions.TypeIs[typing.Union[ForwardRef('TypedStorage'), ForwardRef('UntypedStorage')]]",
      "doc": "Returns True if `obj` is a PyTorch storage object.\n\n    Args:\n        obj (Object): Object to test\n    ",
      "arguments": [
        "obj"
      ],
      "return_type": "typing_extensions.TypeIs[typing.Union[ForwardRef('TypedStorage'), ForwardRef('UntypedStorage')]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns True if `obj` is a PyTorch storage object.\n\n    Args:\n        obj (Object): Object to test\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.is_tensor",
      "signature": "torch.is_tensor(obj: Any, /) -> typing_extensions.TypeIs[ForwardRef('torch.Tensor')]",
      "doc": "Returns True if `obj` is a PyTorch tensor.\n\n    Note that this function is simply doing ``isinstance(obj, Tensor)``.\n    Using that ``isinstance`` check is better for typechecking with mypy,\n    and more explicit - so it's recommended to use that instead of\n    ``is_tensor``.\n\n    Args:\n        obj (object): Object to test\n    Example::\n\n        >>> x = torch.tensor([1, 2, 3])\n        >>> torch.is_tensor(x)\n        True\n\n    ",
      "arguments": [
        "obj"
      ],
      "return_type": "typing_extensions.TypeIs[ForwardRef('torch.Tensor')]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns True if `obj` is a PyTorch tensor.\n\n    Note that this function is simply doing ``isinstance(obj, Tensor)``.\n    Using that ``isinstance`` check is better for typechecking with mypy,\n    and more explicit - so it's recommended to use that instead of\n    ``is_tensor``.\n\n    Args:\n        obj (object): Object to test\n    Example::\n\n        >>> x = torch.tensor([1, 2, 3])\n        >>> torch.is_tensor(x)\n        True\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.is_warn_always_enabled",
      "signature": "torch.is_warn_always_enabled() -> bool",
      "doc": "Returns True if the global warn_always flag is turned on. Refer to\n    :func:`torch.set_warn_always` documentation for more details.\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns True if the global warn_always flag is turned on. Refer to\n    :func:`torch.set_warn_always` documentation for more details.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.load",
      "signature": "torch.load(f: Union[str, os.PathLike[str], IO[bytes]], map_location: Union[Callable[[torch.types.Storage, str], torch.types.Storage], torch.device, str, dict[str, str], NoneType] = None, pickle_module: Any = None, *, weights_only: Optional[bool] = None, mmap: Optional[bool] = None, **pickle_load_args: Any) -> Any",
      "doc": "load(f, map_location=None, pickle_module=pickle, *, weights_only=True, mmap=None, **pickle_load_args)\n\n    Loads an object saved with :func:`torch.save` from a file.\n\n    :func:`torch.load` uses Python's unpickling facilities but treats storages,\n    which underlie tensors, specially. They are first deserialized on the\n    CPU and are then moved to the device they were saved from. If this fails\n    (e.g. because the run time system doesn't have certain devices), an exception\n    is raised. However, storages can be dynamically remapped to an alternative\n    set of devices using the :attr:`map_location` argument.\n\n    If :attr:`map_location` is a callable, it will be called once for each serialized\n    storage with two arguments: storage and location. The storage argument\n    will be the initial deserialization of the storage, residing on the CPU.\n    Each serialized storage has a location tag associated with it which\n    identifies the device it was saved from, and this tag is the second\n    argument passed to :attr:`map_location`. The builtin location tags are ``'cpu'``\n    for CPU tensors and ``'cuda:device_id'`` (e.g. ``'cuda:2'``) for CUDA tensors.\n    :attr:`map_location` should return either ``None`` or a storage. If\n    :attr:`map_location` returns a storage, it will be used as the final deserialized\n    object, already moved to the right device. Otherwise, :func:`torch.load` will\n    fall back to the default behavior, as if :attr:`map_location` wasn't specified.\n\n    If :attr:`map_location` is a :class:`torch.device` object or a string containing\n    a device tag, it indicates the location where all tensors should be loaded.\n\n    Otherwise, if :attr:`map_location` is a dict, it will be used to remap location tags\n    appearing in the file (keys), to ones that specify where to put the\n    storages (values).\n\n    User extensions can register their own location tags and tagging and\n    deserialization methods using :func:`torch.serialization.register_package`.\n\n    Args:\n        f: a file-like object (has to implement :meth:`read`, :meth:`readline`, :meth:`tell`, and :meth:`seek`),\n            or a string or os.PathLike object containing a file name\n        map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\n            locations\n        pickle_module: module used for unpickling metadata and objects (has to\n            match the :attr:`pickle_module` used to serialize file)\n        weights_only: Indicates whether unpickler should be restricted to\n            loading only tensors, primitive types, dictionaries\n            and any types added via :func:`torch.serialization.add_safe_globals`.\n            See :ref:`weights-only` for more details.\n        mmap: Indicates whether the file should be mmaped rather than loading all the storages into memory.\n            Typically, tensor storages in the file will first be moved from disk to CPU memory, after which they\n            are moved to the location that they were tagged with when saving, or specified by ``map_location``. This\n            second step is a no-op if the final location is CPU. When the ``mmap`` flag is set, instead of copying the\n            tensor storages from disk to CPU memory in the first step, ``f`` is mmaped.\n        pickle_load_args: (Python 3 only) optional keyword arguments passed over to\n            :func:`pickle_module.load` and :func:`pickle_module.Unpickler`, e.g.,\n            :attr:`errors=...`.\n\n    .. warning::\n        :func:`torch.load()` unless `weights_only` parameter is set to `True`,\n        uses ``pickle`` module implicitly, which is known to be insecure.\n        It is possible to construct malicious pickle data which will execute arbitrary code\n        during unpickling. Never load data that could have come from an untrusted\n        source in an unsafe mode, or that could have been tampered with. **Only load data you trust**.\n\n    .. note::\n        When you call :func:`torch.load()` on a file which contains GPU tensors, those tensors\n        will be loaded to GPU by default. You can call ``torch.load(.., map_location='cpu')``\n        and then :meth:`load_state_dict` to avoid GPU RAM surge when loading a model checkpoint.\n\n    .. note::\n        By default, we decode byte strings as ``utf-8``.  This is to avoid a common error\n        case ``UnicodeDecodeError: 'ascii' codec can't decode byte 0x...``\n        when loading files saved by Python 2 in Python 3.  If this default\n        is incorrect, you may use an extra :attr:`encoding` keyword argument to specify how\n        these objects should be loaded, e.g., :attr:`encoding='latin1'` decodes them\n        to strings using ``latin1`` encoding, and :attr:`encoding='bytes'` keeps them\n        as byte arrays which can be decoded later with ``byte_array.decode(...)``.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"undefined filepaths\")\n        >>> torch.load(\"tensors.pt\", weights_only=True)\n        # Load all tensors onto the CPU\n        >>> torch.load(\n        ...     \"tensors.pt\",\n        ...     map_location=torch.device(\"cpu\"),\n        ...     weights_only=True,\n        ... )\n        # Load all tensors onto the CPU, using a function\n        >>> torch.load(\n        ...     \"tensors.pt\",\n        ...     map_location=lambda storage, loc: storage,\n        ...     weights_only=True,\n        ... )\n        # Load all tensors onto GPU 1\n        >>> torch.load(\n        ...     \"tensors.pt\",\n        ...     map_location=lambda storage, loc: storage.cuda(1),\n        ...     weights_only=True,\n        ... )  # type: ignore[attr-defined]\n        # Map tensors from GPU 1 to GPU 0\n        >>> torch.load(\n        ...     \"tensors.pt\",\n        ...     map_location={\"cuda:1\": \"cuda:0\"},\n        ...     weights_only=True,\n        ... )\n        # Load tensor from io.BytesIO object\n        # Loading from a buffer setting weights_only=False, warning this can be unsafe\n        >>> with open(\"tensor.pt\", \"rb\") as f:\n        ...     buffer = io.BytesIO(f.read())\n        >>> torch.load(buffer, weights_only=False)\n        # Load a module with 'ascii' encoding for unpickling\n        # Loading from a module setting weights_only=False, warning this can be unsafe\n        >>> torch.load(\"module.pt\", encoding=\"ascii\", weights_only=False)\n    ",
      "arguments": [
        "f",
        "map_location",
        "pickle_module",
        "weights_only",
        "mmap",
        "pickle_load_args"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "load(f, map_location=None, pickle_module=pickle, *, weights_only=True, mmap=None, **pickle_load_args)\n\n    Loads an object saved with :func:`torch.save` from a file.\n\n    :func:`torch.load` uses Python's unpickling facilities but treats storages,\n    which underlie tensors, specially. They are first deserialized on the\n    CPU and are then moved to the device they were saved from. If this fails\n    (e.g. because the run time system doesn't have certain devices), an exception\n    is raised. However, storages can be dynamically remapped to an alternative\n    set of devices using the :attr:`map_location` argument.\n\n    If :attr:`map_location` is a callable, it will be called once for each serialized\n    storage with two arguments: storage and location. The storage argument\n    will be the initial deserialization of the storage, residing on the CPU.\n    Each serialized storage has a location tag associated with it which\n    identifies the device it was saved from, and this tag is the second\n    argument passed to :attr:`map_location`. The builtin location tags are ``'cpu'``\n    for CPU tensors and ``'cuda:device_id'`` (e.g. ``'cuda:2'``) for CUDA tensors.\n    :attr:`map_location` should return either ``None`` or a storage. If\n    :attr:`map_location` returns a storage, it will be used as the final deserialized\n    object, already moved to the right device. Otherwise, :func:`torch.load` will\n    fall back to the default behavior, as if :attr:`map_location` wasn't specified.\n\n    If :attr:`map_location` is a :class:`torch.device` object or a string containing\n    a device tag, it indicates the location where all tensors should be loaded.\n\n    Otherwise, if :attr:`map_location` is a dict, it will be used to remap location tags\n    appearing in the file (keys), to ones that specify where to put the\n    storages (values).\n\n    User extensions can register their own location tags and tagging and\n    deserialization methods using :func:`torch.serialization.register_package`.\n\n    Args:\n        f: a file-like object (has to implement :meth:`read`, :meth:`readline`, :meth:`tell`, and :meth:`seek`),\n            or a string or os.PathLike object containing a file name\n        map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\n            locations\n        pickle_module: module used for unpickling metadata and objects (has to\n            match the :attr:`pickle_module` used to serialize file)\n        weights_only: Indicates whether unpickler should be restricted to\n            loading only tensors, primitive types, dictionaries\n            and any types added via :func:`torch.serialization.add_safe_globals`.\n            See :ref:`weights-only` for more details.\n        mmap: Indicates whether the file should be mmaped rather than loading all the storages into memory.\n            Typically, tensor storages in the file will first be moved from disk to CPU memory, after which they\n            are moved to the location that they were tagged with when saving, or specified by ``map_location``. This\n            second step is a no-op if the final location is CPU. When the ``mmap`` flag is set, instead of copying the\n            tensor storages from disk to CPU memory in the first step, ``f`` is mmaped.\n        pickle_load_args: (Python 3 only) optional keyword arguments passed over to\n            :func:`pickle_module.load` and :func:`pickle_module.Unpickler`, e.g.,\n            :attr:`errors=...`.\n\n    .. warning::\n        :func:`torch.load()` unless `weights_only` parameter is set to `True`,\n        uses ``pickle`` module implicitly, which is known to be insecure.\n        It is possible to construct malicious pickle data which will execute arbitrary code\n        during unpickling. Never load data that could have come from an untrusted\n        source in an unsafe mode, or that could have been tampered with. **Only load data you trust**.\n\n    .. note::\n        When you call :func:`torch.load()` on a file which contains GPU tensors, those tensors\n        will be loaded to GPU by default. You can call ``torch.load(.., map_location='cpu')``\n        and then :meth:`load_state_dict` to avoid GPU RAM surge when loading a model checkpoint.\n\n    .. note::\n        By default, we decode byte strings as ``utf-8``.  This is to avoid a common error\n        case ``UnicodeDecodeError: 'ascii' codec can't decode byte 0x...``\n        when loading files saved by Python 2 in Python 3.  If this default\n        is incorrect, you may use an extra :attr:`encoding` keyword argument to specify how\n        these objects should be loaded, e.g., :attr:`encoding='latin1'` decodes them\n        to strings using ``latin1`` encoding, and :attr:`encoding='bytes'` keeps them\n        as byte arrays which can be decoded later with ``byte_array.decode(...)``.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"undefined filepaths\")\n        >>> torch.load(\"tensors.pt\", weights_only=True)\n        # Load all tensors onto the CPU\n        >>> torch.load(\n        ...     \"tensors.pt\",\n        ...     map_location=torch.device(\"cpu\"),\n        ...     weights_only=True,\n        ... )\n        # Load all tensors onto the CPU, using a function\n        >>> torch.load(\n        ...     \"tensors.pt\",\n        ...     map_location=lambda storage, loc: storage,\n        ...     weights_only=True,\n        ... )\n        # Load all tensors onto GPU 1\n        >>> torch.load(\n        ...     \"tensors.pt\",\n        ...     map_location=lambda storage, loc: storage.cuda(1),\n        ...     weights_only=True,\n        ... )  # type: ignore[attr-defined]\n        # Map tensors from GPU 1 to GPU 0\n        >>> torch.load(\n        ...     \"tensors.pt\",\n        ...     map_location={\"cuda:1\": \"cuda:0\"},\n        ...     weights_only=True,\n        ... )\n        # Load tensor from io.BytesIO object\n        # Loading from a buffer setting weights_only=False, warning this can be unsafe\n        >>> with open(\"tensor.pt\", \"rb\") as f:\n        ...     buffer = io.BytesIO(f.read())\n        >>> torch.load(buffer, weights_only=False)\n        # Load a module with 'ascii' encoding for unpickling\n        # Loading from a module setting weights_only=False, warning this can be unsafe\n        >>> torch.load(\"module.pt\", encoding=\"ascii\", weights_only=False)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.lobpcg",
      "signature": "torch.lobpcg(A: torch.Tensor, k: Optional[int] = None, B: Optional[torch.Tensor] = None, X: Optional[torch.Tensor] = None, n: Optional[int] = None, iK: Optional[torch.Tensor] = None, niter: Optional[int] = None, tol: Optional[float] = None, largest: Optional[bool] = None, method: Optional[str] = None, tracker: None = None, ortho_iparams: Optional[dict[str, int]] = None, ortho_fparams: Optional[dict[str, float]] = None, ortho_bparams: Optional[dict[str, bool]] = None) -> tuple[torch.Tensor, torch.Tensor]",
      "doc": "Find the k largest (or smallest) eigenvalues and the corresponding\n    eigenvectors of a symmetric positive definite generalized\n    eigenvalue problem using matrix-free LOBPCG methods.\n\n    This function is a front-end to the following LOBPCG algorithms\n    selectable via `method` argument:\n\n      `method=\"basic\"` - the LOBPCG method introduced by Andrew\n      Knyazev, see [Knyazev2001]. A less robust method, may fail when\n      Cholesky is applied to singular input.\n\n      `method=\"ortho\"` - the LOBPCG method with orthogonal basis\n      selection [StathopoulosEtal2002]. A robust method.\n\n    Supported inputs are dense, sparse, and batches of dense matrices.\n\n    .. note:: In general, the basic method spends least time per\n      iteration. However, the robust methods converge much faster and\n      are more stable. So, the usage of the basic method is generally\n      not recommended but there exist cases where the usage of the\n      basic method may be preferred.\n\n    .. warning:: The backward method does not support sparse and complex inputs.\n      It works only when `B` is not provided (i.e. `B == None`).\n      We are actively working on extensions, and the details of\n      the algorithms are going to be published promptly.\n\n    .. warning:: While it is assumed that `A` is symmetric, `A.grad` is not.\n      To make sure that `A.grad` is symmetric, so that `A - t * A.grad` is symmetric\n      in first-order optimization routines, prior to running `lobpcg`\n      we do the following symmetrization map: `A -> (A + A.t()) / 2`.\n      The map is performed only when the `A` requires gradients.\n\n    Args:\n\n      A (Tensor): the input tensor of size :math:`(*, m, m)`\n\n      B (Tensor, optional): the input tensor of size :math:`(*, m,\n                  m)`. When not specified, `B` is interpreted as\n                  identity matrix.\n\n      X (tensor, optional): the input tensor of size :math:`(*, m, n)`\n                  where `k <= n <= m`. When specified, it is used as\n                  initial approximation of eigenvectors. X must be a\n                  dense tensor.\n\n      iK (tensor, optional): the input tensor of size :math:`(*, m,\n                  m)`. When specified, it will be used as preconditioner.\n\n      k (integer, optional): the number of requested\n                  eigenpairs. Default is the number of :math:`X`\n                  columns (when specified) or `1`.\n\n      n (integer, optional): if :math:`X` is not specified then `n`\n                  specifies the size of the generated random\n                  approximation of eigenvectors. Default value for `n`\n                  is `k`. If :math:`X` is specified, the value of `n`\n                  (when specified) must be the number of :math:`X`\n                  columns.\n\n      tol (float, optional): residual tolerance for stopping\n                 criterion. Default is `feps ** 0.5` where `feps` is\n                 smallest non-zero floating-point number of the given\n                 input tensor `A` data type.\n\n      largest (bool, optional): when True, solve the eigenproblem for\n                 the largest eigenvalues. Otherwise, solve the\n                 eigenproblem for smallest eigenvalues. Default is\n                 `True`.\n\n      method (str, optional): select LOBPCG method. See the\n                 description of the function above. Default is\n                 \"ortho\".\n\n      niter (int, optional): maximum number of iterations. When\n                 reached, the iteration process is hard-stopped and\n                 the current approximation of eigenpairs is returned.\n                 For infinite iteration but until convergence criteria\n                 is met, use `-1`.\n\n      tracker (callable, optional) : a function for tracing the\n                 iteration process. When specified, it is called at\n                 each iteration step with LOBPCG instance as an\n                 argument. The LOBPCG instance holds the full state of\n                 the iteration process in the following attributes:\n\n                   `iparams`, `fparams`, `bparams` - dictionaries of\n                   integer, float, and boolean valued input\n                   parameters, respectively\n\n                   `ivars`, `fvars`, `bvars`, `tvars` - dictionaries\n                   of integer, float, boolean, and Tensor valued\n                   iteration variables, respectively.\n\n                   `A`, `B`, `iK` - input Tensor arguments.\n\n                   `E`, `X`, `S`, `R` - iteration Tensor variables.\n\n                 For instance:\n\n                   `ivars[\"istep\"]` - the current iteration step\n                   `X` - the current approximation of eigenvectors\n                   `E` - the current approximation of eigenvalues\n                   `R` - the current residual\n                   `ivars[\"converged_count\"]` - the current number of converged eigenpairs\n                   `tvars[\"rerr\"]` - the current state of convergence criteria\n\n                 Note that when `tracker` stores Tensor objects from\n                 the LOBPCG instance, it must make copies of these.\n\n                 If `tracker` sets `bvars[\"force_stop\"] = True`, the\n                 iteration process will be hard-stopped.\n\n      ortho_iparams, ortho_fparams, ortho_bparams (dict, optional):\n                 various parameters to LOBPCG algorithm when using\n                 `method=\"ortho\"`.\n\n    Returns:\n\n      E (Tensor): tensor of eigenvalues of size :math:`(*, k)`\n\n      X (Tensor): tensor of eigenvectors of size :math:`(*, m, k)`\n\n    References:\n\n      [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\n      Preconditioned Eigensolver: Locally Optimal Block Preconditioned\n      Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n      517-541. (25 pages)\n      https://epubs.siam.org/doi/abs/10.1137/S1064827500366124\n\n      [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\n      Wu. (2002) A Block Orthogonalization Procedure with Constant\n      Synchronization Requirements. SIAM J. Sci. Comput., 23(6),\n      2165-2182. (18 pages)\n      https://epubs.siam.org/doi/10.1137/S1064827500370883\n\n      [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\n      Gu. (2018) A Robust and Efficient Implementation of LOBPCG.\n      SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)\n      https://epubs.siam.org/doi/abs/10.1137/17M1129830\n\n    ",
      "arguments": [
        "A",
        "k",
        "B",
        "X",
        "n",
        "iK",
        "niter",
        "tol",
        "largest",
        "method",
        "tracker",
        "ortho_iparams",
        "ortho_fparams",
        "ortho_bparams"
      ],
      "return_type": "tuple[torch.Tensor, torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Find the k largest (or smallest) eigenvalues and the corresponding\n    eigenvectors of a symmetric positive definite generalized\n    eigenvalue problem using matrix-free LOBPCG methods.\n\n    This function is a front-end to the following LOBPCG algorithms\n    selectable via `method` argument:\n\n      `method=\"basic\"` - the LOBPCG method introduced by Andrew\n      Knyazev, see [Knyazev2001]. A less robust method, may fail when\n      Cholesky is applied to singular input.\n\n      `method=\"ortho\"` - the LOBPCG method with orthogonal basis\n      selection [StathopoulosEtal2002]. A robust method.\n\n    Supported inputs are dense, sparse, and batches of dense matrices.\n\n    .. note:: In general, the basic method spends least time per\n      iteration. However, the robust methods converge much faster and\n      are more stable. So, the usage of the basic method is generally\n      not recommended but there exist cases where the usage of the\n      basic method may be preferred.\n\n    .. warning:: The backward method does not support sparse and complex inputs.\n      It works only when `B` is not provided (i.e. `B == None`).\n      We are actively working on extensions, and the details of\n      the algorithms are going to be published promptly.\n\n    .. warning:: While it is assumed that `A` is symmetric, `A.grad` is not.\n      To make sure that `A.grad` is symmetric, so that `A - t * A.grad` is symmetric\n      in first-order optimization routines, prior to running `lobpcg`\n      we do the following symmetrization map: `A -> (A + A.t()) / 2`.\n      The map is performed only when the `A` requires gradients.\n\n    Args:\n\n      A (Tensor): the input tensor of size :math:`(*, m, m)`\n\n      B (Tensor, optional): the input tensor of size :math:`(*, m,\n                  m)`. When not specified, `B` is interpreted as\n                  identity matrix.\n\n      X (tensor, optional): the input tensor of size :math:`(*, m, n)`\n                  where `k <= n <= m`. When specified, it is used as\n                  initial approximation of eigenvectors. X must be a\n                  dense tensor.\n\n      iK (tensor, optional): the input tensor of size :math:`(*, m,\n                  m)`. When specified, it will be used as preconditioner.\n\n      k (integer, optional): the number of requested\n                  eigenpairs. Default is the number of :math:`X`\n                  columns (when specified) or `1`.\n\n      n (integer, optional): if :math:`X` is not specified then `n`\n                  specifies the size of the generated random\n                  approximation of eigenvectors. Default value for `n`\n                  is `k`. If :math:`X` is specified, the value of `n`\n                  (when specified) must be the number of :math:`X`\n                  columns.\n\n      tol (float, optional): residual tolerance for stopping\n                 criterion. Default is `feps ** 0.5` where `feps` is\n                 smallest non-zero floating-point number of the given\n                 input tensor `A` data type.\n\n      largest (bool, optional): when True, solve the eigenproblem for\n                 the largest eigenvalues. Otherwise, solve the\n                 eigenproblem for smallest eigenvalues. Default is\n                 `True`.\n\n      method (str, optional): select LOBPCG method. See the\n                 description of the function above. Default is\n                 \"ortho\".\n\n      niter (int, optional): maximum number of iterations. When\n                 reached, the iteration process is hard-stopped and\n                 the current approximation of eigenpairs is returned.\n                 For infinite iteration but until convergence criteria\n                 is met, use `-1`.\n\n      tracker (callable, optional) : a function for tracing the\n                 iteration process. When specified, it is called at\n                 each iteration step with LOBPCG instance as an\n                 argument. The LOBPCG instance holds the full state of\n                 the iteration process in the following attributes:\n\n                   `iparams`, `fparams`, `bparams` - dictionaries of\n                   integer, float, and boolean valued input\n                   parameters, respectively\n\n                   `ivars`, `fvars`, `bvars`, `tvars` - dictionaries\n                   of integer, float, boolean, and Tensor valued\n                   iteration variables, respectively.\n\n                   `A`, `B`, `iK` - input Tensor arguments.\n\n                   `E`, `X`, `S`, `R` - iteration Tensor variables.\n\n                 For instance:\n\n                   `ivars[\"istep\"]` - the current iteration step\n                   `X` - the current approximation of eigenvectors\n                   `E` - the current approximation of eigenvalues\n                   `R` - the current residual\n                   `ivars[\"converged_count\"]` - the current number of converged eigenpairs\n                   `tvars[\"rerr\"]` - the current state of convergence criteria\n\n                 Note that when `tracker` stores Tensor objects from\n                 the LOBPCG instance, it must make copies of these.\n\n                 If `tracker` sets `bvars[\"force_stop\"] = True`, the\n                 iteration process will be hard-stopped.\n\n      ortho_iparams, ortho_fparams, ortho_bparams (dict, optional):\n                 various parameters to LOBPCG algorithm when using\n                 `method=\"ortho\"`.\n\n    Returns:\n\n      E (Tensor): tensor of eigenvalues of size :math:`(*, k)`\n\n      X (Tensor): tensor of eigenvectors of size :math:`(*, m, k)`\n\n    References:\n\n      [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\n      Preconditioned Eigensolver: Locally Optimal Block Preconditioned\n      Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n      517-541. (25 pages)\n      https://epubs.siam.org/doi/abs/10.1137/S1064827500366124\n\n      [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\n      Wu. (2002) A Block Orthogonalization Procedure with Constant\n      Synchronization Requirements. SIAM J. Sci. Comput., 23(6),\n      2165-2182. (18 pages)\n      https://epubs.siam.org/doi/10.1137/S1064827500370883\n\n      [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\n      Gu. (2018) A Robust and Efficient Implementation of LOBPCG.\n      SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)\n      https://epubs.siam.org/doi/abs/10.1137/17M1129830\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.lstsq",
      "signature": "torch.lstsq(input: torch.Tensor, A: torch.Tensor, *, out=None) -> tuple[torch.Tensor, torch.Tensor]",
      "doc": "",
      "arguments": [
        "input",
        "A",
        "out"
      ],
      "return_type": "tuple[torch.Tensor, torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.lu",
      "signature": "torch.lu(*args, **kwargs)",
      "doc": "Computes the LU factorization of a matrix or batches of matrices\n    :attr:`A`. Returns a tuple containing the LU factorization and\n    pivots of :attr:`A`.  Pivoting is done if :attr:`pivot` is set to\n    ``True``.\n\n    .. warning::\n\n        :func:`torch.lu` is deprecated in favor of :func:`torch.linalg.lu_factor`\n        and :func:`torch.linalg.lu_factor_ex`. :func:`torch.lu` will be removed in a\n        future PyTorch release.\n        ``LU, pivots, info = torch.lu(A, compute_pivots)`` should be replaced with\n\n        .. code:: python\n\n            LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n\n        ``LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)`` should be replaced with\n\n        .. code:: python\n\n            LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots)\n\n    .. note::\n        * The returned permutation matrix for every matrix in the batch is\n          represented by a 1-indexed vector of size ``min(A.shape[-2], A.shape[-1])``.\n          ``pivots[i] == j`` represents that in the ``i``-th step of the algorithm,\n          the ``i``-th row was permuted with the ``j-1``-th row.\n        * LU factorization with :attr:`pivot` = ``False`` is not available\n          for CPU, and attempting to do so will throw an error. However,\n          LU factorization with :attr:`pivot` = ``False`` is available for\n          CUDA.\n        * This function does not check if the factorization was successful\n          or not if :attr:`get_infos` is ``True`` since the status of the\n          factorization is present in the third element of the return tuple.\n        * In the case of batches of square matrices with size less or equal\n          to 32 on a CUDA device, the LU factorization is repeated for\n          singular matrices due to the bug in the MAGMA library\n          (see magma issue 13).\n        * ``L``, ``U``, and ``P`` can be derived using :func:`torch.lu_unpack`.\n\n    .. warning::\n        The gradients of this function will only be finite when :attr:`A` is full rank.\n        This is because the LU decomposition is just differentiable at full rank matrices.\n        Furthermore, if :attr:`A` is close to not being full rank,\n        the gradient will be numerically unstable as it depends on the computation of :math:`L^{-1}` and :math:`U^{-1}`.\n\n    Args:\n        A (Tensor): the tensor to factor of size :math:`(*, m, n)`\n        pivot (bool, optional): controls whether pivoting is done. Default: ``True``\n        get_infos (bool, optional): if set to ``True``, returns an info IntTensor.\n                                    Default: ``False``\n        out (tuple, optional): optional output tuple. If :attr:`get_infos` is ``True``,\n                               then the elements in the tuple are Tensor, IntTensor,\n                               and IntTensor. If :attr:`get_infos` is ``False``, then the\n                               elements in the tuple are Tensor, IntTensor. Default: ``None``\n\n    Returns:\n        (Tensor, IntTensor, IntTensor (optional)): A tuple of tensors containing\n\n            - **factorization** (*Tensor*): the factorization of size :math:`(*, m, n)`\n\n            - **pivots** (*IntTensor*): the pivots of size :math:`(*, \\text{min}(m, n))`.\n              ``pivots`` stores all the intermediate transpositions of rows.\n              The final permutation ``perm`` could be reconstructed by\n              applying ``swap(perm[i], perm[pivots[i] - 1])`` for ``i = 0, ..., pivots.size(-1) - 1``,\n              where ``perm`` is initially the identity permutation of :math:`m` elements\n              (essentially this is what :func:`torch.lu_unpack` is doing).\n\n            - **infos** (*IntTensor*, *optional*): if :attr:`get_infos` is ``True``, this is a tensor of\n              size :math:`(*)` where non-zero values indicate whether factorization for the matrix or\n              each minibatch has succeeded or failed\n\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK)\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> A = torch.randn(2, 3, 3)\n        >>> A_LU, pivots = torch.lu(A)\n        >>> A_LU\n        tensor([[[ 1.3506,  2.5558, -0.0816],\n                 [ 0.1684,  1.1551,  0.1940],\n                 [ 0.1193,  0.6189, -0.5497]],\n\n                [[ 0.4526,  1.2526, -0.3285],\n                 [-0.7988,  0.7175, -0.9701],\n                 [ 0.2634, -0.9255, -0.3459]]])\n        >>> pivots\n        tensor([[ 3,  3,  3],\n                [ 3,  3,  3]], dtype=torch.int32)\n        >>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n        >>> if info.nonzero().size(0) == 0:\n        ...     print('LU factorization succeeded for all samples!')\n        LU factorization succeeded for all samples!\n    ",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Computes the LU factorization of a matrix or batches of matrices\n    :attr:`A`. Returns a tuple containing the LU factorization and\n    pivots of :attr:`A`.  Pivoting is done if :attr:`pivot` is set to\n    ``True``.\n\n    .. warning::\n\n        :func:`torch.lu` is deprecated in favor of :func:`torch.linalg.lu_factor`\n        and :func:`torch.linalg.lu_factor_ex`. :func:`torch.lu` will be removed in a\n        future PyTorch release.\n        ``LU, pivots, info = torch.lu(A, compute_pivots)`` should be replaced with\n\n        .. code:: python\n\n            LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n\n        ``LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)`` should be replaced with\n\n        .. code:: python\n\n            LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots)\n\n    .. note::\n        * The returned permutation matrix for every matrix in the batch is\n          represented by a 1-indexed vector of size ``min(A.shape[-2], A.shape[-1])``.\n          ``pivots[i] == j`` represents that in the ``i``-th step of the algorithm,\n          the ``i``-th row was permuted with the ``j-1``-th row.\n        * LU factorization with :attr:`pivot` = ``False`` is not available\n          for CPU, and attempting to do so will throw an error. However,\n          LU factorization with :attr:`pivot` = ``False`` is available for\n          CUDA.\n        * This function does not check if the factorization was successful\n          or not if :attr:`get_infos` is ``True`` since the status of the\n          factorization is present in the third element of the return tuple.\n        * In the case of batches of square matrices with size less or equal\n          to 32 on a CUDA device, the LU factorization is repeated for\n          singular matrices due to the bug in the MAGMA library\n          (see magma issue 13).\n        * ``L``, ``U``, and ``P`` can be derived using :func:`torch.lu_unpack`.\n\n    .. warning::\n        The gradients of this function will only be finite when :attr:`A` is full rank.\n        This is because the LU decomposition is just differentiable at full rank matrices.\n        Furthermore, if :attr:`A` is close to not being full rank,\n        the gradient will be numerically unstable as it depends on the computation of :math:`L^{-1}` and :math:`U^{-1}`.\n\n    Args:\n        A (Tensor): the tensor to factor of size :math:`(*, m, n)`\n        pivot (bool, optional): controls whether pivoting is done. Default: ``True``\n        get_infos (bool, optional): if set to ``True``, returns an info IntTensor.\n                                    Default: ``False``\n        out (tuple, optional): optional output tuple. If :attr:`get_infos` is ``True``,\n                               then the elements in the tuple are Tensor, IntTensor,\n                               and IntTensor. If :attr:`get_infos` is ``False``, then the\n                               elements in the tuple are Tensor, IntTensor. Default: ``None``\n\n    Returns:\n        (Tensor, IntTensor, IntTensor (optional)): A tuple of tensors containing\n\n            - **factorization** (*Tensor*): the factorization of size :math:`(*, m, n)`\n\n            - **pivots** (*IntTensor*): the pivots of size :math:`(*, \\text{min}(m, n))`.\n              ``pivots`` stores all the intermediate transpositions of rows.\n              The final permutation ``perm`` could be reconstructed by\n              applying ``swap(perm[i], perm[pivots[i] - 1])`` for ``i = 0, ..., pivots.size(-1) - 1``,\n              where ``perm`` is initially the identity permutation of :math:`m` elements\n              (essentially this is what :func:`torch.lu_unpack` is doing).\n\n            - **infos** (*IntTensor*, *optional*): if :attr:`get_infos` is ``True``, this is a tensor of\n              size :math:`(*)` where non-zero values indicate whether factorization for the matrix or\n              each minibatch has succeeded or failed\n\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK)\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> A = torch.randn(2, 3, 3)\n        >>> A_LU, pivots = torch.lu(A)\n        >>> A_LU\n        tensor([[[ 1.3506,  2.5558, -0.0816],\n                 [ 0.1684,  1.1551,  0.1940],\n                 [ 0.1193,  0.6189, -0.5497]],\n\n                [[ 0.4526,  1.2526, -0.3285],\n                 [-0.7988,  0.7175, -0.9701],\n                 [ 0.2634, -0.9255, -0.3459]]])\n        >>> pivots\n        tensor([[ 3,  3,  3],\n                [ 3,  3,  3]], dtype=torch.int32)\n        >>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n        >>> if info.nonzero().size(0) == 0:\n        ...     print('LU factorization succeeded for all samples!')\n        LU factorization succeeded for all samples!\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.manual_seed",
      "signature": "torch.manual_seed(seed) -> torch._C.Generator",
      "doc": "Sets the seed for generating random numbers on all devices. Returns a\n    `torch.Generator` object.\n\n    Args:\n        seed (int): The desired seed. Value must be within the inclusive range\n            `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\n            is raised. Negative inputs are remapped to positive values with the formula\n            `0xffff_ffff_ffff_ffff + seed`.\n    ",
      "arguments": [
        "seed"
      ],
      "return_type": "<class 'torch._C.Generator'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the seed for generating random numbers on all devices. Returns a\n    `torch.Generator` object.\n\n    Args:\n        seed (int): The desired seed. Value must be within the inclusive range\n            `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\n            is raised. Negative inputs are remapped to positive values with the formula\n            `0xffff_ffff_ffff_ffff + seed`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.matrix_rank",
      "signature": "torch.matrix_rank(input, tol=None, symmetric=False, *, out=None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "tol",
        "symmetric",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.meshgrid",
      "signature": "torch.meshgrid(*tensors, indexing: Optional[str] = None) -> tuple[torch.Tensor, ...]",
      "doc": "Creates grids of coordinates specified by the 1D inputs in `attr`:tensors.\n\n        This is helpful when you want to visualize data over some\n        range of inputs. See below for a plotting example.\n\n        Given :math:`N` 1D tensors :math:`T_0 \\ldots T_{N-1}` as\n        inputs with corresponding sizes :math:`S_0 \\ldots S_{N-1}`,\n        this creates :math:`N` N-dimensional tensors :math:`G_0 \\ldots\n        G_{N-1}`, each with shape :math:`(S_0, ..., S_{N-1})` where\n        the output :math:`G_i` is constructed by expanding :math:`T_i`\n        to the result shape.\n\n        .. note::\n            0D inputs are treated equivalently to 1D inputs of a\n            single element.\n\n        .. warning::\n            `torch.meshgrid(*tensors)` currently has the same behavior\n            as calling `numpy.meshgrid(*arrays, indexing='ij')`.\n\n            In the future `torch.meshgrid` will transition to\n            `indexing='xy'` as the default.\n\n            https://github.com/pytorch/pytorch/issues/50276 tracks\n            this issue with the goal of migrating to NumPy's behavior.\n\n        .. seealso::\n\n            :func:`torch.cartesian_prod` has the same effect but it\n            collects the data in a tensor of vectors.\n\n        Args:\n            tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be\n                treated as tensors of size :math:`(1,)` automatically\n\n            indexing: (str, optional): the indexing mode, either \"xy\"\n                or \"ij\", defaults to \"ij\". See warning for future changes.\n\n                If \"xy\" is selected, the first dimension corresponds\n                to the cardinality of the second input and the second\n                dimension corresponds to the cardinality of the first\n                input.\n\n                If \"ij\" is selected, the dimensions are in the same\n                order as the cardinality of the inputs.\n\n        Returns:\n            seq (sequence of Tensors): If the input has :math:`N`\n            tensors of size :math:`S_0 \\ldots S_{N-1}``, then the\n            output will also have :math:`N` tensors, where each tensor\n            is of shape :math:`(S_0, ..., S_{N-1})`.\n\n        Example::\n\n            >>> x = torch.tensor([1, 2, 3])\n            >>> y = torch.tensor([4, 5, 6])\n\n            Observe the element-wise pairings across the grid, (1, 4),\n            (1, 5), ..., (3, 6). This is the same thing as the\n            cartesian product.\n            >>> grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n            >>> grid_x\n            tensor([[1, 1, 1],\n                    [2, 2, 2],\n                    [3, 3, 3]])\n            >>> grid_y\n            tensor([[4, 5, 6],\n                    [4, 5, 6],\n                    [4, 5, 6]])\n\n            This correspondence can be seen when these grids are\n            stacked properly.\n            >>> torch.equal(torch.cat(tuple(torch.dstack([grid_x, grid_y]))),\n            ...             torch.cartesian_prod(x, y))\n            True\n\n            `torch.meshgrid` is commonly used to produce a grid for\n            plotting.\n            >>> # xdoctest: +REQUIRES(module:matplotlib)\n            >>> # xdoctest: +REQUIRES(env:DOCTEST_SHOW)\n            >>> import matplotlib.pyplot as plt\n            >>> xs = torch.linspace(-5, 5, steps=100)\n            >>> ys = torch.linspace(-5, 5, steps=100)\n            >>> x, y = torch.meshgrid(xs, ys, indexing='xy')\n            >>> z = torch.sin(torch.sqrt(x * x + y * y))\n            >>> ax = plt.axes(projection='3d')\n            >>> ax.plot_surface(x.numpy(), y.numpy(), z.numpy())\n            >>> plt.show()\n\n        .. image:: ../_static/img/meshgrid.png\n            :width: 512\n\n        ",
      "arguments": [
        "tensors",
        "indexing"
      ],
      "return_type": "tuple[torch.Tensor, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Creates grids of coordinates specified by the 1D inputs in `attr`:tensors.\n\n        This is helpful when you want to visualize data over some\n        range of inputs. See below for a plotting example.\n\n        Given :math:`N` 1D tensors :math:`T_0 \\ldots T_{N-1}` as\n        inputs with corresponding sizes :math:`S_0 \\ldots S_{N-1}`,\n        this creates :math:`N` N-dimensional tensors :math:`G_0 \\ldots\n        G_{N-1}`, each with shape :math:`(S_0, ..., S_{N-1})` where\n        the output :math:`G_i` is constructed by expanding :math:`T_i`\n        to the result shape.\n\n        .. note::\n            0D inputs are treated equivalently to 1D inputs of a\n            single element.\n\n        .. warning::\n            `torch.meshgrid(*tensors)` currently has the same behavior\n            as calling `numpy.meshgrid(*arrays, indexing='ij')`.\n\n            In the future `torch.meshgrid` will transition to\n            `indexing='xy'` as the default.\n\n            https://github.com/pytorch/pytorch/issues/50276 tracks\n            this issue with the goal of migrating to NumPy's behavior.\n\n        .. seealso::\n\n            :func:`torch.cartesian_prod` has the same effect but it\n            collects the data in a tensor of vectors.\n\n        Args:\n            tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be\n                treated as tensors of size :math:`(1,)` automatically\n\n            indexing: (str, optional): the indexing mode, either \"xy\"\n                or \"ij\", defaults to \"ij\". See warning for future changes.\n\n                If \"xy\" is selected, the first dimension corresponds\n                to the cardinality of the second input and the second\n                dimension corresponds to the cardinality of the first\n                input.\n\n                If \"ij\" is selected, the dimensions are in the same\n                order as the cardinality of the inputs.\n\n        Returns:\n            seq (sequence of Tensors): If the input has :math:`N`\n            tensors of size :math:`S_0 \\ldots S_{N-1}``, then the\n            output will also have :math:`N` tensors, where each tensor\n            is of shape :math:`(S_0, ..., S_{N-1})`.\n\n        Example::\n\n            >>> x = torch.tensor([1, 2, 3])\n            >>> y = torch.tensor([4, 5, 6])\n\n            Observe the element-wise pairings across the grid, (1, 4),\n            (1, 5), ..., (3, 6). This is the same thing as the\n            cartesian product.\n            >>> grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n            >>> grid_x\n            tensor([[1, 1, 1],\n                    [2, 2, 2],\n                    [3, 3, 3]])\n            >>> grid_y\n            tensor([[4, 5, 6],\n                    [4, 5, 6],\n                    [4, 5, 6]])\n\n            This correspondence can be seen when these grids are\n            stacked properly.\n            >>> torch.equal(torch.cat(tuple(torch.dstack([grid_x, grid_y]))),\n            ...             torch.cartesian_prod(x, y))\n            True\n\n            `torch.meshgrid` is commonly used to produce a grid for\n            plotting.\n            >>> # xdoctest: +REQUIRES(module:matplotlib)\n            >>> # xdoctest: +REQUIRES(env:DOCTEST_SHOW)\n            >>> import matplotlib.pyplot as plt\n            >>> xs = torch.linspace(-5, 5, steps=100)\n            >>> ys = torch.linspace(-5, 5, steps=100)\n            >>> x, y = torch.meshgrid(xs, ys, indexing='xy')\n            >>> z = torch.sin(torch.sqrt(x * x + y * y))\n            >>> ax = plt.axes(projection='3d')\n            >>> ax.plot_surface(x.numpy(), y.numpy(), z.numpy())\n            >>> plt.show()\n\n        .. image:: ../_static/img/meshgrid.png\n            :width: 512\n\n        ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.norm",
      "signature": "torch.norm(input, p: Union[float, str, NoneType] = 'fro', dim=None, keepdim=False, out=None, dtype=None)",
      "doc": "Returns the matrix norm or vector norm of a given tensor.\n\n    .. warning::\n\n        torch.norm is deprecated and may be removed in a future PyTorch release.\n        Its documentation and behavior may be incorrect, and it is no longer\n        actively maintained.\n\n        Use :func:`torch.linalg.vector_norm` when computing vector norms and\n        :func:`torch.linalg.matrix_norm` when computing matrix norms.\n        For a function with a similar behavior as this one see :func:`torch.linalg.norm`.\n        Note, however, the signature for these functions is slightly different than the\n        signature for ``torch.norm``.\n\n    Args:\n        input (Tensor): The input tensor. Its data type must be either a floating\n            point or complex type. For complex inputs, the norm is calculated using the\n            absolute value of each element. If the input is complex and neither\n            :attr:`dtype` nor :attr:`out` is specified, the result's data type will\n            be the corresponding floating point type (e.g. float if :attr:`input` is\n            complexfloat).\n\n        p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``\n            The following norms can be calculated:\n\n            ======  ==============  ==========================\n            ord     matrix norm     vector norm\n            ======  ==============  ==========================\n            'fro'   Frobenius norm  --\n            'nuc'   nuclear norm    --\n            Number  --              sum(abs(x)**ord)**(1./ord)\n            ======  ==============  ==========================\n\n            The vector norm can be calculated across any number of dimensions.\n            The corresponding dimensions of :attr:`input` are flattened into\n            one dimension, and the norm is calculated on the flattened\n            dimension.\n\n            Frobenius norm produces the same result as ``p=2`` in all cases\n            except when :attr:`dim` is a list of three or more dims, in which\n            case Frobenius norm throws an error.\n\n            Nuclear norm can only be calculated across exactly two dimensions.\n\n        dim (int, tuple of ints, list of ints, optional):\n            Specifies which dimension or dimensions of :attr:`input` to\n            calculate the norm across. If :attr:`dim` is ``None``, the norm will\n            be calculated across all dimensions of :attr:`input`. If the norm\n            type indicated by :attr:`p` does not support the specified number of\n            dimensions, an error will occur.\n        keepdim (bool, optional): whether the output tensors have :attr:`dim`\n            retained or not. Ignored if :attr:`dim` = ``None`` and\n            :attr:`out` = ``None``. Default: ``False``\n        out (Tensor, optional): the output tensor. Ignored if\n            :attr:`dim` = ``None`` and :attr:`out` = ``None``.\n        dtype (:class:`torch.dtype`, optional): the desired data type of\n            returned tensor. If specified, the input tensor is casted to\n            :attr:`dtype` while performing the operation. Default: None.\n\n    .. note::\n        Even though ``p='fro'`` supports any number of dimensions, the true\n        mathematical definition of Frobenius norm only applies to tensors with\n        exactly two dimensions. :func:`torch.linalg.matrix_norm` with ``ord='fro'``\n        aligns with the mathematical definition, since it can only be applied across\n        exactly two dimensions.\n\n    Example::\n\n        >>> import torch\n        >>> a = torch.arange(9, dtype= torch.float) - 4\n        >>> b = a.reshape((3, 3))\n        >>> torch.norm(a)\n        tensor(7.7460)\n        >>> torch.norm(b)\n        tensor(7.7460)\n        >>> torch.norm(a, float('inf'))\n        tensor(4.)\n        >>> torch.norm(b, float('inf'))\n        tensor(4.)\n        >>> c = torch.tensor([[ 1, 2, 3], [-1, 1, 4]] , dtype=torch.float)\n        >>> torch.norm(c, dim=0)\n        tensor([1.4142, 2.2361, 5.0000])\n        >>> torch.norm(c, dim=1)\n        tensor([3.7417, 4.2426])\n        >>> torch.norm(c, p=1, dim=1)\n        tensor([6., 6.])\n        >>> d = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)\n        >>> torch.norm(d, dim=(1, 2))\n        tensor([ 3.7417, 11.2250])\n        >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n        (tensor(3.7417), tensor(11.2250))\n    ",
      "arguments": [
        "input",
        "p",
        "dim",
        "keepdim",
        "out",
        "dtype"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the matrix norm or vector norm of a given tensor.\n\n    .. warning::\n\n        torch.norm is deprecated and may be removed in a future PyTorch release.\n        Its documentation and behavior may be incorrect, and it is no longer\n        actively maintained.\n\n        Use :func:`torch.linalg.vector_norm` when computing vector norms and\n        :func:`torch.linalg.matrix_norm` when computing matrix norms.\n        For a function with a similar behavior as this one see :func:`torch.linalg.norm`.\n        Note, however, the signature for these functions is slightly different than the\n        signature for ``torch.norm``.\n\n    Args:\n        input (Tensor): The input tensor. Its data type must be either a floating\n            point or complex type. For complex inputs, the norm is calculated using the\n            absolute value of each element. If the input is complex and neither\n            :attr:`dtype` nor :attr:`out` is specified, the result's data type will\n            be the corresponding floating point type (e.g. float if :attr:`input` is\n            complexfloat).\n\n        p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``\n            The following norms can be calculated:\n\n            ======  ==============  ==========================\n            ord     matrix norm     vector norm\n            ======  ==============  ==========================\n            'fro'   Frobenius norm  --\n            'nuc'   nuclear norm    --\n            Number  --              sum(abs(x)**ord)**(1./ord)\n            ======  ==============  ==========================\n\n            The vector norm can be calculated across any number of dimensions.\n            The corresponding dimensions of :attr:`input` are flattened into\n            one dimension, and the norm is calculated on the flattened\n            dimension.\n\n            Frobenius norm produces the same result as ``p=2`` in all cases\n            except when :attr:`dim` is a list of three or more dims, in which\n            case Frobenius norm throws an error.\n\n            Nuclear norm can only be calculated across exactly two dimensions.\n\n        dim (int, tuple of ints, list of ints, optional):\n            Specifies which dimension or dimensions of :attr:`input` to\n            calculate the norm across. If :attr:`dim` is ``None``, the norm will\n            be calculated across all dimensions of :attr:`input`. If the norm\n            type indicated by :attr:`p` does not support the specified number of\n            dimensions, an error will occur.\n        keepdim (bool, optional): whether the output tensors have :attr:`dim`\n            retained or not. Ignored if :attr:`dim` = ``None`` and\n            :attr:`out` = ``None``. Default: ``False``\n        out (Tensor, optional): the output tensor. Ignored if\n            :attr:`dim` = ``None`` and :attr:`out` = ``None``.\n        dtype (:class:`torch.dtype`, optional): the desired data type of\n            returned tensor. If specified, the input tensor is casted to\n            :attr:`dtype` while performing the operation. Default: None.\n\n    .. note::\n        Even though ``p='fro'`` supports any number of dimensions, the true\n        mathematical definition of Frobenius norm only applies to tensors with\n        exactly two dimensions. :func:`torch.linalg.matrix_norm` with ``ord='fro'``\n        aligns with the mathematical definition, since it can only be applied across\n        exactly two dimensions.\n\n    Example::\n\n        >>> import torch\n        >>> a = torch.arange(9, dtype= torch.float) - 4\n        >>> b = a.reshape((3, 3))\n        >>> torch.norm(a)\n        tensor(7.7460)\n        >>> torch.norm(b)\n        tensor(7.7460)\n        >>> torch.norm(a, float('inf'))\n        tensor(4.)\n        >>> torch.norm(b, float('inf'))\n        tensor(4.)\n        >>> c = torch.tensor([[ 1, 2, 3], [-1, 1, 4]] , dtype=torch.float)\n        >>> torch.norm(c, dim=0)\n        tensor([1.4142, 2.2361, 5.0000])\n        >>> torch.norm(c, dim=1)\n        tensor([3.7417, 4.2426])\n        >>> torch.norm(c, p=1, dim=1)\n        tensor([6., 6.])\n        >>> d = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)\n        >>> torch.norm(d, dim=(1, 2))\n        tensor([ 3.7417, 11.2250])\n        >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n        (tensor(3.7417), tensor(11.2250))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.pca_lowrank",
      "signature": "torch.pca_lowrank(A: torch.Tensor, q: Optional[int] = None, center: bool = True, niter: int = 2) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
      "doc": "Performs linear Principal Component Analysis (PCA) on a low-rank\n    matrix, batches of such matrices, or sparse matrix.\n\n    This function returns a namedtuple ``(U, S, V)`` which is the\n    nearly optimal approximation of a singular value decomposition of\n    a centered matrix :math:`A` such that :math:`A \\approx U \\operatorname{diag}(S) V^{\\text{H}}`\n\n    .. note:: The relation of ``(U, S, V)`` to PCA is as follows:\n\n                - :math:`A` is a data matrix with ``m`` samples and\n                  ``n`` features\n\n                - the :math:`V` columns represent the principal directions\n\n                - :math:`S ** 2 / (m - 1)` contains the eigenvalues of\n                  :math:`A^T A / (m - 1)` which is the covariance of\n                  ``A`` when ``center=True`` is provided.\n\n                - ``matmul(A, V[:, :k])`` projects data to the first k\n                  principal components\n\n    .. note:: Different from the standard SVD, the size of returned\n              matrices depend on the specified rank and q\n              values as follows:\n\n                - :math:`U` is m x q matrix\n\n                - :math:`S` is q-vector\n\n                - :math:`V` is n x q matrix\n\n    .. note:: To obtain repeatable results, reset the seed for the\n              pseudorandom number generator\n\n    Args:\n\n        A (Tensor): the input tensor of size :math:`(*, m, n)`\n\n        q (int, optional): a slightly overestimated rank of\n                           :math:`A`. By default, ``q = min(6, m,\n                           n)``.\n\n        center (bool, optional): if True, center the input tensor,\n                                 otherwise, assume that the input is\n                                 centered.\n\n        niter (int, optional): the number of subspace iterations to\n                               conduct; niter must be a nonnegative\n                               integer, and defaults to 2.\n\n    References::\n\n        - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n          structure with randomness: probabilistic algorithms for\n          constructing approximate matrix decompositions,\n          arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n          `arXiv <http://arxiv.org/abs/0909.4061>`_).\n\n    ",
      "arguments": [
        "A",
        "q",
        "center",
        "niter"
      ],
      "return_type": "tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Performs linear Principal Component Analysis (PCA) on a low-rank\n    matrix, batches of such matrices, or sparse matrix.\n\n    This function returns a namedtuple ``(U, S, V)`` which is the\n    nearly optimal approximation of a singular value decomposition of\n    a centered matrix :math:`A` such that :math:`A \\approx U \\operatorname{diag}(S) V^{\\text{H}}`\n\n    .. note:: The relation of ``(U, S, V)`` to PCA is as follows:\n\n                - :math:`A` is a data matrix with ``m`` samples and\n                  ``n`` features\n\n                - the :math:`V` columns represent the principal directions\n\n                - :math:`S ** 2 / (m - 1)` contains the eigenvalues of\n                  :math:`A^T A / (m - 1)` which is the covariance of\n                  ``A`` when ``center=True`` is provided.\n\n                - ``matmul(A, V[:, :k])`` projects data to the first k\n                  principal components\n\n    .. note:: Different from the standard SVD, the size of returned\n              matrices depend on the specified rank and q\n              values as follows:\n\n                - :math:`U` is m x q matrix\n\n                - :math:`S` is q-vector\n\n                - :math:`V` is n x q matrix\n\n    .. note:: To obtain repeatable results, reset the seed for the\n              pseudorandom number generator\n\n    Args:\n\n        A (Tensor): the input tensor of size :math:`(*, m, n)`\n\n        q (int, optional): a slightly overestimated rank of\n                           :math:`A`. By default, ``q = min(6, m,\n                           n)``.\n\n        center (bool, optional): if True, center the input tensor,\n                                 otherwise, assume that the input is\n                                 centered.\n\n        niter (int, optional): the number of subspace iterations to\n                               conduct; niter must be a nonnegative\n                               integer, and defaults to 2.\n\n    References::\n\n        - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n          structure with randomness: probabilistic algorithms for\n          constructing approximate matrix decompositions,\n          arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n          `arXiv <http://arxiv.org/abs/0909.4061>`_).\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.prepare_multiprocessing_environment",
      "signature": "torch.prepare_multiprocessing_environment(path: str) -> None",
      "doc": "",
      "arguments": [
        "path"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.profiler_allow_cudagraph_cupti_lazy_reinit_cuda12",
      "signature": "torch.profiler_allow_cudagraph_cupti_lazy_reinit_cuda12()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.save",
      "signature": "torch.save(obj: object, f: Union[str, os.PathLike[str], IO[bytes]], pickle_module: Any = <module 'pickle' from '/Users/unixwzrd/miniconda3/envs/LLaSA-speech/lib/python3.11/pickle.py'>, pickle_protocol: int = 2, _use_new_zipfile_serialization: bool = True, _disable_byteorder_record: bool = False) -> None",
      "doc": "save(obj, f, pickle_module=pickle, pickle_protocol=2, _use_new_zipfile_serialization=True)\n\n    Saves an object to a disk file.\n\n    See also: :ref:`saving-loading-tensors`\n\n    Args:\n        obj: saved object\n        f: a file-like object (has to implement write and flush) or a string or\n           os.PathLike object containing a file name\n        pickle_module: module used for pickling metadata and objects\n        pickle_protocol: can be specified to override the default protocol\n\n    .. note::\n        A common PyTorch convention is to save tensors using .pt file extension.\n\n    .. note::\n        PyTorch preserves storage sharing across serialization. See\n        :ref:`preserve-storage-sharing` for more details.\n\n    .. note::\n        The 1.6 release of PyTorch switched ``torch.save`` to use a new\n        zipfile-based file format. ``torch.load`` still retains the ability to\n        load files in the old format. If for any reason you want ``torch.save``\n        to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"makes cwd dirty\")\n        >>> # Save to file\n        >>> x = torch.tensor([0, 1, 2, 3, 4])\n        >>> torch.save(x, \"tensor.pt\")\n        >>> # Save to io.BytesIO buffer\n        >>> buffer = io.BytesIO()\n        >>> torch.save(x, buffer)\n    ",
      "arguments": [
        "obj",
        "f",
        "pickle_module",
        "pickle_protocol",
        "_use_new_zipfile_serialization",
        "_disable_byteorder_record"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "save(obj, f, pickle_module=pickle, pickle_protocol=2, _use_new_zipfile_serialization=True)\n\n    Saves an object to a disk file.\n\n    See also: :ref:`saving-loading-tensors`\n\n    Args:\n        obj: saved object\n        f: a file-like object (has to implement write and flush) or a string or\n           os.PathLike object containing a file name\n        pickle_module: module used for pickling metadata and objects\n        pickle_protocol: can be specified to override the default protocol\n\n    .. note::\n        A common PyTorch convention is to save tensors using .pt file extension.\n\n    .. note::\n        PyTorch preserves storage sharing across serialization. See\n        :ref:`preserve-storage-sharing` for more details.\n\n    .. note::\n        The 1.6 release of PyTorch switched ``torch.save`` to use a new\n        zipfile-based file format. ``torch.load`` still retains the ability to\n        load files in the old format. If for any reason you want ``torch.save``\n        to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"makes cwd dirty\")\n        >>> # Save to file\n        >>> x = torch.tensor([0, 1, 2, 3, 4])\n        >>> torch.save(x, \"tensor.pt\")\n        >>> # Save to io.BytesIO buffer\n        >>> buffer = io.BytesIO()\n        >>> torch.save(x, buffer)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.seed",
      "signature": "torch.seed() -> int",
      "doc": "Sets the seed for generating random numbers to a non-deterministic\n    random number on all devices. Returns a 64 bit number used to seed the RNG.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the seed for generating random numbers to a non-deterministic\n    random number on all devices. Returns a 64 bit number used to seed the RNG.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.set_default_device",
      "signature": "torch.set_default_device(device: Union[ForwardRef('torch.device'), str, int, NoneType]) -> None",
      "doc": "Sets the default ``torch.Tensor`` to be allocated on ``device``.  This\n    does not affect factory function calls which are called with an explicit\n    ``device`` argument.  Factory calls will be performed as if they\n    were passed ``device`` as an argument.\n\n    To only temporarily change the default device instead of setting it\n    globally, use ``with torch.device(device):`` instead.\n\n    The default device is initially ``cpu``.  If you set the default tensor\n    device to another device (e.g., ``cuda``) without a device index, tensors\n    will be allocated on whatever the current device for the device type,\n    even after :func:`torch.cuda.set_device` is called.\n\n    .. warning::\n\n        This function imposes a slight performance cost on every Python\n        call to the torch API (not just factory functions).  If this\n        is causing problems for you, please comment on\n        https://github.com/pytorch/pytorch/issues/92701\n\n    .. note::\n\n        This doesn't affect functions that create tensors that share the same memory as the input, like:\n        :func:`torch.from_numpy` and :func:`torch.frombuffer`\n\n    Args:\n        device (device or string): the device to set as default\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"requires cuda, changes global state\")\n        >>> torch.get_default_device()\n        device(type='cpu')\n        >>> torch.set_default_device('cuda')  # current device is 0\n        >>> torch.get_default_device()\n        device(type='cuda', index=0)\n        >>> torch.set_default_device('cuda')\n        >>> torch.cuda.set_device('cuda:1')  # current device is 1\n        >>> torch.get_default_device()\n        device(type='cuda', index=1)\n        >>> torch.set_default_device('cuda:1')\n        >>> torch.get_default_device()\n        device(type='cuda', index=1)\n\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the default ``torch.Tensor`` to be allocated on ``device``.  This\n    does not affect factory function calls which are called with an explicit\n    ``device`` argument.  Factory calls will be performed as if they\n    were passed ``device`` as an argument.\n\n    To only temporarily change the default device instead of setting it\n    globally, use ``with torch.device(device):`` instead.\n\n    The default device is initially ``cpu``.  If you set the default tensor\n    device to another device (e.g., ``cuda``) without a device index, tensors\n    will be allocated on whatever the current device for the device type,\n    even after :func:`torch.cuda.set_device` is called.\n\n    .. warning::\n\n        This function imposes a slight performance cost on every Python\n        call to the torch API (not just factory functions).  If this\n        is causing problems for you, please comment on\n        https://github.com/pytorch/pytorch/issues/92701\n\n    .. note::\n\n        This doesn't affect functions that create tensors that share the same memory as the input, like:\n        :func:`torch.from_numpy` and :func:`torch.frombuffer`\n\n    Args:\n        device (device or string): the device to set as default\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"requires cuda, changes global state\")\n        >>> torch.get_default_device()\n        device(type='cpu')\n        >>> torch.set_default_device('cuda')  # current device is 0\n        >>> torch.get_default_device()\n        device(type='cuda', index=0)\n        >>> torch.set_default_device('cuda')\n        >>> torch.cuda.set_device('cuda:1')  # current device is 1\n        >>> torch.get_default_device()\n        device(type='cuda', index=1)\n        >>> torch.set_default_device('cuda:1')\n        >>> torch.get_default_device()\n        device(type='cuda', index=1)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.set_default_dtype",
      "signature": "torch.set_default_dtype(d: 'torch.dtype', /) -> None",
      "doc": "\n\n    Sets the default floating point dtype to :attr:`d`. Supports floating point dtype\n    as inputs. Other dtypes will cause torch to raise an exception.\n\n    When PyTorch is initialized its default floating point dtype is torch.float32,\n    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like\n    type inference. The default floating point dtype is used to:\n\n    1. Implicitly determine the default complex dtype. When the default floating type is float16,\n       the default complex dtype is complex32. For float32, the default complex dtype is complex64.\n       For float64, it is complex128. For bfloat16, an exception will be raised because\n       there is no corresponding complex type for bfloat16.\n    2. Infer the dtype for tensors constructed using Python floats or complex Python\n       numbers. See examples below.\n    3. Determine the result of type promotion between bool and integer tensors and\n       Python floats and complex Python numbers.\n\n    Args:\n        d (:class:`torch.dtype`): the floating point dtype to make the default.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> # initial default for floating point is torch.float32\n        >>> # Python floats are interpreted as float32\n        >>> torch.tensor([1.2, 3]).dtype\n        torch.float32\n        >>> # initial default for floating point is torch.complex64\n        >>> # Complex Python numbers are interpreted as complex64\n        >>> torch.tensor([1.2, 3j]).dtype\n        torch.complex64\n\n        >>> torch.set_default_dtype(torch.float64)\n        >>> # Python floats are now interpreted as float64\n        >>> torch.tensor([1.2, 3]).dtype  # a new floating point tensor\n        torch.float64\n        >>> # Complex Python numbers are now interpreted as complex128\n        >>> torch.tensor([1.2, 3j]).dtype  # a new complex tensor\n        torch.complex128\n\n        >>> torch.set_default_dtype(torch.float16)\n        >>> # Python floats are now interpreted as float16\n        >>> torch.tensor([1.2, 3]).dtype  # a new floating point tensor\n        torch.float16\n        >>> # Complex Python numbers are now interpreted as complex128\n        >>> torch.tensor([1.2, 3j]).dtype  # a new complex tensor\n        torch.complex32\n\n    ",
      "arguments": [
        "d"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n\n    Sets the default floating point dtype to :attr:`d`. Supports floating point dtype\n    as inputs. Other dtypes will cause torch to raise an exception.\n\n    When PyTorch is initialized its default floating point dtype is torch.float32,\n    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like\n    type inference. The default floating point dtype is used to:\n\n    1. Implicitly determine the default complex dtype. When the default floating type is float16,\n       the default complex dtype is complex32. For float32, the default complex dtype is complex64.\n       For float64, it is complex128. For bfloat16, an exception will be raised because\n       there is no corresponding complex type for bfloat16.\n    2. Infer the dtype for tensors constructed using Python floats or complex Python\n       numbers. See examples below.\n    3. Determine the result of type promotion between bool and integer tensors and\n       Python floats and complex Python numbers.\n\n    Args:\n        d (:class:`torch.dtype`): the floating point dtype to make the default.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> # initial default for floating point is torch.float32\n        >>> # Python floats are interpreted as float32\n        >>> torch.tensor([1.2, 3]).dtype\n        torch.float32\n        >>> # initial default for floating point is torch.complex64\n        >>> # Complex Python numbers are interpreted as complex64\n        >>> torch.tensor([1.2, 3j]).dtype\n        torch.complex64\n\n        >>> torch.set_default_dtype(torch.float64)\n        >>> # Python floats are now interpreted as float64\n        >>> torch.tensor([1.2, 3]).dtype  # a new floating point tensor\n        torch.float64\n        >>> # Complex Python numbers are now interpreted as complex128\n        >>> torch.tensor([1.2, 3j]).dtype  # a new complex tensor\n        torch.complex128\n\n        >>> torch.set_default_dtype(torch.float16)\n        >>> # Python floats are now interpreted as float16\n        >>> torch.tensor([1.2, 3]).dtype  # a new floating point tensor\n        torch.float16\n        >>> # Complex Python numbers are now interpreted as complex128\n        >>> torch.tensor([1.2, 3j]).dtype  # a new complex tensor\n        torch.complex32\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.set_default_tensor_type",
      "signature": "torch.set_default_tensor_type(t: Union[type['torch.Tensor'], str], /) -> None",
      "doc": "\n    .. warning::\n\n        This function is deprecated as of PyTorch 2.1, please use :func:`torch.set_default_dtype()` and\n        :func:`torch.set_default_device()` as alternatives.\n\n    Sets the default ``torch.Tensor`` type to floating point tensor type\n    ``t``. This type will also be used as default floating point type for\n    type inference in :func:`torch.tensor`.\n\n    The default floating point tensor type is initially ``torch.FloatTensor``.\n\n    Args:\n        t (type or string): the floating point tensor type or its name\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\n        torch.float32\n        >>> torch.set_default_tensor_type(torch.DoubleTensor)\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n\n    ",
      "arguments": [
        "t"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    .. warning::\n\n        This function is deprecated as of PyTorch 2.1, please use :func:`torch.set_default_dtype()` and\n        :func:`torch.set_default_device()` as alternatives.\n\n    Sets the default ``torch.Tensor`` type to floating point tensor type\n    ``t``. This type will also be used as default floating point type for\n    type inference in :func:`torch.tensor`.\n\n    The default floating point tensor type is initially ``torch.FloatTensor``.\n\n    Args:\n        t (type or string): the floating point tensor type or its name\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\n        torch.float32\n        >>> torch.set_default_tensor_type(torch.DoubleTensor)\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.set_deterministic_debug_mode",
      "signature": "torch.set_deterministic_debug_mode(debug_mode: Union[int, str]) -> None",
      "doc": "Sets the debug mode for deterministic operations.\n\n    .. note:: This is an alternative interface for\n        :func:`torch.use_deterministic_algorithms`. Refer to that function's\n        documentation for details about affected operations.\n\n    Args:\n        debug_mode(str or int): If \"default\" or 0, don't error or warn on\n            nondeterministic operations. If \"warn\" or 1, warn on\n            nondeterministic operations. If \"error\" or 2, error on\n            nondeterministic operations.\n    ",
      "arguments": [
        "debug_mode"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the debug mode for deterministic operations.\n\n    .. note:: This is an alternative interface for\n        :func:`torch.use_deterministic_algorithms`. Refer to that function's\n        documentation for details about affected operations.\n\n    Args:\n        debug_mode(str or int): If \"default\" or 0, don't error or warn on\n            nondeterministic operations. If \"warn\" or 1, warn on\n            nondeterministic operations. If \"error\" or 2, error on\n            nondeterministic operations.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.set_float32_matmul_precision",
      "signature": "torch.set_float32_matmul_precision(precision: str) -> None",
      "doc": "Sets the internal precision of float32 matrix multiplications.\n\n    Running float32 matrix multiplications in lower precision may significantly increase\n    performance, and in some programs the loss of precision has a negligible impact.\n\n    Supports three settings:\n\n        * \"highest\", float32 matrix multiplications use the float32 datatype (24 mantissa\n          bits with 23 bits explicitly stored) for internal computations.\n        * \"high\", float32 matrix multiplications either use the TensorFloat32 datatype (10\n          mantissa bits explicitly stored) or treat each float32 number as the sum of two bfloat16 numbers\n          (approximately 16 mantissa bits with 14 bits explicitly stored), if the appropriate fast matrix multiplication\n          algorithms are available.  Otherwise float32 matrix multiplications are computed\n          as if the precision is \"highest\".  See below for more information on the bfloat16\n          approach.\n        * \"medium\", float32 matrix multiplications use the bfloat16 datatype (8 mantissa\n          bits with 7 bits explicitly stored) for internal computations, if a fast matrix multiplication algorithm\n          using that datatype internally is available. Otherwise float32\n          matrix multiplications are computed as if the precision is \"high\".\n\n    When using \"high\" precision, float32 multiplications may use a bfloat16-based algorithm\n    that is more complicated than simply truncating to some smaller number mantissa bits\n    (e.g. 10 for TensorFloat32, 7 for bfloat16 explicitly stored).  Refer to [Henry2019]_ for a complete\n    description of this algorithm.  To briefly explain here, the first step is to realize\n    that we can perfectly encode a single float32 number as the sum of three bfloat16\n    numbers (because float32 has 23 mantissa bits while bfloat16 has 7 explicitly stored, and both have the\n    same number of exponent bits).  This means that the product of two float32 numbers can\n    be exactly given by the sum of nine products of bfloat16 numbers.  We can then trade\n    accuracy for speed by dropping some of these products.  The \"high\" precision algorithm\n    specifically keeps only the three most significant products, which conveniently excludes\n    all of the products involving the last 8 mantissa bits of either input.  This means that\n    we can represent our inputs as the sum of two bfloat16 numbers rather than three.\n    Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than\n    float32 ones, it's faster to do three multiplications and 2 additions with bfloat16\n    precision than it is to do a single multiplication with float32 precision.\n\n    .. [Henry2019] http://arxiv.org/abs/1904.06376\n\n    .. note::\n\n        This does not change the output dtype of float32 matrix multiplications,\n        it controls how the internal computation of the matrix multiplication is performed.\n\n    .. note::\n\n        This does not change the precision of convolution operations. Other flags,\n        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution\n        operations.\n\n    .. note::\n\n        This flag currently only affects one native device type: CUDA.\n        If \"high\" or \"medium\" are set then the TensorFloat32 datatype will be used\n        when computing float32 matrix multiplications, equivalent to setting\n        `torch.backends.cuda.matmul.allow_tf32 = True`. When \"highest\" (the default)\n        is set then the float32 datatype is used for internal computations, equivalent\n        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.\n\n    Args:\n        precision(str): can be set to \"highest\" (default), \"high\", or \"medium\" (see above).\n\n    ",
      "arguments": [
        "precision"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the internal precision of float32 matrix multiplications.\n\n    Running float32 matrix multiplications in lower precision may significantly increase\n    performance, and in some programs the loss of precision has a negligible impact.\n\n    Supports three settings:\n\n        * \"highest\", float32 matrix multiplications use the float32 datatype (24 mantissa\n          bits with 23 bits explicitly stored) for internal computations.\n        * \"high\", float32 matrix multiplications either use the TensorFloat32 datatype (10\n          mantissa bits explicitly stored) or treat each float32 number as the sum of two bfloat16 numbers\n          (approximately 16 mantissa bits with 14 bits explicitly stored), if the appropriate fast matrix multiplication\n          algorithms are available.  Otherwise float32 matrix multiplications are computed\n          as if the precision is \"highest\".  See below for more information on the bfloat16\n          approach.\n        * \"medium\", float32 matrix multiplications use the bfloat16 datatype (8 mantissa\n          bits with 7 bits explicitly stored) for internal computations, if a fast matrix multiplication algorithm\n          using that datatype internally is available. Otherwise float32\n          matrix multiplications are computed as if the precision is \"high\".\n\n    When using \"high\" precision, float32 multiplications may use a bfloat16-based algorithm\n    that is more complicated than simply truncating to some smaller number mantissa bits\n    (e.g. 10 for TensorFloat32, 7 for bfloat16 explicitly stored).  Refer to [Henry2019]_ for a complete\n    description of this algorithm.  To briefly explain here, the first step is to realize\n    that we can perfectly encode a single float32 number as the sum of three bfloat16\n    numbers (because float32 has 23 mantissa bits while bfloat16 has 7 explicitly stored, and both have the\n    same number of exponent bits).  This means that the product of two float32 numbers can\n    be exactly given by the sum of nine products of bfloat16 numbers.  We can then trade\n    accuracy for speed by dropping some of these products.  The \"high\" precision algorithm\n    specifically keeps only the three most significant products, which conveniently excludes\n    all of the products involving the last 8 mantissa bits of either input.  This means that\n    we can represent our inputs as the sum of two bfloat16 numbers rather than three.\n    Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than\n    float32 ones, it's faster to do three multiplications and 2 additions with bfloat16\n    precision than it is to do a single multiplication with float32 precision.\n\n    .. [Henry2019] http://arxiv.org/abs/1904.06376\n\n    .. note::\n\n        This does not change the output dtype of float32 matrix multiplications,\n        it controls how the internal computation of the matrix multiplication is performed.\n\n    .. note::\n\n        This does not change the precision of convolution operations. Other flags,\n        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution\n        operations.\n\n    .. note::\n\n        This flag currently only affects one native device type: CUDA.\n        If \"high\" or \"medium\" are set then the TensorFloat32 datatype will be used\n        when computing float32 matrix multiplications, equivalent to setting\n        `torch.backends.cuda.matmul.allow_tf32 = True`. When \"highest\" (the default)\n        is set then the float32 datatype is used for internal computations, equivalent\n        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.\n\n    Args:\n        precision(str): can be set to \"highest\" (default), \"high\", or \"medium\" (see above).\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.set_printoptions",
      "signature": "torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)",
      "doc": "Set options for printing. Items shamelessly taken from NumPy\n\n    Args:\n        precision: Number of digits of precision for floating point output\n            (default = 4).\n        threshold: Total number of array elements which trigger summarization\n            rather than full `repr` (default = 1000).\n        edgeitems: Number of array items in summary at beginning and end of\n            each dimension (default = 3).\n        linewidth: The number of characters per line for the purpose of\n            inserting line breaks (default = 80). Thresholded matrices will\n            ignore this parameter.\n        profile: Sane defaults for pretty printing. Can override with any of\n            the above options. (any one of `default`, `short`, `full`)\n        sci_mode: Enable (True) or disable (False) scientific notation. If\n            None (default) is specified, the value is defined by\n            `torch._tensor_str._Formatter`. This value is automatically chosen\n            by the framework.\n\n    Example::\n\n        >>> # Limit the precision of elements\n        >>> torch.set_printoptions(precision=2)\n        >>> torch.tensor([1.12345])\n        tensor([1.12])\n        >>> # Limit the number of elements shown\n        >>> torch.set_printoptions(threshold=5)\n        >>> torch.arange(10)\n        tensor([0, 1, 2, ..., 7, 8, 9])\n        >>> # Restore defaults\n        >>> torch.set_printoptions(profile='default')\n        >>> torch.tensor([1.12345])\n        tensor([1.1235])\n        >>> torch.arange(10)\n        tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n    ",
      "arguments": [
        "precision",
        "threshold",
        "edgeitems",
        "linewidth",
        "profile",
        "sci_mode"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set options for printing. Items shamelessly taken from NumPy\n\n    Args:\n        precision: Number of digits of precision for floating point output\n            (default = 4).\n        threshold: Total number of array elements which trigger summarization\n            rather than full `repr` (default = 1000).\n        edgeitems: Number of array items in summary at beginning and end of\n            each dimension (default = 3).\n        linewidth: The number of characters per line for the purpose of\n            inserting line breaks (default = 80). Thresholded matrices will\n            ignore this parameter.\n        profile: Sane defaults for pretty printing. Can override with any of\n            the above options. (any one of `default`, `short`, `full`)\n        sci_mode: Enable (True) or disable (False) scientific notation. If\n            None (default) is specified, the value is defined by\n            `torch._tensor_str._Formatter`. This value is automatically chosen\n            by the framework.\n\n    Example::\n\n        >>> # Limit the precision of elements\n        >>> torch.set_printoptions(precision=2)\n        >>> torch.tensor([1.12345])\n        tensor([1.12])\n        >>> # Limit the number of elements shown\n        >>> torch.set_printoptions(threshold=5)\n        >>> torch.arange(10)\n        tensor([0, 1, 2, ..., 7, 8, 9])\n        >>> # Restore defaults\n        >>> torch.set_printoptions(profile='default')\n        >>> torch.tensor([1.12345])\n        tensor([1.1235])\n        >>> torch.arange(10)\n        tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.set_rng_state",
      "signature": "torch.set_rng_state(new_state: torch.Tensor) -> None",
      "doc": "Sets the random number generator state.\n\n    .. note:: This function only works for CPU. For CUDA, please use\n        :func:`torch.manual_seed`, which works for both CPU and CUDA.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n    ",
      "arguments": [
        "new_state"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the random number generator state.\n\n    .. note:: This function only works for CPU. For CUDA, please use\n        :func:`torch.manual_seed`, which works for both CPU and CUDA.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.set_warn_always",
      "signature": "torch.set_warn_always(b: bool, /) -> None",
      "doc": "When this flag is False (default) then some PyTorch warnings may only\n    appear once per process. This helps avoid excessive warning information.\n    Setting it to True causes these warnings to always appear, which may be\n    helpful when debugging.\n\n    Args:\n        b (:class:`bool`): If True, force warnings to always be emitted\n                           If False, set to the default behaviour\n    ",
      "arguments": [
        "b"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "When this flag is False (default) then some PyTorch warnings may only\n    appear once per process. This helps avoid excessive warning information.\n    Setting it to True causes these warnings to always appear, which may be\n    helpful when debugging.\n\n    Args:\n        b (:class:`bool`): If True, force warnings to always be emitted\n                           If False, set to the default behaviour\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.solve",
      "signature": "torch.solve(input: torch.Tensor, A: torch.Tensor, *, out=None) -> tuple[torch.Tensor, torch.Tensor]",
      "doc": "",
      "arguments": [
        "input",
        "A",
        "out"
      ],
      "return_type": "tuple[torch.Tensor, torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.split",
      "signature": "torch.split(tensor: torch.Tensor, split_size_or_sections: Union[int, list[int]], dim: int = 0) -> tuple[torch.Tensor, ...]",
      "doc": "Splits the tensor into chunks. Each chunk is a view of the original tensor.\n\n    If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will\n    be split into equally sized chunks (if possible). Last chunk will be smaller if\n    the tensor size along the given dimension :attr:`dim` is not divisible by\n    :attr:`split_size`.\n\n    If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split\n    into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according\n    to :attr:`split_size_or_sections`.\n\n    Args:\n        tensor (Tensor): tensor to split.\n        split_size_or_sections (int) or (list(int)): size of a single chunk or\n            list of sizes for each chunk\n        dim (int): dimension along which to split the tensor.\n\n    Example::\n\n        >>> a = torch.arange(10).reshape(5, 2)\n        >>> a\n        tensor([[0, 1],\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]])\n        >>> torch.split(a, 2)\n        (tensor([[0, 1],\n                 [2, 3]]),\n         tensor([[4, 5],\n                 [6, 7]]),\n         tensor([[8, 9]]))\n        >>> torch.split(a, [1, 4])\n        (tensor([[0, 1]]),\n         tensor([[2, 3],\n                 [4, 5],\n                 [6, 7],\n                 [8, 9]]))\n    ",
      "arguments": [
        "tensor",
        "split_size_or_sections",
        "dim"
      ],
      "return_type": "tuple[torch.Tensor, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Splits the tensor into chunks. Each chunk is a view of the original tensor.\n\n    If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will\n    be split into equally sized chunks (if possible). Last chunk will be smaller if\n    the tensor size along the given dimension :attr:`dim` is not divisible by\n    :attr:`split_size`.\n\n    If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split\n    into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according\n    to :attr:`split_size_or_sections`.\n\n    Args:\n        tensor (Tensor): tensor to split.\n        split_size_or_sections (int) or (list(int)): size of a single chunk or\n            list of sizes for each chunk\n        dim (int): dimension along which to split the tensor.\n\n    Example::\n\n        >>> a = torch.arange(10).reshape(5, 2)\n        >>> a\n        tensor([[0, 1],\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]])\n        >>> torch.split(a, 2)\n        (tensor([[0, 1],\n                 [2, 3]]),\n         tensor([[4, 5],\n                 [6, 7]]),\n         tensor([[8, 9]]))\n        >>> torch.split(a, [1, 4])\n        (tensor([[0, 1]]),\n         tensor([[2, 3],\n                 [4, 5],\n                 [6, 7],\n                 [8, 9]]))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.stft",
      "signature": "torch.stft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None, align_to_window: Optional[bool] = None) -> torch.Tensor",
      "doc": "Short-time Fourier transform (STFT).\n\n    .. warning::\n        From version 1.8.0, :attr:`return_complex` must always be given\n        explicitly for real inputs and `return_complex=False` has been\n        deprecated. Strongly prefer `return_complex=True` as in a future\n        pytorch release, this function will only return complex tensors.\n\n        Note that :func:`torch.view_as_real` can be used to recover a real\n        tensor with an extra last dimension for real and imaginary components.\n\n    .. warning::\n        From version 2.1, a warning will be provided if a :attr:`window` is\n        not specified. In a future release, this attribute will be required.\n        Not providing a window currently defaults to using a rectangular window,\n        which may result in undesirable artifacts. Consider using tapered windows,\n        such as :func:`torch.hann_window`.\n\n    The STFT computes the Fourier transform of short overlapping windows of the\n    input. This giving frequency components of the signal as they change over\n    time. The interface of this function is modeled after (but *not* a drop-in\n    replacement for) librosa_ stft function.\n\n    .. _librosa: https://librosa.org/doc/latest/generated/librosa.stft.html\n\n    Ignoring the optional batch dimension, this method computes the following\n    expression:\n\n    .. math::\n        X[\\omega, m] = \\sum_{k = 0}^{\\text{win\\_length-1}}%\n                            \\text{window}[k]\\ \\text{input}[m \\times \\text{hop\\_length} + k]\\ %\n                            \\exp\\left(- j \\frac{2 \\pi \\cdot \\omega k}{\\text{n\\_fft}}\\right),\n\n    where :math:`m` is the index of the sliding window, and :math:`\\omega` is\n    the frequency :math:`0 \\leq \\omega < \\text{n\\_fft}` for ``onesided=False``,\n    or :math:`0 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 1` for ``onesided=True``.\n\n    * :attr:`input` must be either a 1-D time sequence or a 2-D batch of time\n      sequences.\n\n    * If :attr:`hop_length` is ``None`` (default), it is treated as equal to\n      ``floor(n_fft / 4)``.\n\n    * If :attr:`win_length` is ``None`` (default), it is treated as equal to\n      :attr:`n_fft`.\n\n    * :attr:`window` can be a 1-D tensor of size :attr:`win_length`, e.g., from\n      :meth:`torch.hann_window`. If :attr:`window` is ``None`` (default), it is\n      treated as if having :math:`1` everywhere in the window. If\n      :math:`\\text{win\\_length} < \\text{n\\_fft}`, :attr:`window` will be padded on\n      both sides to length :attr:`n_fft` before being applied.\n\n    * If :attr:`center` is ``True`` (default), :attr:`input` will be padded on\n      both sides so that the :math:`t`-th frame is centered at time\n      :math:`t \\times \\text{hop\\_length}`. Otherwise, the :math:`t`-th frame\n      begins at time  :math:`t \\times \\text{hop\\_length}`.\n\n    * :attr:`pad_mode` determines the padding method used on :attr:`input` when\n      :attr:`center` is ``True``. See :meth:`torch.nn.functional.pad` for\n      all available options. Default is ``\"reflect\"``.\n\n    * If :attr:`onesided` is ``True`` (default for real input), only values for\n      :math:`\\omega` in :math:`\\left[0, 1, 2, \\dots, \\left\\lfloor\n      \\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right]` are returned because\n      the real-to-complex Fourier transform satisfies the conjugate symmetry,\n      i.e., :math:`X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*`.\n      Note if the input or window tensors are complex, then :attr:`onesided`\n      output is not possible.\n\n    * If :attr:`normalized` is ``True`` (default is ``False``), the function\n      returns the normalized STFT results, i.e., multiplied by :math:`(\\text{frame\\_length})^{-0.5}`.\n\n    * If :attr:`return_complex` is ``True`` (default if input is complex), the\n      return is a ``input.dim() + 1`` dimensional complex tensor. If ``False``,\n      the output is a ``input.dim() + 2`` dimensional real tensor where the last\n      dimension represents the real and imaginary components.\n\n    Returns either a complex tensor of size :math:`(* \\times N \\times T)` if\n    :attr:`return_complex` is true, or a real tensor of size :math:`(* \\times N\n    \\times T \\times 2)`. Where :math:`*` is the optional batch size of\n    :attr:`input`, :math:`N` is the number of frequencies where STFT is applied\n    and :math:`T` is the total number of frames used.\n\n    .. warning::\n      This function changed signature at version 0.4.1. Calling with the\n      previous signature may cause error or return incorrect result.\n\n    Args:\n        input (Tensor): the input tensor of shape `(B?, L)` where `B?` is an optional\n            batch dimension\n        n_fft (int): size of Fourier transform\n        hop_length (int, optional): the distance between neighboring sliding window\n            frames. Default: ``None`` (treated as equal to ``floor(n_fft / 4)``)\n        win_length (int, optional): the size of window frame and STFT filter.\n            Default: ``None``  (treated as equal to :attr:`n_fft`)\n        window (Tensor, optional): the optional window function.\n            Shape must be 1d and `<= n_fft`\n            Default: ``None`` (treated as window of all :math:`1` s)\n        center (bool, optional): whether to pad :attr:`input` on both sides so\n            that the :math:`t`-th frame is centered at time :math:`t \\times \\text{hop\\_length}`.\n            Default: ``True``\n        pad_mode (str, optional): controls the padding method used when\n            :attr:`center` is ``True``. Default: ``\"reflect\"``\n        normalized (bool, optional): controls whether to return the normalized STFT results\n             Default: ``False``\n        onesided (bool, optional): controls whether to return half of results to\n            avoid redundancy for real inputs.\n            Default: ``True`` for real :attr:`input` and :attr:`window`, ``False`` otherwise.\n        return_complex (bool, optional): whether to return a complex tensor, or\n            a real tensor with an extra last dimension for the real and\n            imaginary components.\n\n            .. versionchanged:: 2.0\n               ``return_complex`` is now a required argument for real inputs,\n               as the default is being transitioned to ``True``.\n\n            .. deprecated:: 2.0\n               ``return_complex=False`` is deprecated, instead use ``return_complex=True``\n               Note that calling :func:`torch.view_as_real` on the output will\n               recover the deprecated output format.\n\n    Returns:\n        Tensor: A tensor containing the STFT result with shape `(B?, N, T, C?)` where\n           - `B?` is an optional batch dimension from the input.\n           - `N` is the number of frequency samples, `(n_fft // 2) + 1` for\n             `onesided=True`, or otherwise `n_fft`.\n           - `T` is the number of frames, `1 + L // hop_length`\n             for `center=True`, or `1 + (L - n_fft) // hop_length` otherwise.\n           - `C?` is an optional length-2 dimension of real and imaginary\n             components, present when `return_complex=False`.\n\n    ",
      "arguments": [
        "input",
        "n_fft",
        "hop_length",
        "win_length",
        "window",
        "center",
        "pad_mode",
        "normalized",
        "onesided",
        "return_complex",
        "align_to_window"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Short-time Fourier transform (STFT).\n\n    .. warning::\n        From version 1.8.0, :attr:`return_complex` must always be given\n        explicitly for real inputs and `return_complex=False` has been\n        deprecated. Strongly prefer `return_complex=True` as in a future\n        pytorch release, this function will only return complex tensors.\n\n        Note that :func:`torch.view_as_real` can be used to recover a real\n        tensor with an extra last dimension for real and imaginary components.\n\n    .. warning::\n        From version 2.1, a warning will be provided if a :attr:`window` is\n        not specified. In a future release, this attribute will be required.\n        Not providing a window currently defaults to using a rectangular window,\n        which may result in undesirable artifacts. Consider using tapered windows,\n        such as :func:`torch.hann_window`.\n\n    The STFT computes the Fourier transform of short overlapping windows of the\n    input. This giving frequency components of the signal as they change over\n    time. The interface of this function is modeled after (but *not* a drop-in\n    replacement for) librosa_ stft function.\n\n    .. _librosa: https://librosa.org/doc/latest/generated/librosa.stft.html\n\n    Ignoring the optional batch dimension, this method computes the following\n    expression:\n\n    .. math::\n        X[\\omega, m] = \\sum_{k = 0}^{\\text{win\\_length-1}}%\n                            \\text{window}[k]\\ \\text{input}[m \\times \\text{hop\\_length} + k]\\ %\n                            \\exp\\left(- j \\frac{2 \\pi \\cdot \\omega k}{\\text{n\\_fft}}\\right),\n\n    where :math:`m` is the index of the sliding window, and :math:`\\omega` is\n    the frequency :math:`0 \\leq \\omega < \\text{n\\_fft}` for ``onesided=False``,\n    or :math:`0 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 1` for ``onesided=True``.\n\n    * :attr:`input` must be either a 1-D time sequence or a 2-D batch of time\n      sequences.\n\n    * If :attr:`hop_length` is ``None`` (default), it is treated as equal to\n      ``floor(n_fft / 4)``.\n\n    * If :attr:`win_length` is ``None`` (default), it is treated as equal to\n      :attr:`n_fft`.\n\n    * :attr:`window` can be a 1-D tensor of size :attr:`win_length`, e.g., from\n      :meth:`torch.hann_window`. If :attr:`window` is ``None`` (default), it is\n      treated as if having :math:`1` everywhere in the window. If\n      :math:`\\text{win\\_length} < \\text{n\\_fft}`, :attr:`window` will be padded on\n      both sides to length :attr:`n_fft` before being applied.\n\n    * If :attr:`center` is ``True`` (default), :attr:`input` will be padded on\n      both sides so that the :math:`t`-th frame is centered at time\n      :math:`t \\times \\text{hop\\_length}`. Otherwise, the :math:`t`-th frame\n      begins at time  :math:`t \\times \\text{hop\\_length}`.\n\n    * :attr:`pad_mode` determines the padding method used on :attr:`input` when\n      :attr:`center` is ``True``. See :meth:`torch.nn.functional.pad` for\n      all available options. Default is ``\"reflect\"``.\n\n    * If :attr:`onesided` is ``True`` (default for real input), only values for\n      :math:`\\omega` in :math:`\\left[0, 1, 2, \\dots, \\left\\lfloor\n      \\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right]` are returned because\n      the real-to-complex Fourier transform satisfies the conjugate symmetry,\n      i.e., :math:`X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*`.\n      Note if the input or window tensors are complex, then :attr:`onesided`\n      output is not possible.\n\n    * If :attr:`normalized` is ``True`` (default is ``False``), the function\n      returns the normalized STFT results, i.e., multiplied by :math:`(\\text{frame\\_length})^{-0.5}`.\n\n    * If :attr:`return_complex` is ``True`` (default if input is complex), the\n      return is a ``input.dim() + 1`` dimensional complex tensor. If ``False``,\n      the output is a ``input.dim() + 2`` dimensional real tensor where the last\n      dimension represents the real and imaginary components.\n\n    Returns either a complex tensor of size :math:`(* \\times N \\times T)` if\n    :attr:`return_complex` is true, or a real tensor of size :math:`(* \\times N\n    \\times T \\times 2)`. Where :math:`*` is the optional batch size of\n    :attr:`input`, :math:`N` is the number of frequencies where STFT is applied\n    and :math:`T` is the total number of frames used.\n\n    .. warning::\n      This function changed signature at version 0.4.1. Calling with the\n      previous signature may cause error or return incorrect result.\n\n    Args:\n        input (Tensor): the input tensor of shape `(B?, L)` where `B?` is an optional\n            batch dimension\n        n_fft (int): size of Fourier transform\n        hop_length (int, optional): the distance between neighboring sliding window\n            frames. Default: ``None`` (treated as equal to ``floor(n_fft / 4)``)\n        win_length (int, optional): the size of window frame and STFT filter.\n            Default: ``None``  (treated as equal to :attr:`n_fft`)\n        window (Tensor, optional): the optional window function.\n            Shape must be 1d and `<= n_fft`\n            Default: ``None`` (treated as window of all :math:`1` s)\n        center (bool, optional): whether to pad :attr:`input` on both sides so\n            that the :math:`t`-th frame is centered at time :math:`t \\times \\text{hop\\_length}`.\n            Default: ``True``\n        pad_mode (str, optional): controls the padding method used when\n            :attr:`center` is ``True``. Default: ``\"reflect\"``\n        normalized (bool, optional): controls whether to return the normalized STFT results\n             Default: ``False``\n        onesided (bool, optional): controls whether to return half of results to\n            avoid redundancy for real inputs.\n            Default: ``True`` for real :attr:`input` and :attr:`window`, ``False`` otherwise.\n        return_complex (bool, optional): whether to return a complex tensor, or\n            a real tensor with an extra last dimension for the real and\n            imaginary components.\n\n            .. versionchanged:: 2.0\n               ``return_complex`` is now a required argument for real inputs,\n               as the default is being transitioned to ``True``.\n\n            .. deprecated:: 2.0\n               ``return_complex=False`` is deprecated, instead use ``return_complex=True``\n               Note that calling :func:`torch.view_as_real` on the output will\n               recover the deprecated output format.\n\n    Returns:\n        Tensor: A tensor containing the STFT result with shape `(B?, N, T, C?)` where\n           - `B?` is an optional batch dimension from the input.\n           - `N` is the number of frequency samples, `(n_fft // 2) + 1` for\n             `onesided=True`, or otherwise `n_fft`.\n           - `T` is the number of frames, `1 + L // hop_length`\n             for `center=True`, or `1 + (L - n_fft) // hop_length` otherwise.\n           - `C?` is an optional length-2 dimension of real and imaginary\n             components, present when `return_complex=False`.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.svd_lowrank",
      "signature": "torch.svd_lowrank(A: torch.Tensor, q: Optional[int] = 6, niter: Optional[int] = 2, M: Optional[torch.Tensor] = None) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
      "doc": "Return the singular value decomposition ``(U, S, V)`` of a matrix,\n    batches of matrices, or a sparse matrix :math:`A` such that\n    :math:`A \\approx U \\operatorname{diag}(S) V^{\\text{H}}`. In case :math:`M` is given, then\n    SVD is computed for the matrix :math:`A - M`.\n\n    .. note:: The implementation is based on the Algorithm 5.1 from\n              Halko et al., 2009.\n\n    .. note:: For an adequate approximation of a k-rank matrix\n              :math:`A`, where k is not known in advance but could be\n              estimated, the number of :math:`Q` columns, q, can be\n              choosen according to the following criteria: in general,\n              :math:`k <= q <= min(2*k, m, n)`. For large low-rank\n              matrices, take :math:`q = k + 5..10`.  If k is\n              relatively small compared to :math:`min(m, n)`, choosing\n              :math:`q = k + 0..2` may be sufficient.\n\n    .. note:: This is a randomized method. To obtain repeatable results,\n              set the seed for the pseudorandom number generator\n\n    .. note:: In general, use the full-rank SVD implementation\n              :func:`torch.linalg.svd` for dense matrices due to its 10x\n              higher performance characteristics. The low-rank SVD\n              will be useful for huge sparse matrices that\n              :func:`torch.linalg.svd` cannot handle.\n\n    Args::\n        A (Tensor): the input tensor of size :math:`(*, m, n)`\n\n        q (int, optional): a slightly overestimated rank of A.\n\n        niter (int, optional): the number of subspace iterations to\n                               conduct; niter must be a nonnegative\n                               integer, and defaults to 2\n\n        M (Tensor, optional): the input tensor's mean of size\n                              :math:`(*, m, n)`, which will be broadcasted\n                              to the size of A in this function.\n\n    References::\n        - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n          structure with randomness: probabilistic algorithms for\n          constructing approximate matrix decompositions,\n          arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n          `arXiv <https://arxiv.org/abs/0909.4061>`_).\n\n    ",
      "arguments": [
        "A",
        "q",
        "niter",
        "M"
      ],
      "return_type": "tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the singular value decomposition ``(U, S, V)`` of a matrix,\n    batches of matrices, or a sparse matrix :math:`A` such that\n    :math:`A \\approx U \\operatorname{diag}(S) V^{\\text{H}}`. In case :math:`M` is given, then\n    SVD is computed for the matrix :math:`A - M`.\n\n    .. note:: The implementation is based on the Algorithm 5.1 from\n              Halko et al., 2009.\n\n    .. note:: For an adequate approximation of a k-rank matrix\n              :math:`A`, where k is not known in advance but could be\n              estimated, the number of :math:`Q` columns, q, can be\n              choosen according to the following criteria: in general,\n              :math:`k <= q <= min(2*k, m, n)`. For large low-rank\n              matrices, take :math:`q = k + 5..10`.  If k is\n              relatively small compared to :math:`min(m, n)`, choosing\n              :math:`q = k + 0..2` may be sufficient.\n\n    .. note:: This is a randomized method. To obtain repeatable results,\n              set the seed for the pseudorandom number generator\n\n    .. note:: In general, use the full-rank SVD implementation\n              :func:`torch.linalg.svd` for dense matrices due to its 10x\n              higher performance characteristics. The low-rank SVD\n              will be useful for huge sparse matrices that\n              :func:`torch.linalg.svd` cannot handle.\n\n    Args::\n        A (Tensor): the input tensor of size :math:`(*, m, n)`\n\n        q (int, optional): a slightly overestimated rank of A.\n\n        niter (int, optional): the number of subspace iterations to\n                               conduct; niter must be a nonnegative\n                               integer, and defaults to 2\n\n        M (Tensor, optional): the input tensor's mean of size\n                              :math:`(*, m, n)`, which will be broadcasted\n                              to the size of A in this function.\n\n    References::\n        - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n          structure with randomness: probabilistic algorithms for\n          constructing approximate matrix decompositions,\n          arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n          `arXiv <https://arxiv.org/abs/0909.4061>`_).\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_float",
      "signature": "torch.sym_float(a)",
      "doc": "SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_fresh_size",
      "signature": "torch.sym_fresh_size(expr)",
      "doc": "",
      "arguments": [
        "expr"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_int",
      "signature": "torch.sym_int(a)",
      "doc": "SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_ite",
      "signature": "torch.sym_ite(b, t, f)",
      "doc": "",
      "arguments": [
        "b",
        "t",
        "f"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_max",
      "signature": "torch.sym_max(a, b)",
      "doc": "\n    SymInt-aware utility for max which avoids branching on a < b.\n    Unlike builtins.max(), this only works for int/float, and it always\n    promotes to float if any argument is float (unlike builtins.max, which\n    will faithfully preserve the type of the input argument).\n    ",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    SymInt-aware utility for max which avoids branching on a < b.\n    Unlike builtins.max(), this only works for int/float, and it always\n    promotes to float if any argument is float (unlike builtins.max, which\n    will faithfully preserve the type of the input argument).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_min",
      "signature": "torch.sym_min(a, b)",
      "doc": "SymInt-aware utility for min().",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for min().",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_not",
      "signature": "torch.sym_not(a)",
      "doc": "SymInt-aware utility for logical negation.\n\n    Args:\n        a (SymBool or bool): Object to negate\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for logical negation.\n\n    Args:\n        a (SymBool or bool): Object to negate\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_sqrt",
      "signature": "torch.sym_sqrt(a)",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sym_sum",
      "signature": "torch.sym_sum(args)",
      "doc": "\n    N-ary add which is faster to compute for long lists than iterated binary\n    addition.  Only does something special for integers.\n    ",
      "arguments": [
        "args"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    N-ary add which is faster to compute for long lists than iterated binary\n    addition.  Only does something special for integers.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.symeig",
      "signature": "torch.symeig(input, eigenvectors=False, upper=True, *, out=None) -> tuple[torch.Tensor, torch.Tensor]",
      "doc": "",
      "arguments": [
        "input",
        "eigenvectors",
        "upper",
        "out"
      ],
      "return_type": "tuple[torch.Tensor, torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.tensordot",
      "signature": "torch.tensordot(a, b, dims=2, out: Optional[torch.Tensor] = None)",
      "doc": "Returns a contraction of a and b over multiple dimensions.\n\n    :attr:`tensordot` implements a generalized matrix product.\n\n    Args:\n      a (Tensor): Left tensor to contract\n      b (Tensor): Right tensor to contract\n      dims (int or Tuple[List[int], List[int]] or List[List[int]] containing two lists or Tensor): number of dimensions to\n         contract or explicit lists of dimensions for :attr:`a` and\n         :attr:`b` respectively\n\n    When called with a non-negative integer argument :attr:`dims` = :math:`d`, and\n    the number of dimensions of :attr:`a` and :attr:`b` is :math:`m` and :math:`n`,\n    respectively, :func:`~torch.tensordot` computes\n\n    .. math::\n        r_{i_0,...,i_{m-d}, i_d,...,i_n}\n          = \\sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \\times b_{k_0,...,k_{d-1}, i_d,...,i_n}.\n\n    When called with :attr:`dims` of the list form, the given dimensions will be contracted\n    in place of the last :math:`d` of :attr:`a` and the first :math:`d` of :math:`b`. The sizes\n    in these dimensions must match, but :func:`~torch.tensordot` will deal with broadcasted\n    dimensions.\n\n    Examples::\n\n        >>> a = torch.arange(60.).reshape(3, 4, 5)\n        >>> b = torch.arange(24.).reshape(4, 3, 2)\n        >>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\n        tensor([[4400., 4730.],\n                [4532., 4874.],\n                [4664., 5018.],\n                [4796., 5162.],\n                [4928., 5306.]])\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> a = torch.randn(3, 4, 5, device='cuda')\n        >>> b = torch.randn(4, 5, 6, device='cuda')\n        >>> c = torch.tensordot(a, b, dims=2).cpu()\n        tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n                [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n                [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n        >>> a = torch.randn(3, 5, 4, 6)\n        >>> b = torch.randn(6, 4, 5, 3)\n        >>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\n        tensor([[  7.7193,  -2.4867, -10.3204],\n                [  1.5513, -14.4737,  -6.5113],\n                [ -0.2850,   4.2573,  -3.5997]])\n    ",
      "arguments": [
        "a",
        "b",
        "dims",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns a contraction of a and b over multiple dimensions.\n\n    :attr:`tensordot` implements a generalized matrix product.\n\n    Args:\n      a (Tensor): Left tensor to contract\n      b (Tensor): Right tensor to contract\n      dims (int or Tuple[List[int], List[int]] or List[List[int]] containing two lists or Tensor): number of dimensions to\n         contract or explicit lists of dimensions for :attr:`a` and\n         :attr:`b` respectively\n\n    When called with a non-negative integer argument :attr:`dims` = :math:`d`, and\n    the number of dimensions of :attr:`a` and :attr:`b` is :math:`m` and :math:`n`,\n    respectively, :func:`~torch.tensordot` computes\n\n    .. math::\n        r_{i_0,...,i_{m-d}, i_d,...,i_n}\n          = \\sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \\times b_{k_0,...,k_{d-1}, i_d,...,i_n}.\n\n    When called with :attr:`dims` of the list form, the given dimensions will be contracted\n    in place of the last :math:`d` of :attr:`a` and the first :math:`d` of :math:`b`. The sizes\n    in these dimensions must match, but :func:`~torch.tensordot` will deal with broadcasted\n    dimensions.\n\n    Examples::\n\n        >>> a = torch.arange(60.).reshape(3, 4, 5)\n        >>> b = torch.arange(24.).reshape(4, 3, 2)\n        >>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\n        tensor([[4400., 4730.],\n                [4532., 4874.],\n                [4664., 5018.],\n                [4796., 5162.],\n                [4928., 5306.]])\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> a = torch.randn(3, 4, 5, device='cuda')\n        >>> b = torch.randn(4, 5, 6, device='cuda')\n        >>> c = torch.tensordot(a, b, dims=2).cpu()\n        tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n                [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n                [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n        >>> a = torch.randn(3, 5, 4, 6)\n        >>> b = torch.randn(6, 4, 5, 3)\n        >>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\n        tensor([[  7.7193,  -2.4867, -10.3204],\n                [  1.5513, -14.4737,  -6.5113],\n                [ -0.2850,   4.2573,  -3.5997]])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.typename",
      "signature": "torch.typename(obj: Any, /) -> str",
      "doc": "\n    String representation of the type of an object.\n\n    This function returns a fully qualified string representation of an object's type.\n    Args:\n        obj (object): The object whose type to represent\n    Returns:\n        str: the type of the object `o`\n    Example:\n        >>> x = torch.tensor([1, 2, 3])\n        >>> torch.typename(x)\n        'torch.LongTensor'\n        >>> torch.typename(torch.nn.Parameter)\n        'torch.nn.parameter.Parameter'\n    ",
      "arguments": [
        "obj"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    String representation of the type of an object.\n\n    This function returns a fully qualified string representation of an object's type.\n    Args:\n        obj (object): The object whose type to represent\n    Returns:\n        str: the type of the object `o`\n    Example:\n        >>> x = torch.tensor([1, 2, 3])\n        >>> torch.typename(x)\n        'torch.LongTensor'\n        >>> torch.typename(torch.nn.Parameter)\n        'torch.nn.parameter.Parameter'\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.unique",
      "signature": "torch.unique(*args, **kwargs)",
      "doc": "unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None) -> tuple[Tensor, Tensor, Tensor]\n\n    Returns the unique elements of the input tensor.\n\n    .. note:: This function is different from :func:`torch.unique_consecutive` in the sense that\n        this function also eliminates non-consecutive duplicate values.\n\n    .. note:: Currently in the CUDA implementation and the CPU implementation,\n        `torch.unique` always sort the tensor at the beginning regardless of the `sort` argument.\n        Sorting could be slow, so if your input tensor is already sorted, it is recommended to use\n        :func:`torch.unique_consecutive` which avoids the sorting.\n\n    Args:\n        input (Tensor): the input tensor\n        sorted (bool): Whether to sort the unique elements in ascending order\n            before returning as output.\n        return_inverse (bool): Whether to also return the indices for where\n            elements in the original input ended up in the returned unique list.\n        return_counts (bool): Whether to also return the counts for each unique\n            element.\n        dim (int, optional): the dimension to operate upon. If ``None``, the\n            unique of the flattened input is returned. Otherwise, each of the\n            tensors indexed by the given dimension is treated as one of the\n            elements to apply the unique operation upon. See examples for more\n            details. Default: ``None``\n\n    Returns:\n        (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing\n\n            - **output** (*Tensor*): the output list of unique scalar elements.\n            - **inverse_indices** (*Tensor*): (optional) if\n              :attr:`return_inverse` is True, there will be an additional\n              returned tensor (same shape as input) representing the indices\n              for where elements in the original input map to in the output;\n              otherwise, this function will only return a single tensor.\n            - **counts** (*Tensor*): (optional) if\n              :attr:`return_counts` is True, there will be an additional\n              returned tensor (same shape as output or output.size(dim),\n              if dim was specified) representing the number of occurrences\n              for each unique value or tensor.\n\n    Example::\n\n        >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n        >>> output\n        tensor([1, 2, 3])\n\n        >>> output, inverse_indices = torch.unique(\n        ...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n        >>> output\n        tensor([1, 2, 3])\n        >>> inverse_indices\n        tensor([0, 2, 1, 2])\n\n        >>> output, inverse_indices = torch.unique(\n        ...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n        >>> output\n        tensor([1, 2, 3])\n        >>> inverse_indices\n        tensor([[0, 2],\n                [1, 2]])\n\n        >>> a = torch.tensor([\n        ...     [\n        ...         [1, 1, 0, 0],\n        ...         [1, 1, 0, 0],\n        ...         [0, 0, 1, 1],\n        ...     ],\n        ...     [\n        ...         [0, 0, 1, 1],\n        ...         [0, 0, 1, 1],\n        ...         [1, 1, 1, 1],\n        ...     ],\n        ...     [\n        ...         [1, 1, 0, 0],\n        ...         [1, 1, 0, 0],\n        ...         [0, 0, 1, 1],\n        ...     ],\n        ... ])\n\n        >>> # If we call `torch.unique(a, dim=0)`, each of the tensors `a[idx, :, :]`\n        >>> # will be compared. We can see that `a[0, :, :]` and `a[2, :, :]` match\n        >>> # each other, so one of them will be removed.\n        >>> (a[0, :, :] == a[2, :, :]).all()\n        tensor(True)\n        >>> a_unique_dim0 = torch.unique(a, dim=0)\n        >>> a_unique_dim0\n        tensor([[[0, 0, 1, 1],\n                 [0, 0, 1, 1],\n                 [1, 1, 1, 1]],\n                [[1, 1, 0, 0],\n                 [1, 1, 0, 0],\n                 [0, 0, 1, 1]]])\n\n        >>> # Notice which sub-tensors from `a` match with the sub-tensors from\n        >>> # `a_unique_dim0`:\n        >>> (a_unique_dim0[0, :, :] == a[1, :, :]).all()\n        tensor(True)\n        >>> (a_unique_dim0[1, :, :] == a[0, :, :]).all()\n        tensor(True)\n\n        >>> # For `torch.unique(a, dim=1)`, each of the tensors `a[:, idx, :]` are\n        >>> # compared. `a[:, 0, :]` and `a[:, 1, :]` match each other, so one of\n        >>> # them will be removed.\n        >>> (a[:, 0, :] == a[:, 1, :]).all()\n        tensor(True)\n        >>> torch.unique(a, dim=1)\n        tensor([[[0, 0, 1, 1],\n                 [1, 1, 0, 0]],\n                [[1, 1, 1, 1],\n                 [0, 0, 1, 1]],\n                [[0, 0, 1, 1],\n                 [1, 1, 0, 0]]])\n\n        >>> # For `torch.unique(a, dim=2)`, the tensors `a[:, :, idx]` are compared.\n        >>> # `a[:, :, 0]` and `a[:, :, 1]` match each other. Also, `a[:, :, 2]` and\n        >>> # `a[:, :, 3]` match each other as well. So in this case, two of the\n        >>> # sub-tensors will be removed.\n        >>> (a[:, :, 0] == a[:, :, 1]).all()\n        tensor(True)\n        >>> (a[:, :, 2] == a[:, :, 3]).all()\n        tensor(True)\n        >>> torch.unique(a, dim=2)\n        tensor([[[0, 1],\n                 [0, 1],\n                 [1, 0]],\n                [[1, 0],\n                 [1, 0],\n                 [1, 1]],\n                [[0, 1],\n                 [0, 1],\n                 [1, 0]]])\n    ",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None) -> tuple[Tensor, Tensor, Tensor]\n\n    Returns the unique elements of the input tensor.\n\n    .. note:: This function is different from :func:`torch.unique_consecutive` in the sense that\n        this function also eliminates non-consecutive duplicate values.\n\n    .. note:: Currently in the CUDA implementation and the CPU implementation,\n        `torch.unique` always sort the tensor at the beginning regardless of the `sort` argument.\n        Sorting could be slow, so if your input tensor is already sorted, it is recommended to use\n        :func:`torch.unique_consecutive` which avoids the sorting.\n\n    Args:\n        input (Tensor): the input tensor\n        sorted (bool): Whether to sort the unique elements in ascending order\n            before returning as output.\n        return_inverse (bool): Whether to also return the indices for where\n            elements in the original input ended up in the returned unique list.\n        return_counts (bool): Whether to also return the counts for each unique\n            element.\n        dim (int, optional): the dimension to operate upon. If ``None``, the\n            unique of the flattened input is returned. Otherwise, each of the\n            tensors indexed by the given dimension is treated as one of the\n            elements to apply the unique operation upon. See examples for more\n            details. Default: ``None``\n\n    Returns:\n        (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing\n\n            - **output** (*Tensor*): the output list of unique scalar elements.\n            - **inverse_indices** (*Tensor*): (optional) if\n              :attr:`return_inverse` is True, there will be an additional\n              returned tensor (same shape as input) representing the indices\n              for where elements in the original input map to in the output;\n              otherwise, this function will only return a single tensor.\n            - **counts** (*Tensor*): (optional) if\n              :attr:`return_counts` is True, there will be an additional\n              returned tensor (same shape as output or output.size(dim),\n              if dim was specified) representing the number of occurrences\n              for each unique value or tensor.\n\n    Example::\n\n        >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n        >>> output\n        tensor([1, 2, 3])\n\n        >>> output, inverse_indices = torch.unique(\n        ...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n        >>> output\n        tensor([1, 2, 3])\n        >>> inverse_indices\n        tensor([0, 2, 1, 2])\n\n        >>> output, inverse_indices = torch.unique(\n        ...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n        >>> output\n        tensor([1, 2, 3])\n        >>> inverse_indices\n        tensor([[0, 2],\n                [1, 2]])\n\n        >>> a = torch.tensor([\n        ...     [\n        ...         [1, 1, 0, 0],\n        ...         [1, 1, 0, 0],\n        ...         [0, 0, 1, 1],\n        ...     ],\n        ...     [\n        ...         [0, 0, 1, 1],\n        ...         [0, 0, 1, 1],\n        ...         [1, 1, 1, 1],\n        ...     ],\n        ...     [\n        ...         [1, 1, 0, 0],\n        ...         [1, 1, 0, 0],\n        ...         [0, 0, 1, 1],\n        ...     ],\n        ... ])\n\n        >>> # If we call `torch.unique(a, dim=0)`, each of the tensors `a[idx, :, :]`\n        >>> # will be compared. We can see that `a[0, :, :]` and `a[2, :, :]` match\n        >>> # each other, so one of them will be removed.\n        >>> (a[0, :, :] == a[2, :, :]).all()\n        tensor(True)\n        >>> a_unique_dim0 = torch.unique(a, dim=0)\n        >>> a_unique_dim0\n        tensor([[[0, 0, 1, 1],\n                 [0, 0, 1, 1],\n                 [1, 1, 1, 1]],\n                [[1, 1, 0, 0],\n                 [1, 1, 0, 0],\n                 [0, 0, 1, 1]]])\n\n        >>> # Notice which sub-tensors from `a` match with the sub-tensors from\n        >>> # `a_unique_dim0`:\n        >>> (a_unique_dim0[0, :, :] == a[1, :, :]).all()\n        tensor(True)\n        >>> (a_unique_dim0[1, :, :] == a[0, :, :]).all()\n        tensor(True)\n\n        >>> # For `torch.unique(a, dim=1)`, each of the tensors `a[:, idx, :]` are\n        >>> # compared. `a[:, 0, :]` and `a[:, 1, :]` match each other, so one of\n        >>> # them will be removed.\n        >>> (a[:, 0, :] == a[:, 1, :]).all()\n        tensor(True)\n        >>> torch.unique(a, dim=1)\n        tensor([[[0, 0, 1, 1],\n                 [1, 1, 0, 0]],\n                [[1, 1, 1, 1],\n                 [0, 0, 1, 1]],\n                [[0, 0, 1, 1],\n                 [1, 1, 0, 0]]])\n\n        >>> # For `torch.unique(a, dim=2)`, the tensors `a[:, :, idx]` are compared.\n        >>> # `a[:, :, 0]` and `a[:, :, 1]` match each other. Also, `a[:, :, 2]` and\n        >>> # `a[:, :, 3]` match each other as well. So in this case, two of the\n        >>> # sub-tensors will be removed.\n        >>> (a[:, :, 0] == a[:, :, 1]).all()\n        tensor(True)\n        >>> (a[:, :, 2] == a[:, :, 3]).all()\n        tensor(True)\n        >>> torch.unique(a, dim=2)\n        tensor([[[0, 1],\n                 [0, 1],\n                 [1, 0]],\n                [[1, 0],\n                 [1, 0],\n                 [1, 1]],\n                [[0, 1],\n                 [0, 1],\n                 [1, 0]]])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.unique_consecutive",
      "signature": "torch.unique_consecutive(*args, **kwargs)",
      "doc": "Eliminates all but the first element from every consecutive group of equivalent elements.\n\n    .. note:: This function is different from :func:`torch.unique` in the sense that this function\n        only eliminates consecutive duplicate values. This semantics is similar to `std::unique`\n        in C++.\n\n    Args:\n        input (Tensor): the input tensor\n        return_inverse (bool): Whether to also return the indices for where\n            elements in the original input ended up in the returned unique list.\n        return_counts (bool): Whether to also return the counts for each unique\n            element.\n        dim (int): the dimension to apply unique. If ``None``, the unique of the\n            flattened input is returned. default: ``None``\n\n    Returns:\n        (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing\n\n            - **output** (*Tensor*): the output list of unique scalar elements.\n            - **inverse_indices** (*Tensor*): (optional) if\n              :attr:`return_inverse` is True, there will be an additional\n              returned tensor (same shape as input) representing the indices\n              for where elements in the original input map to in the output;\n              otherwise, this function will only return a single tensor.\n            - **counts** (*Tensor*): (optional) if\n              :attr:`return_counts` is True, there will be an additional\n              returned tensor (same shape as output or output.size(dim),\n              if dim was specified) representing the number of occurrences\n              for each unique value or tensor.\n\n    Example::\n\n        >>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n        >>> output = torch.unique_consecutive(x)\n        >>> output\n        tensor([1, 2, 3, 1, 2])\n\n        >>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n        >>> output\n        tensor([1, 2, 3, 1, 2])\n        >>> inverse_indices\n        tensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n        >>> output, counts = torch.unique_consecutive(x, return_counts=True)\n        >>> output\n        tensor([1, 2, 3, 1, 2])\n        >>> counts\n        tensor([2, 2, 1, 2, 1])\n    ",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Eliminates all but the first element from every consecutive group of equivalent elements.\n\n    .. note:: This function is different from :func:`torch.unique` in the sense that this function\n        only eliminates consecutive duplicate values. This semantics is similar to `std::unique`\n        in C++.\n\n    Args:\n        input (Tensor): the input tensor\n        return_inverse (bool): Whether to also return the indices for where\n            elements in the original input ended up in the returned unique list.\n        return_counts (bool): Whether to also return the counts for each unique\n            element.\n        dim (int): the dimension to apply unique. If ``None``, the unique of the\n            flattened input is returned. default: ``None``\n\n    Returns:\n        (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing\n\n            - **output** (*Tensor*): the output list of unique scalar elements.\n            - **inverse_indices** (*Tensor*): (optional) if\n              :attr:`return_inverse` is True, there will be an additional\n              returned tensor (same shape as input) representing the indices\n              for where elements in the original input map to in the output;\n              otherwise, this function will only return a single tensor.\n            - **counts** (*Tensor*): (optional) if\n              :attr:`return_counts` is True, there will be an additional\n              returned tensor (same shape as output or output.size(dim),\n              if dim was specified) representing the number of occurrences\n              for each unique value or tensor.\n\n    Example::\n\n        >>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n        >>> output = torch.unique_consecutive(x)\n        >>> output\n        tensor([1, 2, 3, 1, 2])\n\n        >>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n        >>> output\n        tensor([1, 2, 3, 1, 2])\n        >>> inverse_indices\n        tensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n        >>> output, counts = torch.unique_consecutive(x, return_counts=True)\n        >>> output\n        tensor([1, 2, 3, 1, 2])\n        >>> counts\n        tensor([2, 2, 1, 2, 1])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.unravel_index",
      "signature": "torch.unravel_index(indices: torch.Tensor, shape: Union[int, collections.abc.Sequence[int], torch.Size]) -> tuple[torch.Tensor, ...]",
      "doc": "Converts a tensor of flat indices into a tuple of coordinate tensors that\n    index into an arbitrary tensor of the specified shape.\n\n    Args:\n        indices (Tensor): An integer tensor containing indices into the\n            flattened version of an arbitrary tensor of shape :attr:`shape`.\n            All elements must be in the range ``[0, prod(shape) - 1]``.\n\n        shape (int, sequence of ints, or torch.Size): The shape of the arbitrary\n            tensor. All elements must be non-negative.\n\n    Returns:\n        tuple of Tensors: Each ``i``-th tensor in the output corresponds with\n        dimension ``i`` of :attr:`shape`. Each tensor has the same shape as\n        ``indices`` and contains one index into dimension ``i`` for each of the\n        flat indices given by ``indices``.\n\n    Example::\n\n        >>> import torch\n        >>> torch.unravel_index(torch.tensor(4), (3, 2))\n        (tensor(2),\n         tensor(0))\n\n        >>> torch.unravel_index(torch.tensor([4, 1]), (3, 2))\n        (tensor([2, 0]),\n         tensor([0, 1]))\n\n        >>> torch.unravel_index(torch.tensor([0, 1, 2, 3, 4, 5]), (3, 2))\n        (tensor([0, 0, 1, 1, 2, 2]),\n         tensor([0, 1, 0, 1, 0, 1]))\n\n        >>> torch.unravel_index(torch.tensor([1234, 5678]), (10, 10, 10, 10))\n        (tensor([1, 5]),\n         tensor([2, 6]),\n         tensor([3, 7]),\n         tensor([4, 8]))\n\n        >>> torch.unravel_index(torch.tensor([[1234], [5678]]), (10, 10, 10, 10))\n        (tensor([[1], [5]]),\n         tensor([[2], [6]]),\n         tensor([[3], [7]]),\n         tensor([[4], [8]]))\n\n        >>> torch.unravel_index(torch.tensor([[1234], [5678]]), (100, 100))\n        (tensor([[12], [56]]),\n         tensor([[34], [78]]))\n    ",
      "arguments": [
        "indices",
        "shape"
      ],
      "return_type": "tuple[torch.Tensor, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Converts a tensor of flat indices into a tuple of coordinate tensors that\n    index into an arbitrary tensor of the specified shape.\n\n    Args:\n        indices (Tensor): An integer tensor containing indices into the\n            flattened version of an arbitrary tensor of shape :attr:`shape`.\n            All elements must be in the range ``[0, prod(shape) - 1]``.\n\n        shape (int, sequence of ints, or torch.Size): The shape of the arbitrary\n            tensor. All elements must be non-negative.\n\n    Returns:\n        tuple of Tensors: Each ``i``-th tensor in the output corresponds with\n        dimension ``i`` of :attr:`shape`. Each tensor has the same shape as\n        ``indices`` and contains one index into dimension ``i`` for each of the\n        flat indices given by ``indices``.\n\n    Example::\n\n        >>> import torch\n        >>> torch.unravel_index(torch.tensor(4), (3, 2))\n        (tensor(2),\n         tensor(0))\n\n        >>> torch.unravel_index(torch.tensor([4, 1]), (3, 2))\n        (tensor([2, 0]),\n         tensor([0, 1]))\n\n        >>> torch.unravel_index(torch.tensor([0, 1, 2, 3, 4, 5]), (3, 2))\n        (tensor([0, 0, 1, 1, 2, 2]),\n         tensor([0, 1, 0, 1, 0, 1]))\n\n        >>> torch.unravel_index(torch.tensor([1234, 5678]), (10, 10, 10, 10))\n        (tensor([1, 5]),\n         tensor([2, 6]),\n         tensor([3, 7]),\n         tensor([4, 8]))\n\n        >>> torch.unravel_index(torch.tensor([[1234], [5678]]), (10, 10, 10, 10))\n        (tensor([[1], [5]]),\n         tensor([[2], [6]]),\n         tensor([[3], [7]]),\n         tensor([[4], [8]]))\n\n        >>> torch.unravel_index(torch.tensor([[1234], [5678]]), (100, 100))\n        (tensor([[12], [56]]),\n         tensor([[34], [78]]))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.use_deterministic_algorithms",
      "signature": "torch.use_deterministic_algorithms(mode: bool, *, warn_only: bool = False) -> None",
      "doc": "Sets whether PyTorch operations must use \"deterministic\"\n    algorithms. That is, algorithms which, given the same input, and when\n    run on the same software and hardware, always produce the same output.\n    When enabled, operations will use deterministic algorithms when available,\n    and if only nondeterministic algorithms are available they will throw a\n    :class:`RuntimeError` when called.\n\n    .. note:: This setting alone is not always enough to make an application\n        reproducible. Refer to :ref:`reproducibility` for more information.\n\n    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative\n        interface for this feature.\n\n    The following normally-nondeterministic operations will act\n    deterministically when ``mode=True``:\n\n        * :class:`torch.nn.Conv1d` when called on CUDA tensor\n        * :class:`torch.nn.Conv2d` when called on CUDA tensor\n        * :class:`torch.nn.Conv3d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor\n        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor\n        * :func:`torch.bmm` when called on sparse-dense CUDA tensors\n        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor\n          and the index is a list of tensors\n        * :func:`torch.Tensor.index_put` with ``accumulate=False``\n        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor\n        * :func:`torch.gather` when called on a CUDA tensor that requires grad\n        * :func:`torch.index_add` when called on CUDA tensor\n        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor\n        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor\n        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor\n        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='sum'`` or ``reduce='mean'`` and called on CUDA tensor\n\n    The following normally-nondeterministic operations will throw a\n    :class:`RuntimeError` when ``mode=True``:\n\n        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxUnpool1d`\n        * :class:`torch.nn.MaxUnpool2d`\n        * :class:`torch.nn.MaxUnpool3d`\n        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor\n          and one of the following modes is used:\n\n          - ``linear``\n          - ``bilinear``\n          - ``bicubic``\n          - ``trilinear``\n\n        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor\n        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when\n          ``mode='max'``\n        * :func:`torch.Tensor.put_` when ``accumulate=False``\n        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor\n        * :func:`torch.histc` when called on a CUDA tensor\n        * :func:`torch.bincount` when called on a CUDA tensor and ``weights``\n          tensor is given\n        * :func:`torch.kthvalue` with called on a CUDA tensor\n        * :func:`torch.median` with indices output when called on a CUDA tensor\n        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor\n        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='prod'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_` when called with a quantized tensor\n\n    In addition, several operations fill uninitialized memory when this setting\n    is turned on and when\n    :attr:`torch.utils.deterministic.fill_uninitialized_memory` is turned on.\n    See the documentation for that attribute for more information.\n\n    A handful of CUDA operations are nondeterministic if the CUDA version is\n    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``\n    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more\n    details: `<https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility>`_\n    If one of these environment variable configurations is not set, a :class:`RuntimeError`\n    will be raised from these operations when called with CUDA tensors:\n\n        * :func:`torch.mm`\n        * :func:`torch.mv`\n        * :func:`torch.bmm`\n\n    Note that deterministic operations tend to have worse performance than\n    nondeterministic operations.\n\n    .. note::\n\n        This flag does not detect or prevent nondeterministic behavior caused\n        by calling an inplace operation on a tensor with an internal memory\n        overlap or by giving such a tensor as the :attr:`out` argument for an\n        operation. In these cases, multiple writes of different data may target\n        a single memory location, and the order of writes is not guaranteed.\n\n    Args:\n        mode (:class:`bool`): If True, makes potentially nondeterministic\n            operations switch to a deterministic algorithm or throw a runtime\n            error. If False, allows nondeterministic operations.\n\n    Keyword args:\n        warn_only (:class:`bool`, optional): If True, operations that do not\n            have a deterministic implementation will throw a warning instead of\n            an error. Default: ``False``\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> torch.use_deterministic_algorithms(True)\n\n        # Forward mode nondeterministic error\n        >>> torch.randn(10, device='cuda').kthvalue(1)\n        ...\n        RuntimeError: kthvalue CUDA does not have a deterministic implementation...\n\n        # Backward mode nondeterministic error\n        >>> torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()\n        ...\n        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...\n    ",
      "arguments": [
        "mode",
        "warn_only"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets whether PyTorch operations must use \"deterministic\"\n    algorithms. That is, algorithms which, given the same input, and when\n    run on the same software and hardware, always produce the same output.\n    When enabled, operations will use deterministic algorithms when available,\n    and if only nondeterministic algorithms are available they will throw a\n    :class:`RuntimeError` when called.\n\n    .. note:: This setting alone is not always enough to make an application\n        reproducible. Refer to :ref:`reproducibility` for more information.\n\n    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative\n        interface for this feature.\n\n    The following normally-nondeterministic operations will act\n    deterministically when ``mode=True``:\n\n        * :class:`torch.nn.Conv1d` when called on CUDA tensor\n        * :class:`torch.nn.Conv2d` when called on CUDA tensor\n        * :class:`torch.nn.Conv3d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor\n        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor\n        * :func:`torch.bmm` when called on sparse-dense CUDA tensors\n        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor\n          and the index is a list of tensors\n        * :func:`torch.Tensor.index_put` with ``accumulate=False``\n        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor\n        * :func:`torch.gather` when called on a CUDA tensor that requires grad\n        * :func:`torch.index_add` when called on CUDA tensor\n        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor\n        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor\n        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor\n        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='sum'`` or ``reduce='mean'`` and called on CUDA tensor\n\n    The following normally-nondeterministic operations will throw a\n    :class:`RuntimeError` when ``mode=True``:\n\n        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxUnpool1d`\n        * :class:`torch.nn.MaxUnpool2d`\n        * :class:`torch.nn.MaxUnpool3d`\n        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor\n          and one of the following modes is used:\n\n          - ``linear``\n          - ``bilinear``\n          - ``bicubic``\n          - ``trilinear``\n\n        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor\n        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when\n          ``mode='max'``\n        * :func:`torch.Tensor.put_` when ``accumulate=False``\n        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor\n        * :func:`torch.histc` when called on a CUDA tensor\n        * :func:`torch.bincount` when called on a CUDA tensor and ``weights``\n          tensor is given\n        * :func:`torch.kthvalue` with called on a CUDA tensor\n        * :func:`torch.median` with indices output when called on a CUDA tensor\n        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor\n        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='prod'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_` when called with a quantized tensor\n\n    In addition, several operations fill uninitialized memory when this setting\n    is turned on and when\n    :attr:`torch.utils.deterministic.fill_uninitialized_memory` is turned on.\n    See the documentation for that attribute for more information.\n\n    A handful of CUDA operations are nondeterministic if the CUDA version is\n    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``\n    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more\n    details: `<https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility>`_\n    If one of these environment variable configurations is not set, a :class:`RuntimeError`\n    will be raised from these operations when called with CUDA tensors:\n\n        * :func:`torch.mm`\n        * :func:`torch.mv`\n        * :func:`torch.bmm`\n\n    Note that deterministic operations tend to have worse performance than\n    nondeterministic operations.\n\n    .. note::\n\n        This flag does not detect or prevent nondeterministic behavior caused\n        by calling an inplace operation on a tensor with an internal memory\n        overlap or by giving such a tensor as the :attr:`out` argument for an\n        operation. In these cases, multiple writes of different data may target\n        a single memory location, and the order of writes is not guaranteed.\n\n    Args:\n        mode (:class:`bool`): If True, makes potentially nondeterministic\n            operations switch to a deterministic algorithm or throw a runtime\n            error. If False, allows nondeterministic operations.\n\n    Keyword args:\n        warn_only (:class:`bool`, optional): If True, operations that do not\n            have a deterministic implementation will throw a warning instead of\n            an error. Default: ``False``\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> torch.use_deterministic_algorithms(True)\n\n        # Forward mode nondeterministic error\n        >>> torch.randn(10, device='cuda').kthvalue(1)\n        ...\n        RuntimeError: kthvalue CUDA does not have a deterministic implementation...\n\n        # Backward mode nondeterministic error\n        >>> torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()\n        ...\n        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.vmap",
      "signature": "torch.vmap(func: Callable, in_dims: Union[int, tuple] = 0, out_dims: Union[int, tuple[int, ...]] = 0, randomness: str = 'error', *, chunk_size=None) -> Callable",
      "doc": "\n    vmap is the vectorizing map; ``vmap(func)`` returns a new function that\n    maps ``func`` over some dimension of the inputs. Semantically, vmap\n    pushes the map into PyTorch operations called by ``func``, effectively\n    vectorizing those operations.\n\n    vmap is useful for handling batch dimensions: one can write a function\n    ``func`` that runs on examples and then lift it to a function that can\n    take batches of examples with ``vmap(func)``. vmap can also be used to\n    compute batched gradients when composed with autograd.\n\n    .. note::\n        :func:`torch.vmap` is aliased to :func:`torch.func.vmap` for\n        convenience. Use whichever one you'd like.\n\n    Args:\n        func (function): A Python function that takes one or more arguments.\n            Must return one or more Tensors.\n        in_dims (int or nested structure): Specifies which dimension of the\n            inputs should be mapped over. ``in_dims`` should have a\n            structure like the inputs. If the ``in_dim`` for a particular\n            input is None, then that indicates there is no map dimension.\n            Default: 0.\n        out_dims (int or Tuple[int]): Specifies where the mapped dimension\n            should appear in the outputs. If ``out_dims`` is a Tuple, then\n            it should have one element per output. Default: 0.\n        randomness (str): Specifies whether the randomness in this\n            vmap should be the same or different across batches. If 'different',\n            the randomness for each batch will be different. If 'same', the\n            randomness will be the same across batches. If 'error', any calls to\n            random functions will error. Default: 'error'. WARNING: this flag\n            only applies to random PyTorch operations and does not apply to\n            Python's random module or numpy randomness.\n        chunk_size (None or int): If None (default), apply a single vmap over inputs.\n            If not None, then compute the vmap :attr:`chunk_size` samples at a time.\n            Note that :attr:`chunk_size=1` is equivalent to computing the vmap with a for-loop.\n            If you run into memory issues computing the vmap, please try a non-None chunk_size.\n\n    Returns:\n        Returns a new \"batched\" function. It takes the same inputs as\n        ``func``, except each input has an extra dimension at the index\n        specified by ``in_dims``. It takes returns the same outputs as\n        ``func``, except each output has an extra dimension at the index\n        specified by ``out_dims``.\n\n    .. warning:\n        :func:`vmap` works best with functional-style code. Please do not\n        perform any side-effects in ``func``, with the exception of\n        in-place PyTorch operations. Examples of side-effects include mutating\n        Python data structures and assigning values to variables not captured\n        in ``func``.\n\n    One example of using :func:`vmap` is to compute batched dot products. PyTorch\n    doesn't provide a batched ``torch.dot`` API; instead of unsuccessfully\n    rummaging through docs, use :func:`vmap` to construct a new function.\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.func.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n        >>> x, y = torch.randn(2, 5), torch.randn(2, 5)\n        >>> batched_dot(x, y)\n\n    :func:`vmap` can be helpful in hiding batch dimensions, leading to a simpler\n    model authoring experience.\n\n        >>> batch_size, feature_size = 3, 5\n        >>> weights = torch.randn(feature_size, requires_grad=True)\n        >>>\n        >>> def model(feature_vec):\n        >>>     # Very simple linear model with activation\n        >>>     return feature_vec.dot(weights).relu()\n        >>>\n        >>> examples = torch.randn(batch_size, feature_size)\n        >>> result = torch.vmap(model)(examples)\n\n    :func:`vmap` can also help vectorize computations that were previously difficult\n    or impossible to batch. One example is higher-order gradient computation.\n    The PyTorch autograd engine computes vjps (vector-Jacobian products).\n    Computing a full Jacobian matrix for some function f: R^N -> R^N usually\n    requires N calls to ``autograd.grad``, one per Jacobian row. Using :func:`vmap`,\n    we can vectorize the whole computation, computing the Jacobian in a single\n    call to ``autograd.grad``.\n\n        >>> # Setup\n        >>> N = 5\n        >>> f = lambda x: x ** 2\n        >>> x = torch.randn(N, requires_grad=True)\n        >>> y = f(x)\n        >>> I_N = torch.eye(N)\n        >>>\n        >>> # Sequential approach\n        >>> jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]\n        >>>                  for v in I_N.unbind()]\n        >>> jacobian = torch.stack(jacobian_rows)\n        >>>\n        >>> # vectorized gradient computation\n        >>> def get_vjp(v):\n        >>>     return torch.autograd.grad(y, x, v)\n        >>> jacobian = torch.vmap(get_vjp)(I_N)\n\n    :func:`vmap` can also be nested, producing an output with multiple batched dimensions\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.vmap(torch.vmap(torch.dot))  # [N1, N0, D], [N1, N0, D] -> [N1, N0]\n        >>> x, y = torch.randn(2, 3, 5), torch.randn(2, 3, 5)\n        >>> batched_dot(x, y) # tensor of size [2, 3]\n\n    If the inputs are not batched along the first dimension, ``in_dims`` specifies\n    the dimension that each inputs are batched along as\n\n        >>> torch.dot                            # [N], [N] -> []\n        >>> batched_dot = torch.vmap(torch.dot, in_dims=1)  # [N, D], [N, D] -> [D]\n        >>> x, y = torch.randn(2, 5), torch.randn(2, 5)\n        >>> batched_dot(x, y)   # output is [5] instead of [2] if batched along the 0th dimension\n\n    If there are multiple inputs each of which is batched along different dimensions,\n    ``in_dims`` must be a tuple with the batch dimension for each input as\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.vmap(torch.dot, in_dims=(0, None))  # [N, D], [D] -> [N]\n        >>> x, y = torch.randn(2, 5), torch.randn(5)\n        >>> batched_dot(x, y) # second arg doesn't have a batch dim because in_dim[1] was None\n\n    If the input is a Python struct, ``in_dims`` must be a tuple containing a struct\n    matching the shape of the input:\n\n        >>> f = lambda dict: torch.dot(dict['x'], dict['y'])\n        >>> x, y = torch.randn(2, 5), torch.randn(5)\n        >>> input = {'x': x, 'y': y}\n        >>> batched_dot = torch.vmap(f, in_dims=({'x': 0, 'y': None},))\n        >>> batched_dot(input)\n\n    By default, the output is batched along the first dimension. However, it can be batched\n    along any dimension by using ``out_dims``\n\n        >>> f = lambda x: x ** 2\n        >>> x = torch.randn(2, 5)\n        >>> batched_pow = torch.vmap(f, out_dims=1)\n        >>> batched_pow(x) # [5, 2]\n\n    For any function that uses kwargs, the returned function will not batch the kwargs but will\n    accept kwargs\n\n        >>> x = torch.randn([2, 5])\n        >>> def fn(x, scale=4.):\n        >>>   return x * scale\n        >>>\n        >>> batched_pow = torch.vmap(fn)\n        >>> assert torch.allclose(batched_pow(x), x * 4)\n        >>> batched_pow(x, scale=x) # scale is not batched, output has shape [2, 2, 5]\n\n    .. note::\n        vmap does not provide general autobatching or handle variable-length\n        sequences out of the box.\n    ",
      "arguments": [
        "func",
        "in_dims",
        "out_dims",
        "randomness",
        "chunk_size"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    vmap is the vectorizing map; ``vmap(func)`` returns a new function that\n    maps ``func`` over some dimension of the inputs. Semantically, vmap\n    pushes the map into PyTorch operations called by ``func``, effectively\n    vectorizing those operations.\n\n    vmap is useful for handling batch dimensions: one can write a function\n    ``func`` that runs on examples and then lift it to a function that can\n    take batches of examples with ``vmap(func)``. vmap can also be used to\n    compute batched gradients when composed with autograd.\n\n    .. note::\n        :func:`torch.vmap` is aliased to :func:`torch.func.vmap` for\n        convenience. Use whichever one you'd like.\n\n    Args:\n        func (function): A Python function that takes one or more arguments.\n            Must return one or more Tensors.\n        in_dims (int or nested structure): Specifies which dimension of the\n            inputs should be mapped over. ``in_dims`` should have a\n            structure like the inputs. If the ``in_dim`` for a particular\n            input is None, then that indicates there is no map dimension.\n            Default: 0.\n        out_dims (int or Tuple[int]): Specifies where the mapped dimension\n            should appear in the outputs. If ``out_dims`` is a Tuple, then\n            it should have one element per output. Default: 0.\n        randomness (str): Specifies whether the randomness in this\n            vmap should be the same or different across batches. If 'different',\n            the randomness for each batch will be different. If 'same', the\n            randomness will be the same across batches. If 'error', any calls to\n            random functions will error. Default: 'error'. WARNING: this flag\n            only applies to random PyTorch operations and does not apply to\n            Python's random module or numpy randomness.\n        chunk_size (None or int): If None (default), apply a single vmap over inputs.\n            If not None, then compute the vmap :attr:`chunk_size` samples at a time.\n            Note that :attr:`chunk_size=1` is equivalent to computing the vmap with a for-loop.\n            If you run into memory issues computing the vmap, please try a non-None chunk_size.\n\n    Returns:\n        Returns a new \"batched\" function. It takes the same inputs as\n        ``func``, except each input has an extra dimension at the index\n        specified by ``in_dims``. It takes returns the same outputs as\n        ``func``, except each output has an extra dimension at the index\n        specified by ``out_dims``.\n\n    .. warning:\n        :func:`vmap` works best with functional-style code. Please do not\n        perform any side-effects in ``func``, with the exception of\n        in-place PyTorch operations. Examples of side-effects include mutating\n        Python data structures and assigning values to variables not captured\n        in ``func``.\n\n    One example of using :func:`vmap` is to compute batched dot products. PyTorch\n    doesn't provide a batched ``torch.dot`` API; instead of unsuccessfully\n    rummaging through docs, use :func:`vmap` to construct a new function.\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.func.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n        >>> x, y = torch.randn(2, 5), torch.randn(2, 5)\n        >>> batched_dot(x, y)\n\n    :func:`vmap` can be helpful in hiding batch dimensions, leading to a simpler\n    model authoring experience.\n\n        >>> batch_size, feature_size = 3, 5\n        >>> weights = torch.randn(feature_size, requires_grad=True)\n        >>>\n        >>> def model(feature_vec):\n        >>>     # Very simple linear model with activation\n        >>>     return feature_vec.dot(weights).relu()\n        >>>\n        >>> examples = torch.randn(batch_size, feature_size)\n        >>> result = torch.vmap(model)(examples)\n\n    :func:`vmap` can also help vectorize computations that were previously difficult\n    or impossible to batch. One example is higher-order gradient computation.\n    The PyTorch autograd engine computes vjps (vector-Jacobian products).\n    Computing a full Jacobian matrix for some function f: R^N -> R^N usually\n    requires N calls to ``autograd.grad``, one per Jacobian row. Using :func:`vmap`,\n    we can vectorize the whole computation, computing the Jacobian in a single\n    call to ``autograd.grad``.\n\n        >>> # Setup\n        >>> N = 5\n        >>> f = lambda x: x ** 2\n        >>> x = torch.randn(N, requires_grad=True)\n        >>> y = f(x)\n        >>> I_N = torch.eye(N)\n        >>>\n        >>> # Sequential approach\n        >>> jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]\n        >>>                  for v in I_N.unbind()]\n        >>> jacobian = torch.stack(jacobian_rows)\n        >>>\n        >>> # vectorized gradient computation\n        >>> def get_vjp(v):\n        >>>     return torch.autograd.grad(y, x, v)\n        >>> jacobian = torch.vmap(get_vjp)(I_N)\n\n    :func:`vmap` can also be nested, producing an output with multiple batched dimensions\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.vmap(torch.vmap(torch.dot))  # [N1, N0, D], [N1, N0, D] -> [N1, N0]\n        >>> x, y = torch.randn(2, 3, 5), torch.randn(2, 3, 5)\n        >>> batched_dot(x, y) # tensor of size [2, 3]\n\n    If the inputs are not batched along the first dimension, ``in_dims`` specifies\n    the dimension that each inputs are batched along as\n\n        >>> torch.dot                            # [N], [N] -> []\n        >>> batched_dot = torch.vmap(torch.dot, in_dims=1)  # [N, D], [N, D] -> [D]\n        >>> x, y = torch.randn(2, 5), torch.randn(2, 5)\n        >>> batched_dot(x, y)   # output is [5] instead of [2] if batched along the 0th dimension\n\n    If there are multiple inputs each of which is batched along different dimensions,\n    ``in_dims`` must be a tuple with the batch dimension for each input as\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.vmap(torch.dot, in_dims=(0, None))  # [N, D], [D] -> [N]\n        >>> x, y = torch.randn(2, 5), torch.randn(5)\n        >>> batched_dot(x, y) # second arg doesn't have a batch dim because in_dim[1] was None\n\n    If the input is a Python struct, ``in_dims`` must be a tuple containing a struct\n    matching the shape of the input:\n\n        >>> f = lambda dict: torch.dot(dict['x'], dict['y'])\n        >>> x, y = torch.randn(2, 5), torch.randn(5)\n        >>> input = {'x': x, 'y': y}\n        >>> batched_dot = torch.vmap(f, in_dims=({'x': 0, 'y': None},))\n        >>> batched_dot(input)\n\n    By default, the output is batched along the first dimension. However, it can be batched\n    along any dimension by using ``out_dims``\n\n        >>> f = lambda x: x ** 2\n        >>> x = torch.randn(2, 5)\n        >>> batched_pow = torch.vmap(f, out_dims=1)\n        >>> batched_pow(x) # [5, 2]\n\n    For any function that uses kwargs, the returned function will not batch the kwargs but will\n    accept kwargs\n\n        >>> x = torch.randn([2, 5])\n        >>> def fn(x, scale=4.):\n        >>>   return x * scale\n        >>>\n        >>> batched_pow = torch.vmap(fn)\n        >>> assert torch.allclose(batched_pow(x), x * 4)\n        >>> batched_pow(x, scale=x) # scale is not batched, output has shape [2, 2, 5]\n\n    .. note::\n        vmap does not provide general autobatching or handle variable-length\n        sequences out of the box.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.while_loop",
      "signature": "torch.while_loop(cond_fn, body_fn, carried_inputs)",
      "doc": "\n    Run body_fn(*carried_inputs) while cond_fn(*carried_inputs) returns a True scalar tensor. Returns the output of body_fn or\n    initial carried_inputs.\n\n    .. warning::\n        `torch.while_loop` is a prototype feature in PyTorch. It has limited support for input and output types and\n        doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\n        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    `while_loop` is a structured control flow operator. It preserves the loop semantic across the torch.compile and torch.export.\n\n    `while_loop` is equivalent to the following:\n\n        def while_loop(cond_fn, body_fn, carried_inputs):\n            val = carried_inputs\n            while cond_fn(*val):\n                val = body_fn(*val)\n            return val\n\n    Args:\n        cond_fn (Callable): A callable function that returns a boolean Scalar tensor or a python boolean.\n\n        body_fn (Callable): A callable function that takes the same inputs as `cond_fn` and returns a tuple of tensors or ints\n\n        carried_inputs (Tuple of possibly nested dict/list/tuple of tensors or ints): A tuple of inputs to cond_fn and body_fn.\n            It's also the initial value of states that are carried across iterations. Note that when pass an integer as carry,\n            the corresponding return of while_loop will be another int with unknown values because we don't know how many\n            iterations while_loop will run.\n\n    Example 1:\n\n        def cond_fn(iter, x):\n            return iter.sum() < 10\n\n        def body_fn(iter, x):\n            return iter + 1, x.sin()\n\n        while_loop(cond_fn, body_fn, (torch.zeros(1), torch.randn(3, 4)))\n\n    Example 2:\n\n        def cond_fn(int_iter, x):\n            return 2 * int_iter < x.shape[0]\n\n        def body_fn(int_iter, x):\n            return int_iter + 1, x + int_iter\n\n        while_loop(cond,_fn, body_fn, (0, torch.randn(3, 4)))\n\n    Restrictions:\n\n        - body_fn must return tensors or int with the same metadata (e.g.shape, dtype) as inputs.\n\n        - body_fn and cond_fn must not in-place mutate the carried_inputs. A clone before the mutation is required.\n\n        - body_fn and cond_fn must not mutate python varialbles (e.g. list/dict) created outside of the body_fn.\n\n        - body_fn and cond_fn's output cannot aliase any of the inputs. A clone is required.\n\n    .. warning::\n        Temporal Limitations:\n\n        - 'while_loop' only supports **inference** right now. Autograd will be supported in the future.\n\n    ",
      "arguments": [
        "cond_fn",
        "body_fn",
        "carried_inputs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Run body_fn(*carried_inputs) while cond_fn(*carried_inputs) returns a True scalar tensor. Returns the output of body_fn or\n    initial carried_inputs.\n\n    .. warning::\n        `torch.while_loop` is a prototype feature in PyTorch. It has limited support for input and output types and\n        doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\n        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    `while_loop` is a structured control flow operator. It preserves the loop semantic across the torch.compile and torch.export.\n\n    `while_loop` is equivalent to the following:\n\n        def while_loop(cond_fn, body_fn, carried_inputs):\n            val = carried_inputs\n            while cond_fn(*val):\n                val = body_fn(*val)\n            return val\n\n    Args:\n        cond_fn (Callable): A callable function that returns a boolean Scalar tensor or a python boolean.\n\n        body_fn (Callable): A callable function that takes the same inputs as `cond_fn` and returns a tuple of tensors or ints\n\n        carried_inputs (Tuple of possibly nested dict/list/tuple of tensors or ints): A tuple of inputs to cond_fn and body_fn.\n            It's also the initial value of states that are carried across iterations. Note that when pass an integer as carry,\n            the corresponding return of while_loop will be another int with unknown values because we don't know how many\n            iterations while_loop will run.\n\n    Example 1:\n\n        def cond_fn(iter, x):\n            return iter.sum() < 10\n\n        def body_fn(iter, x):\n            return iter + 1, x.sin()\n\n        while_loop(cond_fn, body_fn, (torch.zeros(1), torch.randn(3, 4)))\n\n    Example 2:\n\n        def cond_fn(int_iter, x):\n            return 2 * int_iter < x.shape[0]\n\n        def body_fn(int_iter, x):\n            return int_iter + 1, x + int_iter\n\n        while_loop(cond,_fn, body_fn, (0, torch.randn(3, 4)))\n\n    Restrictions:\n\n        - body_fn must return tensors or int with the same metadata (e.g.shape, dtype) as inputs.\n\n        - body_fn and cond_fn must not in-place mutate the carried_inputs. A clone before the mutation is required.\n\n        - body_fn and cond_fn must not mutate python varialbles (e.g. list/dict) created outside of the body_fn.\n\n        - body_fn and cond_fn's output cannot aliase any of the inputs. A clone is required.\n\n    .. warning::\n        Temporal Limitations:\n\n        - 'while_loop' only supports **inference** right now. Autograd will be supported in the future.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._decomp.core_aten_decompositions",
      "signature": "torch._decomp.core_aten_decompositions() -> 'CustomDecompTable'",
      "doc": "",
      "arguments": [],
      "return_type": "CustomDecompTable",
      "implementation_status": "complete",
      "implementation_notes": "Debug test",
      "status_updated": "2025-07-10",
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._decomp.get_decompositions",
      "signature": "torch._decomp.get_decompositions(aten_ops: collections.abc.Sequence[typing.Union[torch._ops.OperatorBase, torch._ops.OpOverloadPacket]], type: str = 'post_autograd') -> dict[torch._ops.OperatorBase, typing.Callable]",
      "doc": "\n    Retrieve a dictionary of decompositions corresponding to the list of\n    operator overloads and overload packets passed as input.  Overload\n    packets will include all decomposed overloads in the packet.  If there is\n    no decomposition for a requested operator, it is silently ignored.\n\n    This API is experimental; we are almost certainly going to give an alternate,\n    more recommended formulation, where a user provides the set of operators\n    they know how to implement, and we provide decompositions for everything\n    not in this set.\n    ",
      "arguments": [
        "aten_ops",
        "type"
      ],
      "return_type": "dict[torch._ops.OperatorBase, typing.Callable]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Retrieve a dictionary of decompositions corresponding to the list of\n    operator overloads and overload packets passed as input.  Overload\n    packets will include all decomposed overloads in the packet.  If there is\n    no decomposition for a requested operator, it is silently ignored.\n\n    This API is experimental; we are almost certainly going to give an alternate,\n    more recommended formulation, where a user provides the set of operators\n    they know how to implement, and we provide decompositions for everything\n    not in this set.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._decomp.lru_cache",
      "signature": "torch._decomp.lru_cache(maxsize=128, typed=False)",
      "doc": "Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    ",
      "arguments": [
        "maxsize",
        "typed"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._decomp.register_decomposition",
      "signature": "torch._decomp.register_decomposition(aten_op, registry=None, *, type='post_autograd', unsafe=False) -> Callable[[Callable[~_P, ~_T]], Callable[~_P, ~_T]]",
      "doc": "\n    A decorator to register a function as a decomposition to the Python\n    decomposition table.  Use it like this::\n\n        @register_decomposition(torch.ops.aten.clamp_min)\n        def clamp_min(x):\n            return torch.clamp(self, min=min)\n\n    If you are writing a new decomposition, consider contributing it\n    directly to PyTorch in torch._decomp.decompositions.\n\n    This API is experimental; we are almost certainly going to extend\n    the API when we make decompositions eligible for use in transforms (e.g.,\n    autograd) and not just backend tracing, where we then need to know if a\n    decomposition can be used to simulate a transform.\n\n    By default, we also will register it to the Meta key of dispatcher,\n    and replace the c++ Meta implementation if there is already one.\n\n    unsafe kwarg is for reuse of this function for registering non-function\n    things\n    ",
      "arguments": [
        "aten_op",
        "registry",
        "type",
        "unsafe"
      ],
      "return_type": "typing.Callable[[typing.Callable[~_P, ~_T]], typing.Callable[~_P, ~_T]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    A decorator to register a function as a decomposition to the Python\n    decomposition table.  Use it like this::\n\n        @register_decomposition(torch.ops.aten.clamp_min)\n        def clamp_min(x):\n            return torch.clamp(self, min=min)\n\n    If you are writing a new decomposition, consider contributing it\n    directly to PyTorch in torch._decomp.decompositions.\n\n    This API is experimental; we are almost certainly going to extend\n    the API when we make decompositions eligible for use in transforms (e.g.,\n    autograd) and not just backend tracing, where we then need to know if a\n    decomposition can be used to simulate a transform.\n\n    By default, we also will register it to the Meta key of dispatcher,\n    and replace the c++ Meta implementation if there is already one.\n\n    unsafe kwarg is for reuse of this function for registering non-function\n    things\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._decomp.remove_decompositions",
      "signature": "torch._decomp.remove_decompositions(decompositions: dict[torch._ops.OperatorBase, typing.Callable], aten_ops: collections.abc.Sequence[typing.Union[torch._ops.OpOverload, torch._ops.OpOverloadPacket]]) -> None",
      "doc": "\n    Given a dictionary of decompositions obtained from get_decompositions(), removes\n    operators associated with a list of operator overloads and overload packets passed\n    as input. If the decomposition dictionary does not contain a decomposition that is\n    specified to be removed, it is silently ignored.\n    ",
      "arguments": [
        "decompositions",
        "aten_ops"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Given a dictionary of decompositions obtained from get_decompositions(), removes\n    operators associated with a list of operator overloads and overload packets passed\n    as input. If the decomposition dictionary does not contain a decomposition that is\n    specified to be removed, it is silently ignored.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._decomp.wraps",
      "signature": "torch._decomp.wraps(wrapped, assigned=('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'), updated=('__dict__',))",
      "doc": "Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    ",
      "arguments": [
        "wrapped",
        "assigned",
        "updated"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.allow_in_graph",
      "signature": "torch._dynamo.allow_in_graph(fn)",
      "doc": "\n    Tells the compiler frontend (Dynamo) to skip symbolic introspection of the function\n    and instead directly write it to the graph when encountered.\n\n    See :func:`torch.compiler.allow_in_graph`'s docstring for the full documentation\n\n    WARNING: this API can be a footgun, please read the documentation carefully.\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Tells the compiler frontend (Dynamo) to skip symbolic introspection of the function\n    and instead directly write it to the graph when encountered.\n\n    See :func:`torch.compiler.allow_in_graph`'s docstring for the full documentation\n\n    WARNING: this API can be a footgun, please read the documentation carefully.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.assume_constant_result",
      "signature": "torch._dynamo.assume_constant_result(fn)",
      "doc": "",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.disable",
      "signature": "torch._dynamo.disable(fn=None, recursive=True, *, reason=None)",
      "doc": "\n    Decorator to disable TorchDynamo\n\n    If recursive=True, Dynamo is completely skipped on the decorated function\n    frame as well as the recursively invoked functions.\n\n    If recursive=False, Dynamo skips frames associated with the function code,\n    but still process recursively invoked frames.\n\n    If reason is provided, it will be printed when Dynamo attempts to trace the disabled function.\n    ",
      "arguments": [
        "fn",
        "recursive",
        "reason"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Decorator to disable TorchDynamo\n\n    If recursive=True, Dynamo is completely skipped on the decorated function\n    frame as well as the recursively invoked functions.\n\n    If recursive=False, Dynamo skips frames associated with the function code,\n    but still process recursively invoked frames.\n\n    If reason is provided, it will be printed when Dynamo attempts to trace the disabled function.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.disallow_in_graph",
      "signature": "torch._dynamo.disallow_in_graph(fn)",
      "doc": "\n    Customize which functions TorchDynamo will exclude in the generated\n    graph and force a graph break on.\n    ::\n\n        torch._dynamo.disallow_in_graph(torch.sub)\n\n\n        @torch._dynamo.optimize(...)\n        def fn(a):\n            x = torch.add(x, 1)\n            x = torch.sub(x, 1)\n            x = torch.add(x, 1)\n            return x\n\n\n        fn(...)\n\n    Will break the graph on `torch.sub`, and give two graphs each with a\n    single `torch.add()` op.\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Customize which functions TorchDynamo will exclude in the generated\n    graph and force a graph break on.\n    ::\n\n        torch._dynamo.disallow_in_graph(torch.sub)\n\n\n        @torch._dynamo.optimize(...)\n        def fn(a):\n            x = torch.add(x, 1)\n            x = torch.sub(x, 1)\n            x = torch.add(x, 1)\n            return x\n\n\n        fn(...)\n\n    Will break the graph on `torch.sub`, and give two graphs each with a\n    single `torch.add()` op.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.explain",
      "signature": "torch._dynamo.explain(f, *extra_args, **extra_kwargs)",
      "doc": "",
      "arguments": [
        "f",
        "extra_args",
        "extra_kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.export",
      "signature": "torch._dynamo.export(f: 'Callable[..., Any]', *extra_args, aten_graph: 'bool' = False, pre_dispatch: 'bool' = False, decomposition_table: 'Optional[dict[torch._ops.OpOverload, Callable[..., Any]]]' = None, tracing_mode: 'str' = 'symbolic', dynamic_shapes: 'Optional[Union[dict[str, Any], tuple[Any], list[Any]]]' = None, specialize_float: 'bool' = True, assume_static_by_default: 'bool' = False, same_signature: 'bool' = True, disable_constraint_solver: 'bool' = False, prefer_deferred_runtime_asserts_over_guards: 'bool' = False, allow_complex_guards_as_runtime_asserts: 'bool' = False, _log_export_usage: 'bool' = True, **extra_kwargs) -> 'Callable[..., ExportResult]'",
      "doc": "\n    Export an input function f to a format that can be executed outside of PyTorch using the FX graph.\n\n    Args:\n        f (callable): A PyTorch function to be exported.\n\n        aten_graph (bool): If True, exports a graph with ATen operators.\n        If False, exports a graph with Python operators. Default is False.\n\n        pre_dispatch (bool): If True, exports a graph with ATen operators,\n        but before any logic in the PyTorch dispatcher has run.\n        This can be useful if you want to apply further transformations on a graph before running it\n        through autograd, autocast, or any other functionalities that are integrated into the dispatcher.\n        This flag is only valid if aten_graph=True is set.\n        Default is False.\n\n        decomposition_table (dict): A dictionary that maps operators to their decomposition functions.\n        Required if aten_graph or tracing_mode is specified. Default is None.\n\n        tracing_mode (str): If \"symbolic\", turn on dynamic shapes support. Default is \"symbolic\".\n\n        dynamic_shapes:\n         An optional argument where the type should either be:\n         1) a dict from argument names of ``f`` to their dynamic shape specifications,\n         2) a tuple that specifies dynamic shape specifications for each input in original order.\n         If you are specifying dynamism on keyword args, you will need to pass them in the order that\n         is defined in the original function signature.\n\n         The dynamic shape of a tensor argument can be specified as either\n         (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n         not required to include static dimension indices in this dict, but when they are,\n         they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n         where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n         are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n         recursively specified by using mappings or sequences of contained specifications.\n\n        same_signature (bool): If True, rewrite the returned graph's signature to be the same as f.\n\n        disable_constraint_solver (bool): Whether the dim constraint solver must be disabled.\n\n    Returns:\n        A function that given args and kwargs, returns a tuple of (graph, guards)\n        Graph: An FX graph representing the execution of the input PyTorch function with the provided arguments and options.\n        Guards: The guards we accumulated during tracing f above\n\n    Raises:\n        AssertionError: If decomposition_table is specified without setting aten_graph=True,\n        or if graph breaks during tracing in export.\n\n        AssertionError: If Dynamo input and output is not consistent with traced input/output.\n\n    Note - this headerdoc was authored by ChatGPT, with slight modifications by the author.\n    ",
      "arguments": [
        "f",
        "extra_args",
        "aten_graph",
        "pre_dispatch",
        "decomposition_table",
        "tracing_mode",
        "dynamic_shapes",
        "specialize_float",
        "assume_static_by_default",
        "same_signature",
        "disable_constraint_solver",
        "prefer_deferred_runtime_asserts_over_guards",
        "allow_complex_guards_as_runtime_asserts",
        "_log_export_usage",
        "extra_kwargs"
      ],
      "return_type": "Callable[..., ExportResult]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Export an input function f to a format that can be executed outside of PyTorch using the FX graph.\n\n    Args:\n        f (callable): A PyTorch function to be exported.\n\n        aten_graph (bool): If True, exports a graph with ATen operators.\n        If False, exports a graph with Python operators. Default is False.\n\n        pre_dispatch (bool): If True, exports a graph with ATen operators,\n        but before any logic in the PyTorch dispatcher has run.\n        This can be useful if you want to apply further transformations on a graph before running it\n        through autograd, autocast, or any other functionalities that are integrated into the dispatcher.\n        This flag is only valid if aten_graph=True is set.\n        Default is False.\n\n        decomposition_table (dict): A dictionary that maps operators to their decomposition functions.\n        Required if aten_graph or tracing_mode is specified. Default is None.\n\n        tracing_mode (str): If \"symbolic\", turn on dynamic shapes support. Default is \"symbolic\".\n\n        dynamic_shapes:\n         An optional argument where the type should either be:\n         1) a dict from argument names of ``f`` to their dynamic shape specifications,\n         2) a tuple that specifies dynamic shape specifications for each input in original order.\n         If you are specifying dynamism on keyword args, you will need to pass them in the order that\n         is defined in the original function signature.\n\n         The dynamic shape of a tensor argument can be specified as either\n         (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n         not required to include static dimension indices in this dict, but when they are,\n         they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n         where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n         are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n         recursively specified by using mappings or sequences of contained specifications.\n\n        same_signature (bool): If True, rewrite the returned graph's signature to be the same as f.\n\n        disable_constraint_solver (bool): Whether the dim constraint solver must be disabled.\n\n    Returns:\n        A function that given args and kwargs, returns a tuple of (graph, guards)\n        Graph: An FX graph representing the execution of the input PyTorch function with the provided arguments and options.\n        Guards: The guards we accumulated during tracing f above\n\n    Raises:\n        AssertionError: If decomposition_table is specified without setting aten_graph=True,\n        or if graph breaks during tracing in export.\n\n        AssertionError: If Dynamo input and output is not consistent with traced input/output.\n\n    Note - this headerdoc was authored by ChatGPT, with slight modifications by the author.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.forbid_in_graph",
      "signature": "torch._dynamo.forbid_in_graph(fn)",
      "doc": "\n    Customize which functions TorchDynamo will assert are not present while tracing.\n\n    If you want a graph break on this function instead, use disallow_in_graph.\n    TODO(voz): We now have allow_in_graph, disallow_in_graph, forbid_in_graph - some more robust\n    documentation would not be amiss.\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Customize which functions TorchDynamo will assert are not present while tracing.\n\n    If you want a graph break on this function instead, use disallow_in_graph.\n    TODO(voz): We now have allow_in_graph, disallow_in_graph, forbid_in_graph - some more robust\n    documentation would not be amiss.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.graph_break",
      "signature": "torch._dynamo.graph_break(msg='')",
      "doc": "Force a graph break",
      "arguments": [
        "msg"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Force a graph break",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.is_compiling",
      "signature": "torch._dynamo.is_compiling() -> bool",
      "doc": "\n        Indicates whether we are tracing/compiling with torch.compile() or torch.export().\n        ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n        Indicates whether we are tracing/compiling with torch.compile() or torch.export().\n        ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.is_dynamo_supported",
      "signature": "torch._dynamo.is_dynamo_supported()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.is_inductor_supported",
      "signature": "torch._dynamo.is_inductor_supported()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.list_backends",
      "signature": "torch._dynamo.list_backends(exclude_tags=('debug', 'experimental')) -> list[str]",
      "doc": "\n    Return valid strings that can be passed to:\n\n        torch.compile(..., backend=\"name\")\n    ",
      "arguments": [
        "exclude_tags"
      ],
      "return_type": "list[str]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return valid strings that can be passed to:\n\n        torch.compile(..., backend=\"name\")\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.lookup_backend",
      "signature": "torch._dynamo.lookup_backend(compiler_fn)",
      "doc": "Expand backend strings to functions",
      "arguments": [
        "compiler_fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Expand backend strings to functions",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.mark_dynamic",
      "signature": "torch._dynamo.mark_dynamic(t, index, *, min=None, max=None)",
      "doc": "\n    Mark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\n\n    [Note - on the state of mark_dynamic]\n\n    The behavior of having a dynamic dimension on a tensor is governed by a few factors:\n\n    1) torch._dynamo.config dynamic_shapes True or False.\n        a) dynamic_shapes=True - dynamic_shapes must be True for mark_dynamic to work.\n        a) dynamic_shapes=False - This config will raise an exception when used in conjunction with\n        mark_dynamic. We will eventually support this.\n\n    2) If the dimension is fully constrained - as in, it does not allow more than a single value\n    in both eager (torch.compile, torch._dynamo.optimize) mode and export mode (torch._dynamo.export),\n    we will raise an error\n\n    3) If the dimension is partially constrained - allowing at least 2 values but not the full unbounded\n    range of shapes, in eager we will pass it through, but export will raise an error.\n\n    4) Attempts to trace this function will explicitly raise. As such, all calls to mark_dynamic must be made\n    before torch.compile.\n\n    ",
      "arguments": [
        "t",
        "index",
        "min",
        "max"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Mark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\n\n    [Note - on the state of mark_dynamic]\n\n    The behavior of having a dynamic dimension on a tensor is governed by a few factors:\n\n    1) torch._dynamo.config dynamic_shapes True or False.\n        a) dynamic_shapes=True - dynamic_shapes must be True for mark_dynamic to work.\n        a) dynamic_shapes=False - This config will raise an exception when used in conjunction with\n        mark_dynamic. We will eventually support this.\n\n    2) If the dimension is fully constrained - as in, it does not allow more than a single value\n    in both eager (torch.compile, torch._dynamo.optimize) mode and export mode (torch._dynamo.export),\n    we will raise an error\n\n    3) If the dimension is partially constrained - allowing at least 2 values but not the full unbounded\n    range of shapes, in eager we will pass it through, but export will raise an error.\n\n    4) Attempts to trace this function will explicitly raise. As such, all calls to mark_dynamic must be made\n    before torch.compile.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.mark_static",
      "signature": "torch._dynamo.mark_static(t, index=None)",
      "doc": "\n    Mark a tensor as having a static dim or mark a nn module class as static.\n\n    For tensors\n    ===========\n    This will prevent us from attempting to compile it dynamically\n    when dynamic=True; this can improve trace-time performance.\n\n    This has lower precedence than mark_dynamic.\n\n    Unlike mark_dynamic, this can be done inside a graph, in which case it\n    induces specialization on the tensor.\n\n    For nn.Module classes\n    =====================\n    For static nn.Module classes, TorchDynamo assumes that the module instance\n    attributes will not be modified after compilation. This will ensure that\n    TorchDynamo keeps integer attributes CONSTANT and not symints.\n\n    From TorchDynamo implementation side, the instances of static-marked\n    nn.Module class will be converted to UnspecializedBuiltinNNModuleVariable,\n    which have the same properties.\n\n    Note that we still have to guard on the attributes, because different\n    instances of the nn.Module can have different values of the attributes. The\n    key point here is that the attributes are static.\n    ",
      "arguments": [
        "t",
        "index"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Mark a tensor as having a static dim or mark a nn module class as static.\n\n    For tensors\n    ===========\n    This will prevent us from attempting to compile it dynamically\n    when dynamic=True; this can improve trace-time performance.\n\n    This has lower precedence than mark_dynamic.\n\n    Unlike mark_dynamic, this can be done inside a graph, in which case it\n    induces specialization on the tensor.\n\n    For nn.Module classes\n    =====================\n    For static nn.Module classes, TorchDynamo assumes that the module instance\n    attributes will not be modified after compilation. This will ensure that\n    TorchDynamo keeps integer attributes CONSTANT and not symints.\n\n    From TorchDynamo implementation side, the instances of static-marked\n    nn.Module class will be converted to UnspecializedBuiltinNNModuleVariable,\n    which have the same properties.\n\n    Note that we still have to guard on the attributes, because different\n    instances of the nn.Module can have different values of the attributes. The\n    key point here is that the attributes are static.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.mark_static_address",
      "signature": "torch._dynamo.mark_static_address(t, guard=True)",
      "doc": "\n    Marks an input tensor whose data_ptr will not change across multiple calls\n    to a dynamo-compiled function. This indicates to cudagraphs that an extra allocation\n    is not needed for this input. The data_ptr will be guarded if guard=True. Note:\n    Tensors marked in this way will be kept alive until `torch._dynamo.reset()` is called.\n    ",
      "arguments": [
        "t",
        "guard"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Marks an input tensor whose data_ptr will not change across multiple calls\n    to a dynamo-compiled function. This indicates to cudagraphs that an extra allocation\n    is not needed for this input. The data_ptr will be guarded if guard=True. Note:\n    Tensors marked in this way will be kept alive until `torch._dynamo.reset()` is called.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.maybe_mark_dynamic",
      "signature": "torch._dynamo.maybe_mark_dynamic(t, index)",
      "doc": "\n    Mark a tensor as having a dynamic dim, but don't enforce it (i.e., if this\n    dimension ends up getting specialized, don't error).\n    ",
      "arguments": [
        "t",
        "index"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Mark a tensor as having a dynamic dim, but don't enforce it (i.e., if this\n    dimension ends up getting specialized, don't error).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.nonstrict_trace",
      "signature": "torch._dynamo.nonstrict_trace(traceable_fn)",
      "doc": "",
      "arguments": [
        "traceable_fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.on_compile_end",
      "signature": "torch._dynamo.on_compile_end(callback: Callable[[], NoneType]) -> Callable[[], NoneType]",
      "doc": "\n    Decorator to register a callback function for the end of the compilation.\n    ",
      "arguments": [
        "callback"
      ],
      "return_type": "typing.Callable[[], NoneType]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Decorator to register a callback function for the end of the compilation.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.on_compile_start",
      "signature": "torch._dynamo.on_compile_start(callback: Callable[[], NoneType]) -> Callable[[], NoneType]",
      "doc": "\n    Decorator to register a callback function for the start of the compilation.\n    ",
      "arguments": [
        "callback"
      ],
      "return_type": "typing.Callable[[], NoneType]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Decorator to register a callback function for the start of the compilation.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.optimize",
      "signature": "torch._dynamo.optimize(*args, **kwargs)",
      "doc": "",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.optimize_assert",
      "signature": "torch._dynamo.optimize_assert(backend, *, hooks=Hooks(guard_export_fn=None, guard_fail_fn=None, guard_filter_fn=None), export=False, export_constraints=None, dynamic=None, rebuild_ctx=None)",
      "doc": "\n    The same as `torch._dynamo.optimize(backend, nopython=True)`\n    ",
      "arguments": [
        "backend",
        "hooks",
        "export",
        "export_constraints",
        "dynamic",
        "rebuild_ctx"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    The same as `torch._dynamo.optimize(backend, nopython=True)`\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.register_backend",
      "signature": "torch._dynamo.register_backend(compiler_fn: Optional[Callable[[torch.fx.graph_module.GraphModule, list[torch.Tensor]], torch._dynamo.backends.registry.CompiledFn]] = None, name: Optional[str] = None, tags: collections.abc.Sequence[str] = ())",
      "doc": "\n    Decorator to add a given compiler to the registry to allow calling\n    `torch.compile` with string shorthand.  Note: for projects not\n    imported by default, it might be easier to pass a function directly\n    as a backend and not use a string.\n\n    Args:\n        compiler_fn: Callable taking a FX graph and fake tensor inputs\n        name: Optional name, defaults to `compiler_fn.__name__`\n        tags: Optional set of string tags to categorize backend with\n    ",
      "arguments": [
        "compiler_fn",
        "name",
        "tags"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Decorator to add a given compiler to the registry to allow calling\n    `torch.compile` with string shorthand.  Note: for projects not\n    imported by default, it might be easier to pass a function directly\n    as a backend and not use a string.\n\n    Args:\n        compiler_fn: Callable taking a FX graph and fake tensor inputs\n        name: Optional name, defaults to `compiler_fn.__name__`\n        tags: Optional set of string tags to categorize backend with\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.replay",
      "signature": "torch._dynamo.replay(filename: 'str') -> 'None'",
      "doc": "",
      "arguments": [
        "filename"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.reset",
      "signature": "torch._dynamo.reset() -> None",
      "doc": "\n    Clear all compile caches and restore initial state.  This function is intended\n    to reset Dynamo's state *as if* you had started a fresh process invocation, which\n    makes it good for testing scenarios where you want to behave as if you started\n    a new process.  It does NOT affect any file system caches.\n\n    NB: this does NOT reset logging state.  Don't use this to test logging\n    initialization/reinitialization.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Clear all compile caches and restore initial state.  This function is intended\n    to reset Dynamo's state *as if* you had started a fresh process invocation, which\n    makes it good for testing scenarios where you want to behave as if you started\n    a new process.  It does NOT affect any file system caches.\n\n    NB: this does NOT reset logging state.  Don't use this to test logging\n    initialization/reinitialization.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.reset_code_caches",
      "signature": "torch._dynamo.reset_code_caches() -> None",
      "doc": "\n    Clears in-memory code cache, which is what stores compiled products.  This\n    resets less state than :func:`reset` and is mostly only used for testing\n    purposes.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Clears in-memory code cache, which is what stores compiled products.  This\n    resets less state than :func:`reset` and is mostly only used for testing\n    purposes.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.reset_code_state",
      "signature": "torch._dynamo.reset_code_state() -> 'None'",
      "doc": "",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.reset_frame_count",
      "signature": "torch._dynamo.reset_frame_count() -> 'None'",
      "doc": "",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.run",
      "signature": "torch._dynamo.run(fn=None)",
      "doc": "Don't do any dynamic compiles, just use prior optimizations",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Don't do any dynamic compiles, just use prior optimizations",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._dynamo.substitute_in_graph",
      "signature": "torch._dynamo.substitute_in_graph(original_fn: Callable[~_P, ~_R], *, can_constant_fold_through: bool = False, skip_signature_check: bool = False, is_embedded_type: bool = False) -> Callable[[Callable[~_P, ~_R]], Callable[~_P, ~_R]]",
      "doc": "\n    Register a polyfill handler for a function, usually a C function from the C extension, to be\n    used in place of the original function when inlining the original function in the graph.\n\n    .. note::\n\n        The polyfill handler is only used when inlining the original function. It is not used when\n        the original function is called directly. In the eager mode, the decorated function calls\n        the performant C function rather than the polyfill handler.\n\n    The polyfill handler is a function that will be called in place of the original function when\n    inlining the original function. The polyfill handler should have the same signature and the same\n    behavior as the original function.\n\n    Args:\n        original_fn (callable): The original function, usually a C function, to register a polyfill\n            handler for.\n        can_constant_fold_through (bool, optional): Whether the polyfill handler can be constant\n            folded through. That is, if the polyfill handler is a pure function and its arguments\n            are constant, the result of the polyfill handler can be constant folded during the\n            compilation. Defaults to ``False``.\n        skip_signature_check (bool, optional): Whether to skip the signature check between the\n            original function and the polyfill handler. Defaults to ``False``.\n\n    Returns:\n        A decorator that registers the polyfill handler for the original function.\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"conflict with the tests: duplicate polyfill handlers\")\n        >>> import operator\n        >>> operator.indexOf([1, 2, 3, 4, 5], 3)\n        2\n        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)\n        Traceback (most recent call last):\n        ...\n        torch._dynamo.exc.Unsupported: ...\n\n        >>> @torch.compiler.substitute_in_graph(operator.indexOf)\n        ... def indexOf(a, b, /):\n        ...     for i, item in enumerate(a):\n        ...         if item is b or item == b:\n        ...             return i\n        ...     raise ValueError(\"sequence.index(x): x not in sequence\")\n        >>>\n        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)\n        2\n    ",
      "arguments": [
        "original_fn",
        "can_constant_fold_through",
        "skip_signature_check",
        "is_embedded_type"
      ],
      "return_type": "typing.Callable[[typing.Callable[~_P, ~_R]], typing.Callable[~_P, ~_R]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Register a polyfill handler for a function, usually a C function from the C extension, to be\n    used in place of the original function when inlining the original function in the graph.\n\n    .. note::\n\n        The polyfill handler is only used when inlining the original function. It is not used when\n        the original function is called directly. In the eager mode, the decorated function calls\n        the performant C function rather than the polyfill handler.\n\n    The polyfill handler is a function that will be called in place of the original function when\n    inlining the original function. The polyfill handler should have the same signature and the same\n    behavior as the original function.\n\n    Args:\n        original_fn (callable): The original function, usually a C function, to register a polyfill\n            handler for.\n        can_constant_fold_through (bool, optional): Whether the polyfill handler can be constant\n            folded through. That is, if the polyfill handler is a pure function and its arguments\n            are constant, the result of the polyfill handler can be constant folded during the\n            compilation. Defaults to ``False``.\n        skip_signature_check (bool, optional): Whether to skip the signature check between the\n            original function and the polyfill handler. Defaults to ``False``.\n\n    Returns:\n        A decorator that registers the polyfill handler for the original function.\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"conflict with the tests: duplicate polyfill handlers\")\n        >>> import operator\n        >>> operator.indexOf([1, 2, 3, 4, 5], 3)\n        2\n        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)\n        Traceback (most recent call last):\n        ...\n        torch._dynamo.exc.Unsupported: ...\n\n        >>> @torch.compiler.substitute_in_graph(operator.indexOf)\n        ... def indexOf(a, b, /):\n        ...     for i, item in enumerate(a):\n        ...         if item is b or item == b:\n        ...             return i\n        ...     raise ValueError(\"sequence.index(x): x not in sequence\")\n        >>>\n        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)\n        2\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.aot_compile",
      "signature": "torch._export.aot_compile(f: Callable, args: tuple[typing.Any], kwargs: Optional[dict[str, Any]] = None, *, dynamic_shapes: Optional[dict[str, Any]] = None, options: Optional[dict[str, Any]] = None, remove_runtime_assertions: bool = False, disable_constraint_solver: bool = False, same_signature: bool = True) -> Union[list[str], str]",
      "doc": "\n    Note: this function is not stable yet\n\n    Traces either an nn.Module's forward function or just a callable with PyTorch\n    operations inside, generates executable cpp code from the program, and returns\n    the path to the generated shared library\n\n    Args:\n        f: the `nn.Module` or callable to trace.\n\n        args: example positional inputs.\n\n        kwargs: optional example keyword inputs.\n\n        dynamic_shapes: Should either be:\n            1) a dict from argument names of ``f`` to their dynamic shape specifications,\n            2) a tuple that specifies dynamic shape specifications for each input in original order.\n            If you are specifying dynamism on keyword args, you will need to pass them in the order that\n            is defined in the original function signature.\n\n            The dynamic shape of a tensor argument can be specified as either\n            (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n            not required to include static dimension indices in this dict, but when they are,\n            they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n            where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n            are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n            recursively specified by using mappings or sequences of contained specifications.\n\n        options: A dictionary of options to control inductor\n\n        disable_constraint_solver: Whether the dim constraint solver must be disabled.\n\n    Returns:\n        Path to the generated shared library\n    ",
      "arguments": [
        "f",
        "args",
        "kwargs",
        "dynamic_shapes",
        "options",
        "remove_runtime_assertions",
        "disable_constraint_solver",
        "same_signature"
      ],
      "return_type": "typing.Union[list[str], str]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Note: this function is not stable yet\n\n    Traces either an nn.Module's forward function or just a callable with PyTorch\n    operations inside, generates executable cpp code from the program, and returns\n    the path to the generated shared library\n\n    Args:\n        f: the `nn.Module` or callable to trace.\n\n        args: example positional inputs.\n\n        kwargs: optional example keyword inputs.\n\n        dynamic_shapes: Should either be:\n            1) a dict from argument names of ``f`` to their dynamic shape specifications,\n            2) a tuple that specifies dynamic shape specifications for each input in original order.\n            If you are specifying dynamism on keyword args, you will need to pass them in the order that\n            is defined in the original function signature.\n\n            The dynamic shape of a tensor argument can be specified as either\n            (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n            not required to include static dimension indices in this dict, but when they are,\n            they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n            where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n            are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n            recursively specified by using mappings or sequences of contained specifications.\n\n        options: A dictionary of options to control inductor\n\n        disable_constraint_solver: Whether the dim constraint solver must be disabled.\n\n    Returns:\n        Path to the generated shared library\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.aot_load",
      "signature": "torch._export.aot_load(so_path: str, device: str) -> Callable",
      "doc": "\n    Loads a shared library generated by aot_compile and returns a callable\n\n    Args:\n        so_path: Path to the shared library\n\n    Returns:\n        A callable\n    ",
      "arguments": [
        "so_path",
        "device"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Loads a shared library generated by aot_compile and returns a callable\n\n    Args:\n        so_path: Path to the shared library\n\n    Returns:\n        A callable\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.compatibility",
      "signature": "torch._export.compatibility(is_backward_compatible: bool) -> Callable[[~_T], ~_T]",
      "doc": "",
      "arguments": [
        "is_backward_compatible"
      ],
      "return_type": "typing.Callable[[~_T], ~_T]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.compile_context",
      "signature": "torch._export.compile_context(context: 'Optional[CompileContext]')",
      "doc": "",
      "arguments": [
        "context"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.contextmanager",
      "signature": "torch._export.contextmanager(func)",
      "doc": "@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    ",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.log_export_usage",
      "signature": "torch._export.log_export_usage(**kwargs)",
      "doc": "",
      "arguments": [
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.lru_cache",
      "signature": "torch._export.lru_cache(maxsize=128, typed=False)",
      "doc": "Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    ",
      "arguments": [
        "maxsize",
        "typed"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.make_fx",
      "signature": "torch._export.make_fx(f: 'Callable', decomposition_table: 'Optional[Mapping[OpOverload, Callable]]' = None, tracing_mode: 'str' = 'real', _allow_non_fake_inputs: 'bool' = False, *, pre_dispatch: 'bool' = False, record_module_stack: 'bool' = False, _allow_fake_constant: 'bool' = False, _error_on_data_dependent_ops: 'bool' = True) -> 'Callable[..., GraphModule]'",
      "doc": "\n    Given a function f, return a new function which when executed with valid\n    arguments to f, returns an FX GraphModule representing the set of operations that\n    were executed during the course of execution.\n    ",
      "arguments": [
        "f",
        "decomposition_table",
        "tracing_mode",
        "_allow_non_fake_inputs",
        "pre_dispatch",
        "record_module_stack",
        "_allow_fake_constant",
        "_error_on_data_dependent_ops"
      ],
      "return_type": "Callable[..., GraphModule]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Given a function f, return a new function which when executed with valid\n    arguments to f, returns an FX GraphModule representing the set of operations that\n    were executed during the course of execution.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.patch",
      "signature": "torch._export.patch(target, new=sentinel.DEFAULT, spec=None, create=False, spec_set=None, autospec=None, new_callable=None, *, unsafe=False, **kwargs)",
      "doc": "\n    `patch` acts as a function decorator, class decorator or a context\n    manager. Inside the body of the function or with statement, the `target`\n    is patched with a `new` object. When the function/with statement exits\n    the patch is undone.\n\n    If `new` is omitted, then the target is replaced with an\n    `AsyncMock if the patched object is an async function or a\n    `MagicMock` otherwise. If `patch` is used as a decorator and `new` is\n    omitted, the created mock is passed in as an extra argument to the\n    decorated function. If `patch` is used as a context manager the created\n    mock is returned by the context manager.\n\n    `target` should be a string in the form `'package.module.ClassName'`. The\n    `target` is imported and the specified object replaced with the `new`\n    object, so the `target` must be importable from the environment you are\n    calling `patch` from. The target is imported when the decorated function\n    is executed, not at decoration time.\n\n    The `spec` and `spec_set` keyword arguments are passed to the `MagicMock`\n    if patch is creating one for you.\n\n    In addition you can pass `spec=True` or `spec_set=True`, which causes\n    patch to pass in the object being mocked as the spec/spec_set object.\n\n    `new_callable` allows you to specify a different class, or callable object,\n    that will be called to create the `new` object. By default `AsyncMock` is\n    used for async functions and `MagicMock` for the rest.\n\n    A more powerful form of `spec` is `autospec`. If you set `autospec=True`\n    then the mock will be created with a spec from the object being replaced.\n    All attributes of the mock will also have the spec of the corresponding\n    attribute of the object being replaced. Methods and functions being\n    mocked will have their arguments checked and will raise a `TypeError` if\n    they are called with the wrong signature. For mocks replacing a class,\n    their return value (the 'instance') will have the same spec as the class.\n\n    Instead of `autospec=True` you can pass `autospec=some_object` to use an\n    arbitrary object as the spec instead of the one being replaced.\n\n    By default `patch` will fail to replace attributes that don't exist. If\n    you pass in `create=True`, and the attribute doesn't exist, patch will\n    create the attribute for you when the patched function is called, and\n    delete it again afterwards. This is useful for writing tests against\n    attributes that your production code creates at runtime. It is off by\n    default because it can be dangerous. With it switched on you can write\n    passing tests against APIs that don't actually exist!\n\n    Patch can be used as a `TestCase` class decorator. It works by\n    decorating each test method in the class. This reduces the boilerplate\n    code when your test methods share a common patchings set. `patch` finds\n    tests by looking for method names that start with `patch.TEST_PREFIX`.\n    By default this is `test`, which matches the way `unittest` finds tests.\n    You can specify an alternative prefix by setting `patch.TEST_PREFIX`.\n\n    Patch can be used as a context manager, with the with statement. Here the\n    patching applies to the indented block after the with statement. If you\n    use \"as\" then the patched object will be bound to the name after the\n    \"as\"; very useful if `patch` is creating a mock object for you.\n\n    Patch will raise a `RuntimeError` if passed some common misspellings of\n    the arguments autospec and spec_set. Pass the argument `unsafe` with the\n    value True to disable that check.\n\n    `patch` takes arbitrary keyword arguments. These will be passed to\n    `AsyncMock` if the patched object is asynchronous, to `MagicMock`\n    otherwise or to `new_callable` if specified.\n\n    `patch.dict(...)`, `patch.multiple(...)` and `patch.object(...)` are\n    available for alternate use-cases.\n    ",
      "arguments": [
        "target",
        "new",
        "spec",
        "create",
        "spec_set",
        "autospec",
        "new_callable",
        "unsafe",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    `patch` acts as a function decorator, class decorator or a context\n    manager. Inside the body of the function or with statement, the `target`\n    is patched with a `new` object. When the function/with statement exits\n    the patch is undone.\n\n    If `new` is omitted, then the target is replaced with an\n    `AsyncMock if the patched object is an async function or a\n    `MagicMock` otherwise. If `patch` is used as a decorator and `new` is\n    omitted, the created mock is passed in as an extra argument to the\n    decorated function. If `patch` is used as a context manager the created\n    mock is returned by the context manager.\n\n    `target` should be a string in the form `'package.module.ClassName'`. The\n    `target` is imported and the specified object replaced with the `new`\n    object, so the `target` must be importable from the environment you are\n    calling `patch` from. The target is imported when the decorated function\n    is executed, not at decoration time.\n\n    The `spec` and `spec_set` keyword arguments are passed to the `MagicMock`\n    if patch is creating one for you.\n\n    In addition you can pass `spec=True` or `spec_set=True`, which causes\n    patch to pass in the object being mocked as the spec/spec_set object.\n\n    `new_callable` allows you to specify a different class, or callable object,\n    that will be called to create the `new` object. By default `AsyncMock` is\n    used for async functions and `MagicMock` for the rest.\n\n    A more powerful form of `spec` is `autospec`. If you set `autospec=True`\n    then the mock will be created with a spec from the object being replaced.\n    All attributes of the mock will also have the spec of the corresponding\n    attribute of the object being replaced. Methods and functions being\n    mocked will have their arguments checked and will raise a `TypeError` if\n    they are called with the wrong signature. For mocks replacing a class,\n    their return value (the 'instance') will have the same spec as the class.\n\n    Instead of `autospec=True` you can pass `autospec=some_object` to use an\n    arbitrary object as the spec instead of the one being replaced.\n\n    By default `patch` will fail to replace attributes that don't exist. If\n    you pass in `create=True`, and the attribute doesn't exist, patch will\n    create the attribute for you when the patched function is called, and\n    delete it again afterwards. This is useful for writing tests against\n    attributes that your production code creates at runtime. It is off by\n    default because it can be dangerous. With it switched on you can write\n    passing tests against APIs that don't actually exist!\n\n    Patch can be used as a `TestCase` class decorator. It works by\n    decorating each test method in the class. This reduces the boilerplate\n    code when your test methods share a common patchings set. `patch` finds\n    tests by looking for method names that start with `patch.TEST_PREFIX`.\n    By default this is `test`, which matches the way `unittest` finds tests.\n    You can specify an alternative prefix by setting `patch.TEST_PREFIX`.\n\n    Patch can be used as a context manager, with the with statement. Here the\n    patching applies to the indented block after the with statement. If you\n    use \"as\" then the patched object will be bound to the name after the\n    \"as\"; very useful if `patch` is creating a mock object for you.\n\n    Patch will raise a `RuntimeError` if passed some common misspellings of\n    the arguments autospec and spec_set. Pass the argument `unsafe` with the\n    value True to disable that check.\n\n    `patch` takes arbitrary keyword arguments. These will be passed to\n    `AsyncMock` if the patched object is asynchronous, to `MagicMock`\n    otherwise or to `new_callable` if specified.\n\n    `patch.dict(...)`, `patch.multiple(...)` and `patch.object(...)` are\n    available for alternate use-cases.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._export.reorder_kwargs",
      "signature": "torch._export.reorder_kwargs(user_kwargs: dict[str, typing.Any], spec: torch.utils._pytree.TreeSpec) -> dict[str, typing.Any]",
      "doc": "Reorder user-provided kwargs to match the order in `spec`. `spec` is\n    expected to be the in_spec of an exported program, i.e. the spec that\n    results from flattening `(args, kwargs)`.\n\n    We need this to provide consistent input ordering, such so that users can\n    pass in foo(a=a, b=b) OR foo(b=b, a=a) and receive the same result.\n    ",
      "arguments": [
        "user_kwargs",
        "spec"
      ],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reorder user-provided kwargs to match the order in `spec`. `spec` is\n    expected to be the in_spec of an exported program, i.e. the spec that\n    results from flattening `(args, kwargs)`.\n\n    We need this to provide consistent input ordering, such so that users can\n    pass in foo(a=a, b=b) OR foo(b=b, a=a) and receive the same result.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._higher_order_ops.associative_scan",
      "signature": "torch._higher_order_ops.associative_scan(combine_fn: Callable[[Any, Any], Any], xs: Any, dim: int, reverse: bool = False, combine_mode: str = 'pointwise') -> torch.Tensor",
      "doc": "\n    Performs an inclusive scan with an associative combine function.\n\n    .. warning::\n        `torch.associative_scan` is a prototype feature in PyTorch. It currently\n        does not support autograd and you may run into miscompiles.\n        Read more about feature classification at:\n        https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    This operator requires runtime code generation and so requires support for\n    ``torch.compile``. Further, only CUDA device codegen is supported at the moment.\n\n    Args:\n        combine_fn (Callable): A binary callable with type ``(Tensor, Tensor) -> Tensor``,\n            or if input is a pytree ``(pytree, pytree) -> pytree``.\n            This function must be pure, i.e., no lifted arguments are supported at the moment,\n            satisfy the associative property and have no side-effects.\n        xs (torch.Tensor): The input tensor, or nested pytree of tensors.\n            All inputs are expected to have the same shape.\n        dim (int): the dimension to scan over\n        reverse (bool): A boolean stating if the scan should be reversed with respect to ``dim``, default ``False``.\n        combine_mode (str): A string indicating whether the ``combine_fn`` is ``pointwise`` or ``generic``, default ``pointwise``.\n            If ``combine_mode=pointwise``, ``combine_fn`` must be pure, may only contain pointwise operations\n            and ``xs`` must be CUDA tensors.\n            In all other cases ``combine_mode=generic`` should be used.\n            Note: ``combine_mode=pointwise`` is more efficient than ``combine_mode=generic``.\n\n\n    Example::\n\n        def add(x: torch.Tensor, y: torch.Tensor):\n            return x + y\n\n        cumsum = associative_scan(add, x, dim)\n\n    ",
      "arguments": [
        "combine_fn",
        "xs",
        "dim",
        "reverse",
        "combine_mode"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Performs an inclusive scan with an associative combine function.\n\n    .. warning::\n        `torch.associative_scan` is a prototype feature in PyTorch. It currently\n        does not support autograd and you may run into miscompiles.\n        Read more about feature classification at:\n        https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    This operator requires runtime code generation and so requires support for\n    ``torch.compile``. Further, only CUDA device codegen is supported at the moment.\n\n    Args:\n        combine_fn (Callable): A binary callable with type ``(Tensor, Tensor) -> Tensor``,\n            or if input is a pytree ``(pytree, pytree) -> pytree``.\n            This function must be pure, i.e., no lifted arguments are supported at the moment,\n            satisfy the associative property and have no side-effects.\n        xs (torch.Tensor): The input tensor, or nested pytree of tensors.\n            All inputs are expected to have the same shape.\n        dim (int): the dimension to scan over\n        reverse (bool): A boolean stating if the scan should be reversed with respect to ``dim``, default ``False``.\n        combine_mode (str): A string indicating whether the ``combine_fn`` is ``pointwise`` or ``generic``, default ``pointwise``.\n            If ``combine_mode=pointwise``, ``combine_fn`` must be pure, may only contain pointwise operations\n            and ``xs`` must be CUDA tensors.\n            In all other cases ``combine_mode=generic`` should be used.\n            Note: ``combine_mode=pointwise`` is more efficient than ``combine_mode=generic``.\n\n\n    Example::\n\n        def add(x: torch.Tensor, y: torch.Tensor):\n            return x + y\n\n        cumsum = associative_scan(add, x, dim)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._higher_order_ops.cond",
      "signature": "torch._higher_order_ops.cond(pred: Union[bool, int, float, torch.Tensor], true_fn: Callable, false_fn: Callable, operands: Union[tuple, list] = ()) -> Any",
      "doc": "\n    Conditionally applies `true_fn` or `false_fn`.\n\n    .. warning::\n        `torch.cond` is a prototype feature in PyTorch. It has limited support for input and output types and\n        doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\n        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    `cond` is structured control flow operator. That is, it is like a Python if-statement,\n    but has restrictions on `true_fn`, `false_fn`, and `operands` that enable it to be\n    capturable using torch.compile and torch.export.\n\n    Assuming the constraints on `cond`'s arguments are met, `cond` is equivalent to the following::\n\n        def cond(pred, true_branch, false_branch, operands):\n            if pred:\n                return true_branch(*operands)\n            else:\n                return false_branch(*operands)\n\n    Args:\n        pred (Union[bool, torch.Tensor]): A boolean expression or a tensor with one element,\n          indicating which branch function to apply.\n\n        true_fn (Callable): A callable function (a -> b) that is within the\n          scope that is being traced.\n\n        false_fn (Callable): A callable function (a -> b) that is within the\n          scope that is being traced. The true branch and false branch must\n          have consistent input and outputs, meaning the inputs have to be\n          the same, and the outputs have to be the same type and shape.\n\n        operands (Tuple of possibly nested dict/list/tuple of torch.Tensor): A tuple of inputs to the\n          true/false functions. It can be empty if true_fn/false_fn doesn't require input. Defaults to ().\n\n    Example::\n\n        def true_fn(x: torch.Tensor):\n            return x.cos()\n        def false_fn(x: torch.Tensor):\n            return x.sin()\n        return cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n    Restrictions:\n        - The conditional statement (aka `pred`) must meet one of the following constraints:\n\n          - It's a `torch.Tensor` with only one element, and torch.bool dtype\n\n          - It's a boolean expression, e.g. `x.shape[0] > 10` or `x.dim() > 1 and x.shape[1] > 10`\n\n        - The branch function (aka `true_fn`/`false_fn`) must meet all of the following constraints:\n\n          - The function signature must match with operands.\n\n          - The function must return a tensor with the same metadata, e.g. shape,\n            dtype, etc.\n\n          - The function cannot have in-place mutations on inputs or global variables.\n            (Note: in-place tensor operations such as `add_` for intermediate results\n            are allowed in a branch)\n\n    ",
      "arguments": [
        "pred",
        "true_fn",
        "false_fn",
        "operands"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Conditionally applies `true_fn` or `false_fn`.\n\n    .. warning::\n        `torch.cond` is a prototype feature in PyTorch. It has limited support for input and output types and\n        doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\n        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    `cond` is structured control flow operator. That is, it is like a Python if-statement,\n    but has restrictions on `true_fn`, `false_fn`, and `operands` that enable it to be\n    capturable using torch.compile and torch.export.\n\n    Assuming the constraints on `cond`'s arguments are met, `cond` is equivalent to the following::\n\n        def cond(pred, true_branch, false_branch, operands):\n            if pred:\n                return true_branch(*operands)\n            else:\n                return false_branch(*operands)\n\n    Args:\n        pred (Union[bool, torch.Tensor]): A boolean expression or a tensor with one element,\n          indicating which branch function to apply.\n\n        true_fn (Callable): A callable function (a -> b) that is within the\n          scope that is being traced.\n\n        false_fn (Callable): A callable function (a -> b) that is within the\n          scope that is being traced. The true branch and false branch must\n          have consistent input and outputs, meaning the inputs have to be\n          the same, and the outputs have to be the same type and shape.\n\n        operands (Tuple of possibly nested dict/list/tuple of torch.Tensor): A tuple of inputs to the\n          true/false functions. It can be empty if true_fn/false_fn doesn't require input. Defaults to ().\n\n    Example::\n\n        def true_fn(x: torch.Tensor):\n            return x.cos()\n        def false_fn(x: torch.Tensor):\n            return x.sin()\n        return cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n    Restrictions:\n        - The conditional statement (aka `pred`) must meet one of the following constraints:\n\n          - It's a `torch.Tensor` with only one element, and torch.bool dtype\n\n          - It's a boolean expression, e.g. `x.shape[0] > 10` or `x.dim() > 1 and x.shape[1] > 10`\n\n        - The branch function (aka `true_fn`/`false_fn`) must meet all of the following constraints:\n\n          - The function signature must match with operands.\n\n          - The function must return a tensor with the same metadata, e.g. shape,\n            dtype, etc.\n\n          - The function cannot have in-place mutations on inputs or global variables.\n            (Note: in-place tensor operations such as `add_` for intermediate results\n            are allowed in a branch)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._higher_order_ops.foreach_map",
      "signature": "torch._higher_order_ops.foreach_map(op: Callable, *operands: Any, **kwargs: dict[str, typing.Any])",
      "doc": "",
      "arguments": [
        "op",
        "operands",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._higher_order_ops.scan",
      "signature": "torch._higher_order_ops.scan(combine_fn: Callable[[Any, Any], tuple[Any, Any]], init: Any, xs: Any, *, dim: int = 0, reverse: bool = False) -> tuple[typing.Any, typing.Any]",
      "doc": "\n    Performs an inclusive scan with a combine function.\n\n    .. warning::\n        `torch.scan` is a prototype feature in PyTorch. It currently\n        does not support autograd and you may run into miscompiles.\n        Read more about feature classification at:\n        https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    Args:\n        combine_fn (Callable): A binary callable with type ``(Tensor, Tensor) -> (Tensor, Tensor)``,\n            or if xs is a pytree ``(pytree, pytree) -> (pytree, pytree)``.\n            The first input to ``combine_fn`` is the previous or initial scan carry\n            and the second input element to ``combine_fn`` is a slice of the input along dim.\n            The first output element of ``combine_fn`` is the next scan carry\n            and the second output  of ``combine_fn`` represents a slice of the output.\n            This function must be pure, i.e., no lifted arguments are supported at the moment\n            and may not have any side effects.\n        init (torch.Tensor or pytree with tensor leaves): The inital scan carry, a tensor, or nested pytree of tensors.\n            The ``init`` is expected to have the same pytree structure as the first output element (i.e. carry)\n            of ``combine_fn``.\n        xs (torch.Tensor or pytree with tensor leaves): The input tensor, or nested pytree of tensors.\n\n    Kwargs:\n        dim (int): the dimension to scan over, default 0.\n        reverse (bool): A boolean stating if the scan should be reversed with respect to ``dim``, default ``False``.\n\n    Returns:\n        final_carry (torch.Tensor or pytree with tensor leaves),\n            the final carry of the scan operation with same pytree structure as init.\n        out (torch.Tensor or pytree with tensor leaves),\n            each tensor leaf is a stacked output along first dim, where each slice is the output of a scan iteration.\n\n    Example::\n\n        def add(x: torch.Tensor, y: torch.Tensor):\n            next_carry = y = x + y\n            return next_carry, y\n\n        i0 = torch.zeros(1)\n        xs = torch.arange(5)\n        # returns torch.tensor([10.]), torch.tensor([[0], [1.], [3.], [6.], [10.]])\n        last_carry, cumsum = scan(add, init=i0, xs=xs)\n\n\n    ",
      "arguments": [
        "combine_fn",
        "init",
        "xs",
        "dim",
        "reverse"
      ],
      "return_type": "tuple[typing.Any, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Performs an inclusive scan with a combine function.\n\n    .. warning::\n        `torch.scan` is a prototype feature in PyTorch. It currently\n        does not support autograd and you may run into miscompiles.\n        Read more about feature classification at:\n        https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    Args:\n        combine_fn (Callable): A binary callable with type ``(Tensor, Tensor) -> (Tensor, Tensor)``,\n            or if xs is a pytree ``(pytree, pytree) -> (pytree, pytree)``.\n            The first input to ``combine_fn`` is the previous or initial scan carry\n            and the second input element to ``combine_fn`` is a slice of the input along dim.\n            The first output element of ``combine_fn`` is the next scan carry\n            and the second output  of ``combine_fn`` represents a slice of the output.\n            This function must be pure, i.e., no lifted arguments are supported at the moment\n            and may not have any side effects.\n        init (torch.Tensor or pytree with tensor leaves): The inital scan carry, a tensor, or nested pytree of tensors.\n            The ``init`` is expected to have the same pytree structure as the first output element (i.e. carry)\n            of ``combine_fn``.\n        xs (torch.Tensor or pytree with tensor leaves): The input tensor, or nested pytree of tensors.\n\n    Kwargs:\n        dim (int): the dimension to scan over, default 0.\n        reverse (bool): A boolean stating if the scan should be reversed with respect to ``dim``, default ``False``.\n\n    Returns:\n        final_carry (torch.Tensor or pytree with tensor leaves),\n            the final carry of the scan operation with same pytree structure as init.\n        out (torch.Tensor or pytree with tensor leaves),\n            each tensor leaf is a stacked output along first dim, where each slice is the output of a scan iteration.\n\n    Example::\n\n        def add(x: torch.Tensor, y: torch.Tensor):\n            next_carry = y = x + y\n            return next_carry, y\n\n        i0 = torch.zeros(1)\n        xs = torch.arange(5)\n        # returns torch.tensor([10.]), torch.tensor([[0], [1.], [3.], [6.], [10.]])\n        last_carry, cumsum = scan(add, init=i0, xs=xs)\n\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._higher_order_ops.strict_mode",
      "signature": "torch._higher_order_ops.strict_mode(callable, operands)",
      "doc": "",
      "arguments": [
        "callable",
        "operands"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._higher_order_ops.while_loop",
      "signature": "torch._higher_order_ops.while_loop(cond_fn, body_fn, carried_inputs)",
      "doc": "\n    Run body_fn(*carried_inputs) while cond_fn(*carried_inputs) returns a True scalar tensor. Returns the output of body_fn or\n    initial carried_inputs.\n\n    .. warning::\n        `torch.while_loop` is a prototype feature in PyTorch. It has limited support for input and output types and\n        doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\n        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    `while_loop` is a structured control flow operator. It preserves the loop semantic across the torch.compile and torch.export.\n\n    `while_loop` is equivalent to the following:\n\n        def while_loop(cond_fn, body_fn, carried_inputs):\n            val = carried_inputs\n            while cond_fn(*val):\n                val = body_fn(*val)\n            return val\n\n    Args:\n        cond_fn (Callable): A callable function that returns a boolean Scalar tensor or a python boolean.\n\n        body_fn (Callable): A callable function that takes the same inputs as `cond_fn` and returns a tuple of tensors or ints\n\n        carried_inputs (Tuple of possibly nested dict/list/tuple of tensors or ints): A tuple of inputs to cond_fn and body_fn.\n            It's also the initial value of states that are carried across iterations. Note that when pass an integer as carry,\n            the corresponding return of while_loop will be another int with unknown values because we don't know how many\n            iterations while_loop will run.\n\n    Example 1:\n\n        def cond_fn(iter, x):\n            return iter.sum() < 10\n\n        def body_fn(iter, x):\n            return iter + 1, x.sin()\n\n        while_loop(cond_fn, body_fn, (torch.zeros(1), torch.randn(3, 4)))\n\n    Example 2:\n\n        def cond_fn(int_iter, x):\n            return 2 * int_iter < x.shape[0]\n\n        def body_fn(int_iter, x):\n            return int_iter + 1, x + int_iter\n\n        while_loop(cond,_fn, body_fn, (0, torch.randn(3, 4)))\n\n    Restrictions:\n\n        - body_fn must return tensors or int with the same metadata (e.g.shape, dtype) as inputs.\n\n        - body_fn and cond_fn must not in-place mutate the carried_inputs. A clone before the mutation is required.\n\n        - body_fn and cond_fn must not mutate python varialbles (e.g. list/dict) created outside of the body_fn.\n\n        - body_fn and cond_fn's output cannot aliase any of the inputs. A clone is required.\n\n    .. warning::\n        Temporal Limitations:\n\n        - 'while_loop' only supports **inference** right now. Autograd will be supported in the future.\n\n    ",
      "arguments": [
        "cond_fn",
        "body_fn",
        "carried_inputs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Run body_fn(*carried_inputs) while cond_fn(*carried_inputs) returns a True scalar tensor. Returns the output of body_fn or\n    initial carried_inputs.\n\n    .. warning::\n        `torch.while_loop` is a prototype feature in PyTorch. It has limited support for input and output types and\n        doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\n        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\n    `while_loop` is a structured control flow operator. It preserves the loop semantic across the torch.compile and torch.export.\n\n    `while_loop` is equivalent to the following:\n\n        def while_loop(cond_fn, body_fn, carried_inputs):\n            val = carried_inputs\n            while cond_fn(*val):\n                val = body_fn(*val)\n            return val\n\n    Args:\n        cond_fn (Callable): A callable function that returns a boolean Scalar tensor or a python boolean.\n\n        body_fn (Callable): A callable function that takes the same inputs as `cond_fn` and returns a tuple of tensors or ints\n\n        carried_inputs (Tuple of possibly nested dict/list/tuple of tensors or ints): A tuple of inputs to cond_fn and body_fn.\n            It's also the initial value of states that are carried across iterations. Note that when pass an integer as carry,\n            the corresponding return of while_loop will be another int with unknown values because we don't know how many\n            iterations while_loop will run.\n\n    Example 1:\n\n        def cond_fn(iter, x):\n            return iter.sum() < 10\n\n        def body_fn(iter, x):\n            return iter + 1, x.sin()\n\n        while_loop(cond_fn, body_fn, (torch.zeros(1), torch.randn(3, 4)))\n\n    Example 2:\n\n        def cond_fn(int_iter, x):\n            return 2 * int_iter < x.shape[0]\n\n        def body_fn(int_iter, x):\n            return int_iter + 1, x + int_iter\n\n        while_loop(cond,_fn, body_fn, (0, torch.randn(3, 4)))\n\n    Restrictions:\n\n        - body_fn must return tensors or int with the same metadata (e.g.shape, dtype) as inputs.\n\n        - body_fn and cond_fn must not in-place mutate the carried_inputs. A clone before the mutation is required.\n\n        - body_fn and cond_fn must not mutate python varialbles (e.g. list/dict) created outside of the body_fn.\n\n        - body_fn and cond_fn's output cannot aliase any of the inputs. A clone is required.\n\n    .. warning::\n        Temporal Limitations:\n\n        - 'while_loop' only supports **inference** right now. Autograd will be supported in the future.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._inductor.aot_compile",
      "signature": "torch._inductor.aot_compile(gm: 'torch.fx.GraphModule', args: 'tuple[Any]', kwargs: 'Optional[dict[str, Any]]' = None, *, options: 'Optional[dict[str, Any]]' = None) -> 'Union[str, list[str]]'",
      "doc": "\n    Ahead-of-time compile a given FX graph with TorchInductor into a shared library.\n\n    Args:\n        gm: The FX graph to compile.\n        args:  Example arguments\n        kwargs: Example keyword arguments\n        options:  Optional dict of config options.  See `torch._inductor.config`.\n\n    Returns:\n        Path to the generated shared library, or a list of files generated by\n        AOTI if aot_inductor.package=True.\n        TODO: make it return a list by default\n    ",
      "arguments": [
        "gm",
        "args",
        "kwargs",
        "options"
      ],
      "return_type": "Union[str, list[str]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Ahead-of-time compile a given FX graph with TorchInductor into a shared library.\n\n    Args:\n        gm: The FX graph to compile.\n        args:  Example arguments\n        kwargs: Example keyword arguments\n        options:  Optional dict of config options.  See `torch._inductor.config`.\n\n    Returns:\n        Path to the generated shared library, or a list of files generated by\n        AOTI if aot_inductor.package=True.\n        TODO: make it return a list by default\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._inductor.aoti_compile_and_package",
      "signature": "torch._inductor.aoti_compile_and_package(exported_program: 'ExportedProgram', _deprecated_unused_args=None, _deprecated_unused_kwargs=None, *, package_path: 'Optional[FileLike]' = None, inductor_configs: 'Optional[dict[str, Any]]' = None) -> 'str'",
      "doc": "\n    Compiles the exported program with AOTInductor, and packages it into a .pt2\n    artifact specified by the input package_path. To load the package, you can\n    call ``torch._inductor.aoti_load_package(package_path)``.\n\n    An example usage is as follows:\n\n    .. code-block:: python\n\n        ep = torch.export.export(M(), ...)\n        aoti_file = torch._inductor.aoti_compile_and_package(\n            ep, package_path=\"my_package.pt2\"\n        )\n        compiled_model = torch._inductor.aoti_load_package(\"my_package.pt2\")\n\n    To compile and save multiple models into a single ``.pt2`` artifact, you can do\n    the following:\n\n    .. code-block:: python\n\n        ep1 = torch.export.export(M1(), ...)\n        aoti_file1 = torch._inductor.aot_compile(\n            ep1, ..., options={\"aot_inductor.package\": True}\n        )\n        ep2 = torch.export.export(M2(), ...)\n        aoti_file2 = torch._inductor.aot_compile(\n            ep2, ..., options={\"aot_inductor.package\": True}\n        )\n\n        from torch._inductor.package import package_aoti, load_package\n\n        package_aoti(\"my_package.pt2\", {\"model1\": aoti_file1, \"model2\": aoti_file2})\n\n        compiled_model1 = load_package(\"my_package.pt2\", \"model1\")\n        compiled_model2 = load_package(\"my_package.pt2\", \"model2\")\n\n    Args:\n        exported_program: An exported program created through a call from torch.export\n        package_path: Optional specified path to the generated .pt2 artifact.\n        inductor_configs: Optional dictionary of configs to control inductor.\n\n    Returns:\n        Path to the generated artifact\n    ",
      "arguments": [
        "exported_program",
        "_deprecated_unused_args",
        "_deprecated_unused_kwargs",
        "package_path",
        "inductor_configs"
      ],
      "return_type": "str",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Compiles the exported program with AOTInductor, and packages it into a .pt2\n    artifact specified by the input package_path. To load the package, you can\n    call ``torch._inductor.aoti_load_package(package_path)``.\n\n    An example usage is as follows:\n\n    .. code-block:: python\n\n        ep = torch.export.export(M(), ...)\n        aoti_file = torch._inductor.aoti_compile_and_package(\n            ep, package_path=\"my_package.pt2\"\n        )\n        compiled_model = torch._inductor.aoti_load_package(\"my_package.pt2\")\n\n    To compile and save multiple models into a single ``.pt2`` artifact, you can do\n    the following:\n\n    .. code-block:: python\n\n        ep1 = torch.export.export(M1(), ...)\n        aoti_file1 = torch._inductor.aot_compile(\n            ep1, ..., options={\"aot_inductor.package\": True}\n        )\n        ep2 = torch.export.export(M2(), ...)\n        aoti_file2 = torch._inductor.aot_compile(\n            ep2, ..., options={\"aot_inductor.package\": True}\n        )\n\n        from torch._inductor.package import package_aoti, load_package\n\n        package_aoti(\"my_package.pt2\", {\"model1\": aoti_file1, \"model2\": aoti_file2})\n\n        compiled_model1 = load_package(\"my_package.pt2\", \"model1\")\n        compiled_model2 = load_package(\"my_package.pt2\", \"model2\")\n\n    Args:\n        exported_program: An exported program created through a call from torch.export\n        package_path: Optional specified path to the generated .pt2 artifact.\n        inductor_configs: Optional dictionary of configs to control inductor.\n\n    Returns:\n        Path to the generated artifact\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._inductor.aoti_load_package",
      "signature": "torch._inductor.aoti_load_package(path: 'FileLike', run_single_threaded: 'bool' = False) -> 'Any'",
      "doc": "\n    Loads the model from the PT2 package.\n\n    If multiple models were packaged into the PT2, this will load the default\n    model. To load a specific model, you can directly call the load API\n\n    .. code-block:: python\n\n        from torch._inductor.package import load_package\n\n        compiled_model1 = load_package(\"my_package.pt2\", \"model1\")\n        compiled_model2 = load_package(\"my_package.pt2\", \"model2\")\n\n    Args:\n        path: Path to the .pt2 package\n        run_single_threaded (bool): Whether the model should be run without\n            thread synchronization logic. This is useful to avoid conflicts with\n            CUDAGraphs.\n    ",
      "arguments": [
        "path",
        "run_single_threaded"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Loads the model from the PT2 package.\n\n    If multiple models were packaged into the PT2, this will load the default\n    model. To load a specific model, you can directly call the load API\n\n    .. code-block:: python\n\n        from torch._inductor.package import load_package\n\n        compiled_model1 = load_package(\"my_package.pt2\", \"model1\")\n        compiled_model2 = load_package(\"my_package.pt2\", \"model2\")\n\n    Args:\n        path: Path to the .pt2 package\n        run_single_threaded (bool): Whether the model should be run without\n            thread synchronization logic. This is useful to avoid conflicts with\n            CUDAGraphs.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._inductor.compile",
      "signature": "torch._inductor.compile(gm: 'torch.fx.GraphModule', example_inputs: 'list[InputType]', options: 'Optional[dict[str, Any]]' = None)",
      "doc": "\n    Compile a given FX graph with TorchInductor.  This allows compiling\n    FX graphs captured without using TorchDynamo.\n\n    Args:\n        gm: The FX graph to compile.\n        example_inputs:  List of tensor inputs.\n        options:  Optional dict of config options.  See `torch._inductor.config`.\n\n    Returns:\n        Callable with same behavior as gm but faster.\n    ",
      "arguments": [
        "gm",
        "example_inputs",
        "options"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Compile a given FX graph with TorchInductor.  This allows compiling\n    FX graphs captured without using TorchDynamo.\n\n    Args:\n        gm: The FX graph to compile.\n        example_inputs:  List of tensor inputs.\n        options:  Optional dict of config options.  See `torch._inductor.config`.\n\n    Returns:\n        Callable with same behavior as gm but faster.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._inductor.cudagraph_mark_step_begin",
      "signature": "torch._inductor.cudagraph_mark_step_begin()",
      "doc": "Indicates that a new iteration of inference or training is about to begin.",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Indicates that a new iteration of inference or training is about to begin.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._inductor.list_mode_options",
      "signature": "torch._inductor.list_mode_options(mode: 'Optional[str]' = None, dynamic: 'Optional[bool]' = None) -> 'dict[str, Any]'",
      "doc": "Returns a dictionary describing the optimizations that each of the available\n    modes passed to `torch.compile()` performs.\n\n    Args:\n        mode (str, optional): The mode to return the optimizations for.\n        If None, returns optimizations for all modes\n        dynamic (bool, optional): Whether dynamic shape is enabled.\n\n    Example::\n        >>> torch._inductor.list_mode_options()\n    ",
      "arguments": [
        "mode",
        "dynamic"
      ],
      "return_type": "dict[str, Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns a dictionary describing the optimizations that each of the available\n    modes passed to `torch.compile()` performs.\n\n    Args:\n        mode (str, optional): The mode to return the optimizations for.\n        If None, returns optimizations for all modes\n        dynamic (bool, optional): Whether dynamic shape is enabled.\n\n    Example::\n        >>> torch._inductor.list_mode_options()\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._inductor.list_options",
      "signature": "torch._inductor.list_options() -> 'list[str]'",
      "doc": "Returns a dictionary describing the optimizations and debug configurations\n    that are available to `torch.compile()`.\n\n    The options are documented in `torch._inductor.config`.\n\n    Example::\n\n        >>> torch._inductor.list_options()\n    ",
      "arguments": [],
      "return_type": "list[str]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns a dictionary describing the optimizations and debug configurations\n    that are available to `torch.compile()`.\n\n    The options are documented in `torch._inductor.config`.\n\n    Example::\n\n        >>> torch._inductor.list_options()\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._inductor.standalone_compile",
      "signature": "torch._inductor.standalone_compile(gm: 'torch.fx.GraphModule', example_inputs: 'list[InputType]', options: 'Optional[dict[str, Any]]' = None) -> 'CompiledArtifact'",
      "doc": "\n    Precompilation API for inductor.\n\n    .. code-block:: python\n\n        compiled_artifact = torch._inductor.standalone_compile(gm, args)\n        compiled_artifact.save(path=path, format=\"binary\")\n\n        # Later on a new process\n        loaded = torch._inductor.CompiledArtifact.load(path=path, format=\"binary\")\n        compiled_out = loaded(*args)\n\n    Args:\n        gm: Graph Module\n        example_inputs: Inputs for the graph module\n        options: Inductor compilation options\n\n    Returns:\n        CompiledArtifact that can be saved to disk or invoked directly.\n    ",
      "arguments": [
        "gm",
        "example_inputs",
        "options"
      ],
      "return_type": "CompiledArtifact",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Precompilation API for inductor.\n\n    .. code-block:: python\n\n        compiled_artifact = torch._inductor.standalone_compile(gm, args)\n        compiled_artifact.save(path=path, format=\"binary\")\n\n        # Later on a new process\n        loaded = torch._inductor.CompiledArtifact.load(path=path, format=\"binary\")\n        compiled_out = loaded(*args)\n\n    Args:\n        gm: Graph Module\n        example_inputs: Inputs for the graph module\n        options: Inductor compilation options\n\n    Returns:\n        CompiledArtifact that can be saved to disk or invoked directly.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.add_step_closure",
      "signature": "torch._lazy.add_step_closure(closure, args=(), run_async=False)",
      "doc": "Adds a closure to the list of the ones to be run at the end of the step.\n    Many times during model training there is the need to print/report (print to\n    console, post to tensorboard, etc...) information which require the content of\n    intermediary tensors to be inspected.\n    Inspecting different tensors content in different points of the model code\n    requires many executions and typically causes performance issues.\n    Adding a step closure will ensure that it will be run after the barrier, when\n    all the live tensors will be already materialized to device data.\n    Live tensors which will include the ones captured by the closure arguments.\n    So using `add_step_closure()` will ensure a single execution will be\n    performed, even when multiple closures are queued, requiring multiple tensors\n    to be inspected.\n    Step closures will be run sequentially in the order they have been queued.\n    Note that even though using this API the execution will be optimized, it is\n    advised to throttle the printing/reporting events once every N steps.\n    Args:\n      closure (callable): The function to be called.\n      args (tuple): The arguments to be passed to the closure.\n      run_async: If True, run the closure asynchronously.\n    ",
      "arguments": [
        "closure",
        "args",
        "run_async"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Adds a closure to the list of the ones to be run at the end of the step.\n    Many times during model training there is the need to print/report (print to\n    console, post to tensorboard, etc...) information which require the content of\n    intermediary tensors to be inspected.\n    Inspecting different tensors content in different points of the model code\n    requires many executions and typically causes performance issues.\n    Adding a step closure will ensure that it will be run after the barrier, when\n    all the live tensors will be already materialized to device data.\n    Live tensors which will include the ones captured by the closure arguments.\n    So using `add_step_closure()` will ensure a single execution will be\n    performed, even when multiple closures are queued, requiring multiple tensors\n    to be inspected.\n    Step closures will be run sequentially in the order they have been queued.\n    Note that even though using this API the execution will be optimized, it is\n    advised to throttle the printing/reporting events once every N steps.\n    Args:\n      closure (callable): The function to be called.\n      args (tuple): The arguments to be passed to the closure.\n      run_async: If True, run the closure asynchronously.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.get_tensor_id",
      "signature": "torch._lazy.get_tensor_id(tensor)",
      "doc": "Return a unique id of the lazy tensor maintained by LTC",
      "arguments": [
        "tensor"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a unique id of the lazy tensor maintained by LTC",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.mark_step",
      "signature": "torch._lazy.mark_step(device: str = '', wait=False)",
      "doc": "Triggers a mark step, which amounts to\n    - collecting a group of 'live' lazy tensors to index into the compilation cache\n      (lowering/compiling their IR graphs if not cached)\n    - kicking off execution of the compiled function\n    - (optionally, wait=True) waiting for cpu-side execution to complete (does not sync the accelerator)\n    ",
      "arguments": [
        "device",
        "wait"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Triggers a mark step, which amounts to\n    - collecting a group of 'live' lazy tensors to index into the compilation cache\n      (lowering/compiling their IR graphs if not cached)\n    - kicking off execution of the compiled function\n    - (optionally, wait=True) waiting for cpu-side execution to complete (does not sync the accelerator)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.run_step_closures",
      "signature": "torch._lazy.run_step_closures()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.save",
      "signature": "torch._lazy.save(tensors, *args, **kwargs)",
      "doc": "",
      "arguments": [
        "tensors",
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.sync_multi",
      "signature": "torch._lazy.sync_multi(tensors, devices)",
      "doc": "\n    Sync the list of lazy tensors so there IR get lowered for the activate backend\n    and the compiled computation graph get cached.\n    ",
      "arguments": [
        "tensors",
        "devices"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Sync the list of lazy tensors so there IR get lowered for the activate backend\n    and the compiled computation graph get cached.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.to_cpu",
      "signature": "torch._lazy.to_cpu(tensors, devices=None)",
      "doc": "",
      "arguments": [
        "tensors",
        "devices"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.tree_flatten",
      "signature": "torch._lazy.tree_flatten(tree: Any, is_leaf: Optional[Callable[[Any], bool]] = None) -> tuple[list[typing.Any], torch.utils._pytree.TreeSpec]",
      "doc": "Flattens a pytree into a list of values and a TreeSpec that can be used\n    to reconstruct the pytree.\n    ",
      "arguments": [
        "tree",
        "is_leaf"
      ],
      "return_type": "tuple[list[typing.Any], torch.utils._pytree.TreeSpec]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Flattens a pytree into a list of values and a TreeSpec that can be used\n    to reconstruct the pytree.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.tree_unflatten",
      "signature": "torch._lazy.tree_unflatten(leaves: collections.abc.Iterable[typing.Any], treespec: torch.utils._pytree.TreeSpec) -> Any",
      "doc": "Given a list of values and a TreeSpec, builds a pytree.\n    This is the inverse operation of `tree_flatten`.\n    ",
      "arguments": [
        "leaves",
        "treespec"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Given a list of values and a TreeSpec, builds a pytree.\n    This is the inverse operation of `tree_flatten`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._lazy.wait_device_ops",
      "signature": "torch._lazy.wait_device_ops(devices=None)",
      "doc": "Waits for all the async operations on the given devices to complete.\n    Args:\n      devices (string..., optional): The devices whose async ops need to be waited\n        for. If empty, all the local devices will be waited for.\n    ",
      "arguments": [
        "devices"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Waits for all the async operations on the given devices to complete.\n    Args:\n      devices (string..., optional): The devices whose async ops need to be waited\n        for. If empty, all the local devices will be waited for.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._library.capture_triton",
      "signature": "torch._library.capture_triton(triton_kernel: Callable, /) -> Any",
      "doc": "This API has been renamed to wrap_triton",
      "arguments": [
        "triton_kernel"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "This API has been renamed to wrap_triton",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._library.register_fake_class",
      "signature": "torch._library.register_fake_class(qualname, fake_class: Optional[torch._library.fake_class_registry.HasStaticMethodFromReal] = None)",
      "doc": "Register a fake implementation for this class.\n\n    It's in the same spirit of registering a fake implementation for\n    an operator but with the difference that it\n    associates a fake class with the original torch bind class (registered\n    with torch::class_). In this way, torch.compile can handle them properly\n    in components such as Dynamo and AOTAutograd.\n\n    This API may be used as a decorator (see example). For the fake class, users\n    are required to provide a from_real classmethod that takes a real object and\n    returns an instance of the fake class. All tensors in the fake object should also\n    be properly fakified with to_fake_tensor() in from_real.\n\n\n    Examples:\n        # For a custom class Foo defined in test_custom_class_registration.cpp:\n\n        TORCH_LIBRARY(_TorchScriptTesting, m) {\n          m.class_<TensorQueue>(\"_TensorQueue\")\n            .def(torch::init<at::Tensor>())\n            .def(\"push\", &TensorQueue::push)\n            .def(\"pop\", &TensorQueue::pop)\n            .def(\"top\", &TensorQueue::top)\n            .def(\"size\", &TensorQueue::size)\n            .def(\"clone_queue\", &TensorQueue::clone_queue)\n            .def(\"__obj_flatten__\", &TensorQueue::__obj_flatten__)\n            .def_pickle(\n                // __getstate__\n                [](const c10::intrusive_ptr<TensorQueue>& self)\n                    -> c10::Dict<std::string, at::Tensor> {\n                  return self->serialize();\n                },\n                // __setstate__\n                [](c10::Dict<std::string, at::Tensor> data)\n                    -> c10::intrusive_ptr<TensorQueue> {\n                  return c10::make_intrusive<TensorQueue>(std::move(data));\n                });\n            };\n        # We could register a fake class FakeTensorQueue in Python as follows:\n        import torch\n\n        @torch._library.register_fake_class(\"_TorchScriptTesting::_TensorQueue\")\n        class FakeTensorQueue:\n            def __init__(self, queue):\n                self.queue = queue\n\n            @classmethod\n            def __obj_unflatten__(cls, flattened_ctx):\n                return cls(**dict(ctx))\n\n            def push(self, x):\n                self.queue.append(x)\n\n            def pop(self):\n                return self.queue.pop(0)\n\n            def size(self):\n                return len(self.queue)\n\n    In this example, the original TensorQeue need to addd a __obj_flatten__ method\n    to the class TensorQueue and the flattend result is passed into FakeTensorQueue's\n    __obj_unflatten__ as inputs to create a fake class. This protocol allows pytorch to look\n    at the contents of the script object and properly handle them in the subsystems\n    like dynamo, aot_aotugrad or more.\n    ",
      "arguments": [
        "qualname",
        "fake_class"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Register a fake implementation for this class.\n\n    It's in the same spirit of registering a fake implementation for\n    an operator but with the difference that it\n    associates a fake class with the original torch bind class (registered\n    with torch::class_). In this way, torch.compile can handle them properly\n    in components such as Dynamo and AOTAutograd.\n\n    This API may be used as a decorator (see example). For the fake class, users\n    are required to provide a from_real classmethod that takes a real object and\n    returns an instance of the fake class. All tensors in the fake object should also\n    be properly fakified with to_fake_tensor() in from_real.\n\n\n    Examples:\n        # For a custom class Foo defined in test_custom_class_registration.cpp:\n\n        TORCH_LIBRARY(_TorchScriptTesting, m) {\n          m.class_<TensorQueue>(\"_TensorQueue\")\n            .def(torch::init<at::Tensor>())\n            .def(\"push\", &TensorQueue::push)\n            .def(\"pop\", &TensorQueue::pop)\n            .def(\"top\", &TensorQueue::top)\n            .def(\"size\", &TensorQueue::size)\n            .def(\"clone_queue\", &TensorQueue::clone_queue)\n            .def(\"__obj_flatten__\", &TensorQueue::__obj_flatten__)\n            .def_pickle(\n                // __getstate__\n                [](const c10::intrusive_ptr<TensorQueue>& self)\n                    -> c10::Dict<std::string, at::Tensor> {\n                  return self->serialize();\n                },\n                // __setstate__\n                [](c10::Dict<std::string, at::Tensor> data)\n                    -> c10::intrusive_ptr<TensorQueue> {\n                  return c10::make_intrusive<TensorQueue>(std::move(data));\n                });\n            };\n        # We could register a fake class FakeTensorQueue in Python as follows:\n        import torch\n\n        @torch._library.register_fake_class(\"_TorchScriptTesting::_TensorQueue\")\n        class FakeTensorQueue:\n            def __init__(self, queue):\n                self.queue = queue\n\n            @classmethod\n            def __obj_unflatten__(cls, flattened_ctx):\n                return cls(**dict(ctx))\n\n            def push(self, x):\n                self.queue.append(x)\n\n            def pop(self):\n                return self.queue.pop(0)\n\n            def size(self):\n                return len(self.queue)\n\n    In this example, the original TensorQeue need to addd a __obj_flatten__ method\n    to the class TensorQueue and the flattend result is passed into FakeTensorQueue's\n    __obj_unflatten__ as inputs to create a fake class. This protocol allows pytorch to look\n    at the contents of the script object and properly handle them in the subsystems\n    like dynamo, aot_aotugrad or more.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._library.triton_op",
      "signature": "torch._library.triton_op(name: str, fn: Optional[Callable] = None, /, *, mutates_args: Union[str, collections.abc.Iterable[str]], schema: Optional[str] = None) -> Callable",
      "doc": "Create a custom operator whose implementation is backed by 1+ triton kernels.\n\n    This is a more structured way of using triton kernels with PyTorch.\n    Prefer using triton kernels with no ``torch.library`` custom operator wrappers\n    (like :func:`torch.library.custom_op`, :func:`torch.library.triton_op`) because\n    that is simpler;\n    only use :func:`torch.library.custom_op`/:func:`torch.library.triton_op` if you\n    want to create an operator that behaves like PyTorch built-in operators.\n    For example, you may use a ``torch.library`` wrapper API to define the\n    behavior of the triton kernel when passed a tensor subclass or under\n    a TorchDispatchMode.\n\n    Use :func:`torch.library.triton_op` instead of :func:`torch.library.custom_op`\n    when the implementation\n    consists of 1+ triton kernels. :func:`torch.library.custom_op` treats\n    custom operators as opaque (:func:`torch.compile` and\n    :func:`torch.export.export` will never trace into them), but ``triton_op``\n    makes the implementation visible to these subsystems, allowing them\n    to optimize the triton kernel(s).\n\n    Note that ``fn`` must only consist of calls to PyTorch-understood\n    operators and triton kernels. Any triton kernels called inside ``fn``\n    must be wrapped in a call to :func:`torch.library.wrap_triton`.\n\n    Args:\n        name (str): A name for the custom op that looks like \"{namespace}::{name}\",\n            e.g. \"mylib::my_linear\". The name is used as the op's stable identifier\n            in PyTorch subsystems (e.g. torch.export, FX graphs).\n            To avoid name collisions, please use your project name as the namespace;\n            e.g. all custom ops in pytorch/fbgemm use \"fbgemm\" as the namespace.\n        mutates_args (Iterable[str] or \"unknown\"): The names of args that the function mutates.\n            This MUST be accurate, otherwise, the behavior is undefined. If \"unknown\",\n            it pessimistically assumes that all inputs to the operator are being mutated.\n        schema (None | str): A schema string for the operator. If None\n            (recommended) we'll infer a schema for the operator from its type\n            annotations. We recommend letting us infer a schema unless you\n            have a specific reason not to.\n            Example: \"(Tensor x, int y) -> (Tensor, Tensor)\".\n\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> import torch\n        >>> from torch.library import triton_op, wrap_triton\n        >>>\n        >>> import triton\n        >>> from triton import language as tl\n        >>>\n        >>> @triton.jit\n        >>> def add_kernel(\n        >>>     in_ptr0,\n        >>>     in_ptr1,\n        >>>     out_ptr,\n        >>>     n_elements,\n        >>>     BLOCK_SIZE: \"tl.constexpr\",\n        >>> ):\n        >>>     pid = tl.program_id(axis=0)\n        >>>     block_start = pid * BLOCK_SIZE\n        >>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        >>>     mask = offsets < n_elements\n        >>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n        >>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n        >>>     output = x + y\n        >>>     tl.store(out_ptr + offsets, output, mask=mask)\n        >>>\n        >>> @triton_op(\"mylib::add\", mutates_args={})\n        >>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        >>>     output = torch.empty_like(x)\n        >>>     n_elements = output.numel()\n        >>>\n        >>>     def grid(meta):\n        >>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        >>>\n        >>>     # NB: we need to wrap the triton kernel in a call to wrap_triton\n        >>>     wrap_triton(add_kernel)[grid](x, y, output, n_elements, 16)\n        >>>     return output\n        >>>\n        >>> @torch.compile\n        >>> def f(x, y):\n        >>>     return add(x, y)\n        >>>\n        >>> x = torch.randn(3, device=\"cuda\")\n        >>> y = torch.randn(3, device=\"cuda\")\n        >>>\n        >>> z = f(x, y)\n        >>> assert torch.allclose(z, x + y)\n\n    ",
      "arguments": [
        "name",
        "fn",
        "mutates_args",
        "schema"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Create a custom operator whose implementation is backed by 1+ triton kernels.\n\n    This is a more structured way of using triton kernels with PyTorch.\n    Prefer using triton kernels with no ``torch.library`` custom operator wrappers\n    (like :func:`torch.library.custom_op`, :func:`torch.library.triton_op`) because\n    that is simpler;\n    only use :func:`torch.library.custom_op`/:func:`torch.library.triton_op` if you\n    want to create an operator that behaves like PyTorch built-in operators.\n    For example, you may use a ``torch.library`` wrapper API to define the\n    behavior of the triton kernel when passed a tensor subclass or under\n    a TorchDispatchMode.\n\n    Use :func:`torch.library.triton_op` instead of :func:`torch.library.custom_op`\n    when the implementation\n    consists of 1+ triton kernels. :func:`torch.library.custom_op` treats\n    custom operators as opaque (:func:`torch.compile` and\n    :func:`torch.export.export` will never trace into them), but ``triton_op``\n    makes the implementation visible to these subsystems, allowing them\n    to optimize the triton kernel(s).\n\n    Note that ``fn`` must only consist of calls to PyTorch-understood\n    operators and triton kernels. Any triton kernels called inside ``fn``\n    must be wrapped in a call to :func:`torch.library.wrap_triton`.\n\n    Args:\n        name (str): A name for the custom op that looks like \"{namespace}::{name}\",\n            e.g. \"mylib::my_linear\". The name is used as the op's stable identifier\n            in PyTorch subsystems (e.g. torch.export, FX graphs).\n            To avoid name collisions, please use your project name as the namespace;\n            e.g. all custom ops in pytorch/fbgemm use \"fbgemm\" as the namespace.\n        mutates_args (Iterable[str] or \"unknown\"): The names of args that the function mutates.\n            This MUST be accurate, otherwise, the behavior is undefined. If \"unknown\",\n            it pessimistically assumes that all inputs to the operator are being mutated.\n        schema (None | str): A schema string for the operator. If None\n            (recommended) we'll infer a schema for the operator from its type\n            annotations. We recommend letting us infer a schema unless you\n            have a specific reason not to.\n            Example: \"(Tensor x, int y) -> (Tensor, Tensor)\".\n\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> import torch\n        >>> from torch.library import triton_op, wrap_triton\n        >>>\n        >>> import triton\n        >>> from triton import language as tl\n        >>>\n        >>> @triton.jit\n        >>> def add_kernel(\n        >>>     in_ptr0,\n        >>>     in_ptr1,\n        >>>     out_ptr,\n        >>>     n_elements,\n        >>>     BLOCK_SIZE: \"tl.constexpr\",\n        >>> ):\n        >>>     pid = tl.program_id(axis=0)\n        >>>     block_start = pid * BLOCK_SIZE\n        >>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        >>>     mask = offsets < n_elements\n        >>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n        >>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n        >>>     output = x + y\n        >>>     tl.store(out_ptr + offsets, output, mask=mask)\n        >>>\n        >>> @triton_op(\"mylib::add\", mutates_args={})\n        >>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        >>>     output = torch.empty_like(x)\n        >>>     n_elements = output.numel()\n        >>>\n        >>>     def grid(meta):\n        >>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        >>>\n        >>>     # NB: we need to wrap the triton kernel in a call to wrap_triton\n        >>>     wrap_triton(add_kernel)[grid](x, y, output, n_elements, 16)\n        >>>     return output\n        >>>\n        >>> @torch.compile\n        >>> def f(x, y):\n        >>>     return add(x, y)\n        >>>\n        >>> x = torch.randn(3, device=\"cuda\")\n        >>> y = torch.randn(3, device=\"cuda\")\n        >>>\n        >>> z = f(x, y)\n        >>> assert torch.allclose(z, x + y)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._library.wrap_triton",
      "signature": "torch._library.wrap_triton(triton_kernel: Callable, /) -> Any",
      "doc": "Allows capture of a triton kernel into a graph via make_fx or\n    non-strict ``torch.export``.\n\n    These technologies perform Dispatcher-based tracing (via\n    ``__torch_dispatch__``) and cannot see calls to raw triton kernels.\n    The ``wrap_triton`` API wraps a triton kernel into a callable that\n    can actually be traced into a graph.\n\n    Please use this API together with :func:`torch.library.triton_op`.\n\n    Examples:\n\n        >>> # xdoctest: +SKIP\n        >>> import torch\n        >>> import triton\n        >>> from triton import language as tl\n        >>> from torch.fx.experimental.proxy_tensor import make_fx\n        >>> from torch.library import wrap_triton\n        >>>\n        >>> @triton.jit\n        >>> def add_kernel(\n        >>>     in_ptr0,\n        >>>     in_ptr1,\n        >>>     out_ptr,\n        >>>     n_elements,\n        >>>     BLOCK_SIZE: \"tl.constexpr\",\n        >>> ):\n        >>>     pid = tl.program_id(axis=0)\n        >>>     block_start = pid * BLOCK_SIZE\n        >>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        >>>     mask = offsets < n_elements\n        >>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n        >>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n        >>>     output = x + y\n        >>>     tl.store(out_ptr + offsets, output, mask=mask)\n        >>>\n        >>> def add(x, y):\n        >>>     output = torch.empty_like(x)\n        >>>     n_elements = output.numel()\n        >>>\n        >>>     def grid_fn(meta):\n        >>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        >>>\n        >>>     wrap_triton(add_kernel)[grid_fn](x, y, output, n_elements, 16)\n        >>>     return output\n        >>>\n        >>> x = torch.randn(3, device=\"cuda\")\n        >>> y = torch.randn(3, device=\"cuda\")\n        >>> gm = make_fx(add)(x, y)\n        >>> print(gm.code)\n        >>> # def forward(self, x_1, y_1):\n        >>> #     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)\n        >>> #     triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation(\n        >>> #         kernel_idx = 0, constant_args_idx = 0,\n        >>> #         grid = [(1, 1, 1)], kwargs = {\n        >>> #             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,\n        >>> #             'n_elements': 3, 'BLOCK_SIZE': 16\n        >>> #         })\n        >>> #     return empty_like\n\n    ",
      "arguments": [
        "triton_kernel"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Allows capture of a triton kernel into a graph via make_fx or\n    non-strict ``torch.export``.\n\n    These technologies perform Dispatcher-based tracing (via\n    ``__torch_dispatch__``) and cannot see calls to raw triton kernels.\n    The ``wrap_triton`` API wraps a triton kernel into a callable that\n    can actually be traced into a graph.\n\n    Please use this API together with :func:`torch.library.triton_op`.\n\n    Examples:\n\n        >>> # xdoctest: +SKIP\n        >>> import torch\n        >>> import triton\n        >>> from triton import language as tl\n        >>> from torch.fx.experimental.proxy_tensor import make_fx\n        >>> from torch.library import wrap_triton\n        >>>\n        >>> @triton.jit\n        >>> def add_kernel(\n        >>>     in_ptr0,\n        >>>     in_ptr1,\n        >>>     out_ptr,\n        >>>     n_elements,\n        >>>     BLOCK_SIZE: \"tl.constexpr\",\n        >>> ):\n        >>>     pid = tl.program_id(axis=0)\n        >>>     block_start = pid * BLOCK_SIZE\n        >>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        >>>     mask = offsets < n_elements\n        >>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n        >>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n        >>>     output = x + y\n        >>>     tl.store(out_ptr + offsets, output, mask=mask)\n        >>>\n        >>> def add(x, y):\n        >>>     output = torch.empty_like(x)\n        >>>     n_elements = output.numel()\n        >>>\n        >>>     def grid_fn(meta):\n        >>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        >>>\n        >>>     wrap_triton(add_kernel)[grid_fn](x, y, output, n_elements, 16)\n        >>>     return output\n        >>>\n        >>> x = torch.randn(3, device=\"cuda\")\n        >>> y = torch.randn(3, device=\"cuda\")\n        >>> gm = make_fx(add)(x, y)\n        >>> print(gm.code)\n        >>> # def forward(self, x_1, y_1):\n        >>> #     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)\n        >>> #     triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation(\n        >>> #         kernel_idx = 0, constant_args_idx = 0,\n        >>> #         grid = [(1, 1, 1)], kwargs = {\n        >>> #             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,\n        >>> #             'n_elements': 3, 'BLOCK_SIZE': 16\n        >>> #         })\n        >>> #     return empty_like\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._logging.dtrace_structured",
      "signature": "torch._logging.dtrace_structured(name: str, metadata_fn: Callable[[], Union[dict[str, Any], tuple[str, int]]] = <class 'dict'>, *, payload_fn: Callable[[], Union[str, object, NoneType]] = <function <lambda> at 0x11319f9c0>, suppress_context: bool = False, expect_trace_id: bool = False, record_logging_overhead: bool = True)",
      "doc": "\n    For logging more detailed information used for debugging. This may result in\n    the program becoming slow.\n    ",
      "arguments": [
        "name",
        "metadata_fn",
        "payload_fn",
        "suppress_context",
        "expect_trace_id",
        "record_logging_overhead"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    For logging more detailed information used for debugging. This may result in\n    the program becoming slow.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._logging.getArtifactLogger",
      "signature": "torch._logging.getArtifactLogger(module_qname, artifact_name)",
      "doc": "",
      "arguments": [
        "module_qname",
        "artifact_name"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._logging.get_structured_logging_overhead",
      "signature": "torch._logging.get_structured_logging_overhead() -> Optional[float]",
      "doc": "",
      "arguments": [],
      "return_type": "typing.Optional[float]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._logging.set_logs",
      "signature": "torch._logging.set_logs(*, all: Optional[int] = None, dynamo: Optional[int] = None, aot: Optional[int] = None, autograd: Optional[int] = None, dynamic: Optional[int] = None, inductor: Optional[int] = None, distributed: Optional[int] = None, c10d: Optional[int] = None, ddp: Optional[int] = None, fsdp: Optional[int] = None, dtensor: Optional[int] = None, onnx: Optional[int] = None, bytecode: bool = False, aot_graphs: bool = False, aot_joint_graph: bool = False, ddp_graphs: bool = False, graph: bool = False, graph_code: bool = False, graph_breaks: bool = False, graph_sizes: bool = False, guards: bool = False, recompiles: bool = False, recompiles_verbose: bool = False, trace_source: bool = False, trace_call: bool = False, trace_bytecode: bool = False, output_code: bool = False, kernel_code: bool = False, schedule: bool = False, perf_hints: bool = False, pre_grad_graphs: bool = False, post_grad_graphs: bool = False, ir_pre_fusion: bool = False, ir_post_fusion: bool = False, onnx_diagnostics: bool = False, fusion: bool = False, overlap: bool = False, export: Optional[int] = None, modules: Optional[dict[str, Union[int, bool]]] = None, cudagraphs: bool = False, sym_node: bool = False, compiled_autograd: bool = False, compiled_autograd_verbose: bool = False, cudagraph_static_inputs: bool = False, benchmarking: bool = False, autotuning: bool = False, graph_region_expansion: bool = False)",
      "doc": "\n    Sets the log level for individual components and toggles individual log\n    artifact types.\n\n    .. warning:: This feature is a prototype and may have compatibility\n        breaking changes in the future.\n\n    .. note:: The ``TORCH_LOGS`` environment variable has complete precedence\n        over this function, so if it was set, this function does nothing.\n\n    A component is a set of related features in PyTorch. All of the log\n    messages emitted from a given component have their own log levels. If the\n    log level of a particular message has priority greater than or equal to its\n    component's log level setting, it is emitted. Otherwise, it is suppressed.\n    This allows you to, for instance, silence large groups of log messages that\n    are not relevant to you and increase verbosity of logs for components that\n    are relevant. The expected log level values, ordered from highest to lowest\n    priority, are:\n\n        * ``logging.CRITICAL``\n        * ``logging.ERROR``\n        * ``logging.WARNING``\n        * ``logging.INFO``\n        * ``logging.DEBUG``\n        * ``logging.NOTSET``\n\n    See documentation for the Python ``logging`` module for more information on\n    log levels: `<https://docs.python.org/3/library/logging.html#logging-levels>`_\n\n    An artifact is a particular type of log message. Each artifact is assigned\n    to a parent component. A component can emit many different kinds of\n    artifacts. In general, an artifact is emitted if either its corresponding\n    setting in the argument list below is turned on or if its parent component\n    is set to a log level less than or equal to the log level of the artifact.\n\n    Keyword args:\n        all (:class:`Optional[int]`):\n            The default log level for all components. Default: ``logging.WARN``\n\n        dynamo (:class:`Optional[int]`):\n            The log level for the TorchDynamo component. Default: ``logging.WARN``\n\n        aot (:class:`Optional[int]`):\n            The log level for the AOTAutograd component. Default: ``logging.WARN``\n\n        autograd (:class:`Optional[int]`):\n            The log level for autograd. Default: ``logging.WARN``\n\n        inductor (:class:`Optional[int]`):\n            The log level for the TorchInductor component. Default: ``logging.WARN``\n\n        dynamic (:class:`Optional[int]`):\n            The log level for dynamic shapes. Default: ``logging.WARN``\n\n        distributed (:class:`Optional[int]`):\n            Whether to log c10d communication operations and other debug info from PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        c10d (:class:`Optional[int]`):\n            Whether to log c10d communication operations related debug info in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        ddp (:class:`Optional[int]`):\n            Whether to log debug info related to ``DistributedDataParallel``(DDP) from PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        fsdp (:class:`Optional[int]`):\n            Whether to log debug info related to ``FullyShardedDataParallel``(FSDP) in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        dtensor (:class:`Optional[int]`):\n            Whether to log debug info related to ``DTensor``(DTensor) in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        onnx (:class:`Optional[int]`):\n            The log level for the ONNX exporter component. Default: ``logging.WARN``\n\n        bytecode (:class:`bool`):\n            Whether to emit the original and generated bytecode from TorchDynamo.\n            Default: ``False``\n\n        aot_graphs (:class:`bool`):\n            Whether to emit the graphs generated by AOTAutograd. Default: ``False``\n\n        aot_joint_graph (:class:`bool`):\n            Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: ``False``\n\n        ddp_graphs (:class:`bool`):\n            Whether to emit graphs generated by DDPOptimizer. Default: ``False``\n\n        graph (:class:`bool`):\n            Whether to emit the graph captured by TorchDynamo in tabular format.\n            Default: ``False``\n\n        graph_code (:class:`bool`):\n            Whether to emit the python source of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        graph_breaks (:class:`bool`):\n            Whether to emit the graph breaks encountered by TorchDynamo.\n            Default: ``False``\n\n        graph_sizes (:class:`bool`):\n            Whether to emit tensor sizes of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        guards (:class:`bool`):\n            Whether to emit the guards generated by TorchDynamo for each compiled\n            function. Default: ``False``\n\n        recompiles (:class:`bool`):\n            Whether to emit a guard failure reason and message every time\n            TorchDynamo recompiles a function. Default: ``False``\n\n        recompiles_verbose (:class:`bool`):\n            Whether to emit all guard failure reasons when TorchDynamo recompiles\n            a function, even those that are not actually run. Default: ``False``\n\n        trace_source (:class:`bool`):\n            Whether to emit when TorchDynamo begins tracing a new line. Default: ``False``\n\n        trace_call (:class:`bool`):\n            Whether to emit detailed line location when TorchDynamo creates an FX node\n            corresponding to function call. Python 3.11+ only. Default: ``False``\n\n        trace_bytecode (:class:`bool`):\n            Whether to emit bytecode instructions and traced stack state as TorchDynamo\n            traces bytecode. Default: ``False``\n\n        output_code (:class:`bool`):\n            Whether to emit the TorchInductor output code on a per-graph basis. Default: ``False``\n\n        kernel_code (:class:`bool`):\n            Whether to emit the TorchInductor output code on a per-kernel bases. Default: ``False``\n\n        schedule (:class:`bool`):\n            Whether to emit the TorchInductor schedule. Default: ``False``\n\n        perf_hints (:class:`bool`):\n            Whether to emit the TorchInductor perf hints. Default: ``False``\n\n        pre_grad_graphs (:class:`bool`):\n            Whether to emit the graphs before inductor grad passes. Default: ``False``\n\n        post_grad_graphs (:class:`bool`):\n            Whether to emit the graphs generated by after post grad passes. Default: ``False``\n\n        ir_pre_fusion (:class:`bool`):\n            Whether to emit the graphs before inductor fusion passes. Default: ``False``\n\n        ir_post_fusion (:class:`bool`):\n            Whether to emit the graphs after inductor fusion passes. Default: ``False``\n\n        onnx_diagnostics (:class:`bool`):\n            Whether to emit the ONNX exporter diagnostics in logging. Default: ``False``\n\n        fusion (:class:`bool`):\n            Whether to emit detailed Inductor fusion decisions. Default: ``False``\n\n        overlap (:class:`bool`):\n            Whether to emit detailed Inductor compute/comm overlap decisions. Default: ``False``\n\n        sym_node (:class:`bool`):\n            Whether to emit debug info for various SymNode opterations. Default: ``False``\n\n        export (:class:`Optional[int]`):\n            The log level for export. Default: ``logging.WARN``\n\n        benchmarking (:class:`bool`):\n            Whether to emit detailed Inductor benchmarking information. Default: ``False``\n\n        modules (dict):\n            This argument provides an alternate way to specify the above log\n            component and artifact settings, in the format of a keyword args\n            dictionary given as a single argument. There are two cases\n            where this is useful (1) if a new log component or artifact has\n            been registered but a keyword argument for it has not been added\n            to this function and (2) if the log level for an unregistered module\n            needs to be set. This can be done by providing the fully-qualified module\n            name as the key, with the log level as the value. Default: ``None``\n\n        cudagraph_static_inputs (:class:`bool`):\n            Whether to emit debug info for cudagraph static input detection. Default: ``False``\n\n        autotuning (:class:`bool`):\n            Autotuning choice logs, such as kernel source, perf, and tuning parameters. Default: ``False``\n\n        graph_region_expansion (:class:`bool`):\n            Whether to emit the detailed steps of the duplicate graph region tracker expansion algorithm. Default: ``False``\n\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> import logging\n\n        # The following changes the \"dynamo\" component to emit DEBUG-level\n        # logs, and to emit \"graph_code\" artifacts.\n\n        >>> torch._logging.set_logs(dynamo=logging.DEBUG, graph_code=True)\n\n        # The following enables the logs for a different module\n\n        >>> torch._logging.set_logs(modules={\"unregistered.module.name\": logging.DEBUG})\n    ",
      "arguments": [
        "all",
        "dynamo",
        "aot",
        "autograd",
        "dynamic",
        "inductor",
        "distributed",
        "c10d",
        "ddp",
        "fsdp",
        "dtensor",
        "onnx",
        "bytecode",
        "aot_graphs",
        "aot_joint_graph",
        "ddp_graphs",
        "graph",
        "graph_code",
        "graph_breaks",
        "graph_sizes",
        "guards",
        "recompiles",
        "recompiles_verbose",
        "trace_source",
        "trace_call",
        "trace_bytecode",
        "output_code",
        "kernel_code",
        "schedule",
        "perf_hints",
        "pre_grad_graphs",
        "post_grad_graphs",
        "ir_pre_fusion",
        "ir_post_fusion",
        "onnx_diagnostics",
        "fusion",
        "overlap",
        "export",
        "modules",
        "cudagraphs",
        "sym_node",
        "compiled_autograd",
        "compiled_autograd_verbose",
        "cudagraph_static_inputs",
        "benchmarking",
        "autotuning",
        "graph_region_expansion"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Sets the log level for individual components and toggles individual log\n    artifact types.\n\n    .. warning:: This feature is a prototype and may have compatibility\n        breaking changes in the future.\n\n    .. note:: The ``TORCH_LOGS`` environment variable has complete precedence\n        over this function, so if it was set, this function does nothing.\n\n    A component is a set of related features in PyTorch. All of the log\n    messages emitted from a given component have their own log levels. If the\n    log level of a particular message has priority greater than or equal to its\n    component's log level setting, it is emitted. Otherwise, it is suppressed.\n    This allows you to, for instance, silence large groups of log messages that\n    are not relevant to you and increase verbosity of logs for components that\n    are relevant. The expected log level values, ordered from highest to lowest\n    priority, are:\n\n        * ``logging.CRITICAL``\n        * ``logging.ERROR``\n        * ``logging.WARNING``\n        * ``logging.INFO``\n        * ``logging.DEBUG``\n        * ``logging.NOTSET``\n\n    See documentation for the Python ``logging`` module for more information on\n    log levels: `<https://docs.python.org/3/library/logging.html#logging-levels>`_\n\n    An artifact is a particular type of log message. Each artifact is assigned\n    to a parent component. A component can emit many different kinds of\n    artifacts. In general, an artifact is emitted if either its corresponding\n    setting in the argument list below is turned on or if its parent component\n    is set to a log level less than or equal to the log level of the artifact.\n\n    Keyword args:\n        all (:class:`Optional[int]`):\n            The default log level for all components. Default: ``logging.WARN``\n\n        dynamo (:class:`Optional[int]`):\n            The log level for the TorchDynamo component. Default: ``logging.WARN``\n\n        aot (:class:`Optional[int]`):\n            The log level for the AOTAutograd component. Default: ``logging.WARN``\n\n        autograd (:class:`Optional[int]`):\n            The log level for autograd. Default: ``logging.WARN``\n\n        inductor (:class:`Optional[int]`):\n            The log level for the TorchInductor component. Default: ``logging.WARN``\n\n        dynamic (:class:`Optional[int]`):\n            The log level for dynamic shapes. Default: ``logging.WARN``\n\n        distributed (:class:`Optional[int]`):\n            Whether to log c10d communication operations and other debug info from PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        c10d (:class:`Optional[int]`):\n            Whether to log c10d communication operations related debug info in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        ddp (:class:`Optional[int]`):\n            Whether to log debug info related to ``DistributedDataParallel``(DDP) from PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        fsdp (:class:`Optional[int]`):\n            Whether to log debug info related to ``FullyShardedDataParallel``(FSDP) in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        dtensor (:class:`Optional[int]`):\n            Whether to log debug info related to ``DTensor``(DTensor) in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        onnx (:class:`Optional[int]`):\n            The log level for the ONNX exporter component. Default: ``logging.WARN``\n\n        bytecode (:class:`bool`):\n            Whether to emit the original and generated bytecode from TorchDynamo.\n            Default: ``False``\n\n        aot_graphs (:class:`bool`):\n            Whether to emit the graphs generated by AOTAutograd. Default: ``False``\n\n        aot_joint_graph (:class:`bool`):\n            Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: ``False``\n\n        ddp_graphs (:class:`bool`):\n            Whether to emit graphs generated by DDPOptimizer. Default: ``False``\n\n        graph (:class:`bool`):\n            Whether to emit the graph captured by TorchDynamo in tabular format.\n            Default: ``False``\n\n        graph_code (:class:`bool`):\n            Whether to emit the python source of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        graph_breaks (:class:`bool`):\n            Whether to emit the graph breaks encountered by TorchDynamo.\n            Default: ``False``\n\n        graph_sizes (:class:`bool`):\n            Whether to emit tensor sizes of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        guards (:class:`bool`):\n            Whether to emit the guards generated by TorchDynamo for each compiled\n            function. Default: ``False``\n\n        recompiles (:class:`bool`):\n            Whether to emit a guard failure reason and message every time\n            TorchDynamo recompiles a function. Default: ``False``\n\n        recompiles_verbose (:class:`bool`):\n            Whether to emit all guard failure reasons when TorchDynamo recompiles\n            a function, even those that are not actually run. Default: ``False``\n\n        trace_source (:class:`bool`):\n            Whether to emit when TorchDynamo begins tracing a new line. Default: ``False``\n\n        trace_call (:class:`bool`):\n            Whether to emit detailed line location when TorchDynamo creates an FX node\n            corresponding to function call. Python 3.11+ only. Default: ``False``\n\n        trace_bytecode (:class:`bool`):\n            Whether to emit bytecode instructions and traced stack state as TorchDynamo\n            traces bytecode. Default: ``False``\n\n        output_code (:class:`bool`):\n            Whether to emit the TorchInductor output code on a per-graph basis. Default: ``False``\n\n        kernel_code (:class:`bool`):\n            Whether to emit the TorchInductor output code on a per-kernel bases. Default: ``False``\n\n        schedule (:class:`bool`):\n            Whether to emit the TorchInductor schedule. Default: ``False``\n\n        perf_hints (:class:`bool`):\n            Whether to emit the TorchInductor perf hints. Default: ``False``\n\n        pre_grad_graphs (:class:`bool`):\n            Whether to emit the graphs before inductor grad passes. Default: ``False``\n\n        post_grad_graphs (:class:`bool`):\n            Whether to emit the graphs generated by after post grad passes. Default: ``False``\n\n        ir_pre_fusion (:class:`bool`):\n            Whether to emit the graphs before inductor fusion passes. Default: ``False``\n\n        ir_post_fusion (:class:`bool`):\n            Whether to emit the graphs after inductor fusion passes. Default: ``False``\n\n        onnx_diagnostics (:class:`bool`):\n            Whether to emit the ONNX exporter diagnostics in logging. Default: ``False``\n\n        fusion (:class:`bool`):\n            Whether to emit detailed Inductor fusion decisions. Default: ``False``\n\n        overlap (:class:`bool`):\n            Whether to emit detailed Inductor compute/comm overlap decisions. Default: ``False``\n\n        sym_node (:class:`bool`):\n            Whether to emit debug info for various SymNode opterations. Default: ``False``\n\n        export (:class:`Optional[int]`):\n            The log level for export. Default: ``logging.WARN``\n\n        benchmarking (:class:`bool`):\n            Whether to emit detailed Inductor benchmarking information. Default: ``False``\n\n        modules (dict):\n            This argument provides an alternate way to specify the above log\n            component and artifact settings, in the format of a keyword args\n            dictionary given as a single argument. There are two cases\n            where this is useful (1) if a new log component or artifact has\n            been registered but a keyword argument for it has not been added\n            to this function and (2) if the log level for an unregistered module\n            needs to be set. This can be done by providing the fully-qualified module\n            name as the key, with the log level as the value. Default: ``None``\n\n        cudagraph_static_inputs (:class:`bool`):\n            Whether to emit debug info for cudagraph static input detection. Default: ``False``\n\n        autotuning (:class:`bool`):\n            Autotuning choice logs, such as kernel source, perf, and tuning parameters. Default: ``False``\n\n        graph_region_expansion (:class:`bool`):\n            Whether to emit the detailed steps of the duplicate graph region tracker expansion algorithm. Default: ``False``\n\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> import logging\n\n        # The following changes the \"dynamo\" component to emit DEBUG-level\n        # logs, and to emit \"graph_code\" artifacts.\n\n        >>> torch._logging.set_logs(dynamo=logging.DEBUG, graph_code=True)\n\n        # The following enables the logs for a different module\n\n        >>> torch._logging.set_logs(modules={\"unregistered.module.name\": logging.DEBUG})\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._logging.trace_structured",
      "signature": "torch._logging.trace_structured(name: str, metadata_fn: Callable[[], Union[dict[str, Any], tuple[str, int]]] = <class 'dict'>, *, payload_fn: Callable[[], Union[str, object, NoneType]] = <function <lambda> at 0x11319f880>, suppress_context: bool = False, expect_trace_id: bool = True, record_logging_overhead: bool = True, compile_id: Optional[torch._guards.CompileId] = None) -> None",
      "doc": "\n    metadata is an arbitrary JSON compatible struct, but it's expected to not be\n    too long (e.g., less than 1MB)\n\n    payload is an arbitrary string, which can be arbitrarily long (but expected to have\n    newlines so no lines are too long)\n    ",
      "arguments": [
        "name",
        "metadata_fn",
        "payload_fn",
        "suppress_context",
        "expect_trace_id",
        "record_logging_overhead",
        "compile_id"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    metadata is an arbitrary JSON compatible struct, but it's expected to not be\n    too long (e.g., less than 1MB)\n\n    payload is an arbitrary string, which can be arbitrarily long (but expected to have\n    newlines so no lines are too long)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.abs",
      "signature": "torch._numpy.abs(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.absolute",
      "signature": "torch._numpy.absolute(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.add",
      "signature": "torch._numpy.add(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.all",
      "signature": "torch._numpy.all(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, *, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.allclose",
      "signature": "torch._numpy.allclose(a: 'ArrayLike', b: 'ArrayLike', rtol=1e-05, atol=1e-08, equal_nan=False)",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "rtol",
        "atol",
        "equal_nan"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.alltrue",
      "signature": "torch._numpy.alltrue(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, *, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.amax",
      "signature": "torch._numpy.amax(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, initial: 'NotImplementedType' = None, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims",
        "initial",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.amin",
      "signature": "torch._numpy.amin(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, initial: 'NotImplementedType' = None, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims",
        "initial",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.angle",
      "signature": "torch._numpy.angle(z: 'ArrayLike', deg=False)",
      "doc": "",
      "arguments": [
        "z",
        "deg"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.any",
      "signature": "torch._numpy.any(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, *, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.append",
      "signature": "torch._numpy.append(arr: 'ArrayLike', values: 'ArrayLike', axis=None)",
      "doc": "",
      "arguments": [
        "arr",
        "values",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.arange",
      "signature": "torch._numpy.arange(start: 'Optional[ArrayLikeOrScalar]' = None, stop: 'Optional[ArrayLikeOrScalar]' = None, step: 'Optional[ArrayLikeOrScalar]' = 1, dtype: 'Optional[DTypeLike]' = None, *, like: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "start",
        "stop",
        "step",
        "dtype",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.arccos",
      "signature": "torch._numpy.arccos(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.arccosh",
      "signature": "torch._numpy.arccosh(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.arcsin",
      "signature": "torch._numpy.arcsin(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.arcsinh",
      "signature": "torch._numpy.arcsinh(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.arctan",
      "signature": "torch._numpy.arctan(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.arctan2",
      "signature": "torch._numpy.arctan2(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.arctanh",
      "signature": "torch._numpy.arctanh(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.argmax",
      "signature": "torch._numpy.argmax(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, *, keepdims: 'KeepDims' = False)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.argmin",
      "signature": "torch._numpy.argmin(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, *, keepdims: 'KeepDims' = False)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.argsort",
      "signature": "torch._numpy.argsort(a: 'ArrayLike', axis=-1, kind=None, order: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "kind",
        "order"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.argwhere",
      "signature": "torch._numpy.argwhere(a: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.around",
      "signature": "torch._numpy.around(a: 'ArrayLike', decimals=0, out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "decimals",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.array",
      "signature": "torch._numpy.array(obj, dtype=None, *, copy=True, order='K', subok=False, ndmin=0, like=None)",
      "doc": "",
      "arguments": [
        "obj",
        "dtype",
        "copy",
        "order",
        "subok",
        "ndmin",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.array_equal",
      "signature": "torch._numpy.array_equal(a1: 'ArrayLike', a2: 'ArrayLike', equal_nan=False)",
      "doc": "",
      "arguments": [
        "a1",
        "a2",
        "equal_nan"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.array_equiv",
      "signature": "torch._numpy.array_equiv(a1: 'ArrayLike', a2: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a1",
        "a2"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.array_split",
      "signature": "torch._numpy.array_split(ary: 'ArrayLike', indices_or_sections, axis=0)",
      "doc": "",
      "arguments": [
        "ary",
        "indices_or_sections",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.asarray",
      "signature": "torch._numpy.asarray(a, dtype=None, order='K', *, like=None)",
      "doc": "",
      "arguments": [
        "a",
        "dtype",
        "order",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.ascontiguousarray",
      "signature": "torch._numpy.ascontiguousarray(a, dtype=None, *, like=None)",
      "doc": "",
      "arguments": [
        "a",
        "dtype",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.atleast_1d",
      "signature": "torch._numpy.atleast_1d(*arys: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "arys"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.atleast_2d",
      "signature": "torch._numpy.atleast_2d(*arys: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "arys"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.atleast_3d",
      "signature": "torch._numpy.atleast_3d(*arys: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "arys"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.average",
      "signature": "torch._numpy.average(a: 'ArrayLike', axis=None, weights: 'ArrayLike' = None, returned=False, *, keepdims=False)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "weights",
        "returned",
        "keepdims"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.bartlett",
      "signature": "torch._numpy.bartlett(M)",
      "doc": "",
      "arguments": [
        "M"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.bincount",
      "signature": "torch._numpy.bincount(x: 'ArrayLike', /, weights: 'Optional[ArrayLike]' = None, minlength=0)",
      "doc": "",
      "arguments": [
        "x",
        "weights",
        "minlength"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.bitwise_and",
      "signature": "torch._numpy.bitwise_and(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.bitwise_not",
      "signature": "torch._numpy.bitwise_not(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.bitwise_or",
      "signature": "torch._numpy.bitwise_or(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.bitwise_xor",
      "signature": "torch._numpy.bitwise_xor(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.blackman",
      "signature": "torch._numpy.blackman(M)",
      "doc": "",
      "arguments": [
        "M"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.broadcast_arrays",
      "signature": "torch._numpy.broadcast_arrays(*args: 'ArrayLike', subok: 'NotImplementedType' = False)",
      "doc": "",
      "arguments": [
        "args",
        "subok"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.broadcast_shapes",
      "signature": "torch._numpy.broadcast_shapes(*shapes)",
      "doc": "broadcast_shapes(*shapes) -> Size\n\n    Similar to :func:`broadcast_tensors` but for shapes.\n\n    This is equivalent to\n    ``torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape``\n    but avoids the need create to intermediate tensors. This is useful for\n    broadcasting tensors of common batch shape but different rightmost shape,\n    e.g. to broadcast mean vectors with covariance matrices.\n\n    Example::\n\n        >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\n        torch.Size([1, 3, 2])\n\n    Args:\n        \\*shapes (torch.Size): Shapes of tensors.\n\n    Returns:\n        shape (torch.Size): A shape compatible with all input shapes.\n\n    Raises:\n        RuntimeError: If shapes are incompatible.\n    ",
      "arguments": [
        "shapes"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "broadcast_shapes(*shapes) -> Size\n\n    Similar to :func:`broadcast_tensors` but for shapes.\n\n    This is equivalent to\n    ``torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape``\n    but avoids the need create to intermediate tensors. This is useful for\n    broadcasting tensors of common batch shape but different rightmost shape,\n    e.g. to broadcast mean vectors with covariance matrices.\n\n    Example::\n\n        >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\n        torch.Size([1, 3, 2])\n\n    Args:\n        \\*shapes (torch.Size): Shapes of tensors.\n\n    Returns:\n        shape (torch.Size): A shape compatible with all input shapes.\n\n    Raises:\n        RuntimeError: If shapes are incompatible.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.broadcast_to",
      "signature": "torch._numpy.broadcast_to(array: 'ArrayLike', shape, subok: 'NotImplementedType' = False)",
      "doc": "",
      "arguments": [
        "array",
        "shape",
        "subok"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.can_cast",
      "signature": "torch._numpy.can_cast(from_, to, casting='safe')",
      "doc": "",
      "arguments": [
        "from_",
        "to",
        "casting"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.cbrt",
      "signature": "torch._numpy.cbrt(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.ceil",
      "signature": "torch._numpy.ceil(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.choose",
      "signature": "torch._numpy.choose(a: 'ArrayLike', choices: 'Sequence[ArrayLike]', out: 'Optional[OutArray]' = None, mode: 'NotImplementedType' = 'raise')",
      "doc": "",
      "arguments": [
        "a",
        "choices",
        "out",
        "mode"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.clip",
      "signature": "torch._numpy.clip(a: 'ArrayLike', min: 'Optional[ArrayLike]' = None, max: 'Optional[ArrayLike]' = None, out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "min",
        "max",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.column_stack",
      "signature": "torch._numpy.column_stack(tup: 'Sequence[ArrayLike]', *, dtype: 'Optional[DTypeLike]' = None, casting: 'Optional[CastingModes]' = 'same_kind')",
      "doc": "",
      "arguments": [
        "tup",
        "dtype",
        "casting"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.common_type",
      "signature": "torch._numpy.common_type(*tensors: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.concatenate",
      "signature": "torch._numpy.concatenate(ar_tuple: 'Sequence[ArrayLike]', axis=0, out: 'Optional[OutArray]' = None, dtype: 'Optional[DTypeLike]' = None, casting: 'Optional[CastingModes]' = 'same_kind')",
      "doc": "",
      "arguments": [
        "ar_tuple",
        "axis",
        "out",
        "dtype",
        "casting"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.conj",
      "signature": "torch._numpy.conj(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.conjugate",
      "signature": "torch._numpy.conjugate(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.convolve",
      "signature": "torch._numpy.convolve(a: 'ArrayLike', v: 'ArrayLike', mode='full')",
      "doc": "",
      "arguments": [
        "a",
        "v",
        "mode"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.copy",
      "signature": "torch._numpy.copy(a: 'ArrayLike', order: 'NotImplementedType' = 'K', subok: 'NotImplementedType' = False)",
      "doc": "",
      "arguments": [
        "a",
        "order",
        "subok"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.copysign",
      "signature": "torch._numpy.copysign(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.copyto",
      "signature": "torch._numpy.copyto(dst: 'NDArray', src: 'ArrayLike', casting: 'Optional[CastingModes]' = 'same_kind', where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "dst",
        "src",
        "casting",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.corrcoef",
      "signature": "torch._numpy.corrcoef(x: 'ArrayLike', y: 'Optional[ArrayLike]' = None, rowvar=True, bias=None, ddof=None, *, dtype: 'Optional[DTypeLike]' = None)",
      "doc": "",
      "arguments": [
        "x",
        "y",
        "rowvar",
        "bias",
        "ddof",
        "dtype"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.correlate",
      "signature": "torch._numpy.correlate(a: 'ArrayLike', v: 'ArrayLike', mode='valid')",
      "doc": "",
      "arguments": [
        "a",
        "v",
        "mode"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.cos",
      "signature": "torch._numpy.cos(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.cosh",
      "signature": "torch._numpy.cosh(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.count_nonzero",
      "signature": "torch._numpy.count_nonzero(a: 'ArrayLike', axis: 'AxisLike' = None, *, keepdims: 'KeepDims' = False)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "keepdims"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.cov",
      "signature": "torch._numpy.cov(m: 'ArrayLike', y: 'Optional[ArrayLike]' = None, rowvar=True, bias=False, ddof=None, fweights: 'Optional[ArrayLike]' = None, aweights: 'Optional[ArrayLike]' = None, *, dtype: 'Optional[DTypeLike]' = None)",
      "doc": "",
      "arguments": [
        "m",
        "y",
        "rowvar",
        "bias",
        "ddof",
        "fweights",
        "aweights",
        "dtype"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.cross",
      "signature": "torch._numpy.cross(a: 'ArrayLike', b: 'ArrayLike', axisa=-1, axisb=-1, axisc=-1, axis=None)",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "axisa",
        "axisb",
        "axisc",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.cumprod",
      "signature": "torch._numpy.cumprod(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.cumproduct",
      "signature": "torch._numpy.cumproduct(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.cumsum",
      "signature": "torch._numpy.cumsum(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.deg2rad",
      "signature": "torch._numpy.deg2rad(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.degrees",
      "signature": "torch._numpy.degrees(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.diag",
      "signature": "torch._numpy.diag(v: 'ArrayLike', k=0)",
      "doc": "",
      "arguments": [
        "v",
        "k"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.diag_indices",
      "signature": "torch._numpy.diag_indices(n, ndim=2)",
      "doc": "",
      "arguments": [
        "n",
        "ndim"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.diag_indices_from",
      "signature": "torch._numpy.diag_indices_from(arr: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "arr"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.diagflat",
      "signature": "torch._numpy.diagflat(v: 'ArrayLike', k=0)",
      "doc": "",
      "arguments": [
        "v",
        "k"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.diagonal",
      "signature": "torch._numpy.diagonal(a: 'ArrayLike', offset=0, axis1=0, axis2=1)",
      "doc": "",
      "arguments": [
        "a",
        "offset",
        "axis1",
        "axis2"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.diff",
      "signature": "torch._numpy.diff(a: 'ArrayLike', n=1, axis=-1, prepend: 'Optional[ArrayLike]' = None, append: 'Optional[ArrayLike]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "n",
        "axis",
        "prepend",
        "append"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.divide",
      "signature": "torch._numpy.divide(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.divmod",
      "signature": "torch._numpy.divmod(x1: 'ArrayLike', x2: 'ArrayLike', out1: 'Optional[OutArray]' = None, out2: 'Optional[OutArray]' = None, /, out: 'tuple[Optional[OutArray], Optional[OutArray]]' = (None, None), *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out1",
        "out2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.dot",
      "signature": "torch._numpy.dot(a: 'ArrayLike', b: 'ArrayLike', out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.dsplit",
      "signature": "torch._numpy.dsplit(ary: 'ArrayLike', indices_or_sections)",
      "doc": "",
      "arguments": [
        "ary",
        "indices_or_sections"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.dstack",
      "signature": "torch._numpy.dstack(tup: 'Sequence[ArrayLike]', *, dtype: 'Optional[DTypeLike]' = None, casting: 'Optional[CastingModes]' = 'same_kind')",
      "doc": "",
      "arguments": [
        "tup",
        "dtype",
        "casting"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.dtype",
      "signature": "torch._numpy.dtype(arg)",
      "doc": "",
      "arguments": [
        "arg"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.einsum",
      "signature": "torch._numpy.einsum(*operands, out=None, dtype=None, order='K', casting='safe', optimize=False)",
      "doc": "",
      "arguments": [
        "operands",
        "out",
        "dtype",
        "order",
        "casting",
        "optimize"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.empty",
      "signature": "torch._numpy.empty(shape, dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'C', *, like: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "shape",
        "dtype",
        "order",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.empty_like",
      "signature": "torch._numpy.empty_like(prototype: 'ArrayLike', dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'K', subok: 'NotImplementedType' = False, shape=None)",
      "doc": "",
      "arguments": [
        "prototype",
        "dtype",
        "order",
        "subok",
        "shape"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.equal",
      "signature": "torch._numpy.equal(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.exp",
      "signature": "torch._numpy.exp(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.exp2",
      "signature": "torch._numpy.exp2(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.expand_dims",
      "signature": "torch._numpy.expand_dims(a: 'ArrayLike', axis)",
      "doc": "",
      "arguments": [
        "a",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.expm1",
      "signature": "torch._numpy.expm1(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.eye",
      "signature": "torch._numpy.eye(N, M=None, k=0, dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'C', *, like: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "N",
        "M",
        "k",
        "dtype",
        "order",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.fabs",
      "signature": "torch._numpy.fabs(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.fill_diagonal",
      "signature": "torch._numpy.fill_diagonal(a: 'ArrayLike', val: 'ArrayLike', wrap=False)",
      "doc": "",
      "arguments": [
        "a",
        "val",
        "wrap"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.finfo",
      "signature": "torch._numpy.finfo(dtyp)",
      "doc": "",
      "arguments": [
        "dtyp"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.fix",
      "signature": "torch._numpy.fix(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.flatnonzero",
      "signature": "torch._numpy.flatnonzero(a: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.flip",
      "signature": "torch._numpy.flip(m: 'ArrayLike', axis=None)",
      "doc": "",
      "arguments": [
        "m",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.fliplr",
      "signature": "torch._numpy.fliplr(m: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "m"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.flipud",
      "signature": "torch._numpy.flipud(m: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "m"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.float_power",
      "signature": "torch._numpy.float_power(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.floor",
      "signature": "torch._numpy.floor(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.floor_divide",
      "signature": "torch._numpy.floor_divide(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.fmax",
      "signature": "torch._numpy.fmax(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.fmin",
      "signature": "torch._numpy.fmin(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.fmod",
      "signature": "torch._numpy.fmod(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.from_dlpack",
      "signature": "torch._numpy.from_dlpack(x, /)",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.full",
      "signature": "torch._numpy.full(shape, fill_value: 'ArrayLike', dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'C', *, like: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "shape",
        "fill_value",
        "dtype",
        "order",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.full_like",
      "signature": "torch._numpy.full_like(a: 'ArrayLike', fill_value, dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'K', subok: 'NotImplementedType' = False, shape=None)",
      "doc": "",
      "arguments": [
        "a",
        "fill_value",
        "dtype",
        "order",
        "subok",
        "shape"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.gcd",
      "signature": "torch._numpy.gcd(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.geomspace",
      "signature": "torch._numpy.geomspace(start: 'ArrayLike', stop: 'ArrayLike', num=50, endpoint=True, dtype: 'Optional[DTypeLike]' = None, axis=0)",
      "doc": "",
      "arguments": [
        "start",
        "stop",
        "num",
        "endpoint",
        "dtype",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.gradient",
      "signature": "torch._numpy.gradient(f: 'ArrayLike', *varargs, axis=None, edge_order=1)",
      "doc": "",
      "arguments": [
        "f",
        "varargs",
        "axis",
        "edge_order"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.greater",
      "signature": "torch._numpy.greater(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.greater_equal",
      "signature": "torch._numpy.greater_equal(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.hamming",
      "signature": "torch._numpy.hamming(M)",
      "doc": "",
      "arguments": [
        "M"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.hanning",
      "signature": "torch._numpy.hanning(M)",
      "doc": "",
      "arguments": [
        "M"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.heaviside",
      "signature": "torch._numpy.heaviside(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.histogram",
      "signature": "torch._numpy.histogram(a: 'ArrayLike', bins: 'ArrayLike' = 10, range=None, normed=None, weights: 'Optional[ArrayLike]' = None, density=None)",
      "doc": "",
      "arguments": [
        "a",
        "bins",
        "range",
        "normed",
        "weights",
        "density"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.histogram2d",
      "signature": "torch._numpy.histogram2d(x, y, bins=10, range: 'Optional[ArrayLike]' = None, normed=None, weights: 'Optional[ArrayLike]' = None, density=None)",
      "doc": "",
      "arguments": [
        "x",
        "y",
        "bins",
        "range",
        "normed",
        "weights",
        "density"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.histogramdd",
      "signature": "torch._numpy.histogramdd(sample, bins=10, range: 'Optional[ArrayLike]' = None, normed=None, weights: 'Optional[ArrayLike]' = None, density=None)",
      "doc": "",
      "arguments": [
        "sample",
        "bins",
        "range",
        "normed",
        "weights",
        "density"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.hsplit",
      "signature": "torch._numpy.hsplit(ary: 'ArrayLike', indices_or_sections)",
      "doc": "",
      "arguments": [
        "ary",
        "indices_or_sections"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.hstack",
      "signature": "torch._numpy.hstack(tup: 'Sequence[ArrayLike]', *, dtype: 'Optional[DTypeLike]' = None, casting: 'Optional[CastingModes]' = 'same_kind')",
      "doc": "",
      "arguments": [
        "tup",
        "dtype",
        "casting"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.hypot",
      "signature": "torch._numpy.hypot(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.i0",
      "signature": "torch._numpy.i0(x: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.identity",
      "signature": "torch._numpy.identity(n, dtype: 'Optional[DTypeLike]' = None, *, like: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "n",
        "dtype",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.iinfo",
      "signature": "torch._numpy.iinfo(dtyp)",
      "doc": "",
      "arguments": [
        "dtyp"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.imag",
      "signature": "torch._numpy.imag(a: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.indices",
      "signature": "torch._numpy.indices(dimensions, dtype: 'Optional[DTypeLike]' = <class 'int'>, sparse=False)",
      "doc": "",
      "arguments": [
        "dimensions",
        "dtype",
        "sparse"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.inner",
      "signature": "torch._numpy.inner(a: 'ArrayLike', b: 'ArrayLike', /)",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.invert",
      "signature": "torch._numpy.invert(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isclose",
      "signature": "torch._numpy.isclose(a: 'ArrayLike', b: 'ArrayLike', rtol=1e-05, atol=1e-08, equal_nan=False)",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "rtol",
        "atol",
        "equal_nan"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.iscomplex",
      "signature": "torch._numpy.iscomplex(x: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.iscomplexobj",
      "signature": "torch._numpy.iscomplexobj(x: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isfinite",
      "signature": "torch._numpy.isfinite(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isinf",
      "signature": "torch._numpy.isinf(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isnan",
      "signature": "torch._numpy.isnan(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isneginf",
      "signature": "torch._numpy.isneginf(x: 'ArrayLike', out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "x",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isposinf",
      "signature": "torch._numpy.isposinf(x: 'ArrayLike', out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "x",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isreal",
      "signature": "torch._numpy.isreal(x: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isrealobj",
      "signature": "torch._numpy.isrealobj(x: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.isscalar",
      "signature": "torch._numpy.isscalar(a)",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.issubdtype",
      "signature": "torch._numpy.issubdtype(arg1, arg2)",
      "doc": "",
      "arguments": [
        "arg1",
        "arg2"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.kaiser",
      "signature": "torch._numpy.kaiser(M, beta)",
      "doc": "",
      "arguments": [
        "M",
        "beta"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.kron",
      "signature": "torch._numpy.kron(a: 'ArrayLike', b: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.lcm",
      "signature": "torch._numpy.lcm(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.ldexp",
      "signature": "torch._numpy.ldexp(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.left_shift",
      "signature": "torch._numpy.left_shift(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.less",
      "signature": "torch._numpy.less(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.less_equal",
      "signature": "torch._numpy.less_equal(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.linspace",
      "signature": "torch._numpy.linspace(start: 'ArrayLike', stop: 'ArrayLike', num=50, endpoint=True, retstep=False, dtype: 'Optional[DTypeLike]' = None, axis=0)",
      "doc": "",
      "arguments": [
        "start",
        "stop",
        "num",
        "endpoint",
        "retstep",
        "dtype",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.log",
      "signature": "torch._numpy.log(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.log10",
      "signature": "torch._numpy.log10(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.log1p",
      "signature": "torch._numpy.log1p(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.log2",
      "signature": "torch._numpy.log2(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.logaddexp",
      "signature": "torch._numpy.logaddexp(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.logaddexp2",
      "signature": "torch._numpy.logaddexp2(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.logical_and",
      "signature": "torch._numpy.logical_and(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.logical_not",
      "signature": "torch._numpy.logical_not(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.logical_or",
      "signature": "torch._numpy.logical_or(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.logical_xor",
      "signature": "torch._numpy.logical_xor(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.logspace",
      "signature": "torch._numpy.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype: 'Optional[DTypeLike]' = None, axis=0)",
      "doc": "",
      "arguments": [
        "start",
        "stop",
        "num",
        "endpoint",
        "base",
        "dtype",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.matmul",
      "signature": "torch._numpy.matmul(x1: 'ArrayLike', x2: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None, axes: 'NotImplementedType' = None, axis: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj",
        "axes",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.max",
      "signature": "torch._numpy.max(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, initial: 'NotImplementedType' = None, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims",
        "initial",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.maximum",
      "signature": "torch._numpy.maximum(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.mean",
      "signature": "torch._numpy.mean(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, *, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out",
        "keepdims",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.median",
      "signature": "torch._numpy.median(a: 'ArrayLike', axis=None, out: 'Optional[OutArray]' = None, overwrite_input=False, keepdims: 'KeepDims' = False)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "overwrite_input",
        "keepdims"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.meshgrid",
      "signature": "torch._numpy.meshgrid(*xi: 'ArrayLike', copy=True, sparse=False, indexing='xy')",
      "doc": "",
      "arguments": [
        "xi",
        "copy",
        "sparse",
        "indexing"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.min",
      "signature": "torch._numpy.min(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, initial: 'NotImplementedType' = None, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims",
        "initial",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.min_scalar_type",
      "signature": "torch._numpy.min_scalar_type(a: 'ArrayLike', /)",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.minimum",
      "signature": "torch._numpy.minimum(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.mod",
      "signature": "torch._numpy.mod(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.modf",
      "signature": "torch._numpy.modf(x, /, *args, **kwds)",
      "doc": "",
      "arguments": [
        "x",
        "args",
        "kwds"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.moveaxis",
      "signature": "torch._numpy.moveaxis(a: 'ArrayLike', source, destination)",
      "doc": "",
      "arguments": [
        "a",
        "source",
        "destination"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.multiply",
      "signature": "torch._numpy.multiply(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.nan_to_num",
      "signature": "torch._numpy.nan_to_num(x: 'ArrayLike', copy: 'NotImplementedType' = True, nan=0.0, posinf=None, neginf=None)",
      "doc": "",
      "arguments": [
        "x",
        "copy",
        "nan",
        "posinf",
        "neginf"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.ndim",
      "signature": "torch._numpy.ndim(a: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.negative",
      "signature": "torch._numpy.negative(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.nextafter",
      "signature": "torch._numpy.nextafter(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.nonzero",
      "signature": "torch._numpy.nonzero(a: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.not_equal",
      "signature": "torch._numpy.not_equal(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.ones",
      "signature": "torch._numpy.ones(shape, dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'C', *, like: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "shape",
        "dtype",
        "order",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.ones_like",
      "signature": "torch._numpy.ones_like(a: 'ArrayLike', dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'K', subok: 'NotImplementedType' = False, shape=None)",
      "doc": "",
      "arguments": [
        "a",
        "dtype",
        "order",
        "subok",
        "shape"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.outer",
      "signature": "torch._numpy.outer(a: 'ArrayLike', b: 'ArrayLike', out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.pad",
      "signature": "torch._numpy.pad(array: 'ArrayLike', pad_width: 'ArrayLike', mode='constant', **kwargs)",
      "doc": "",
      "arguments": [
        "array",
        "pad_width",
        "mode",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.percentile",
      "signature": "torch._numpy.percentile(a: 'ArrayLike', q: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, overwrite_input=False, method='linear', keepdims: 'KeepDims' = False, *, interpolation: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "q",
        "axis",
        "out",
        "overwrite_input",
        "method",
        "keepdims",
        "interpolation"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.positive",
      "signature": "torch._numpy.positive(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.power",
      "signature": "torch._numpy.power(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.prod",
      "signature": "torch._numpy.prod(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, initial: 'NotImplementedType' = None, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out",
        "keepdims",
        "initial",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.product",
      "signature": "torch._numpy.product(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, initial: 'NotImplementedType' = None, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out",
        "keepdims",
        "initial",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.ptp",
      "signature": "torch._numpy.ptp(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.put",
      "signature": "torch._numpy.put(a: 'NDArray', indices: 'ArrayLike', values: 'ArrayLike', mode: 'NotImplementedType' = 'raise')",
      "doc": "",
      "arguments": [
        "a",
        "indices",
        "values",
        "mode"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.put_along_axis",
      "signature": "torch._numpy.put_along_axis(arr: 'ArrayLike', indices: 'ArrayLike', values: 'ArrayLike', axis)",
      "doc": "",
      "arguments": [
        "arr",
        "indices",
        "values",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.quantile",
      "signature": "torch._numpy.quantile(a: 'ArrayLike', q: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, overwrite_input=False, method='linear', keepdims: 'KeepDims' = False, *, interpolation: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "q",
        "axis",
        "out",
        "overwrite_input",
        "method",
        "keepdims",
        "interpolation"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.rad2deg",
      "signature": "torch._numpy.rad2deg(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.radians",
      "signature": "torch._numpy.radians(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.ravel",
      "signature": "torch._numpy.ravel(a: 'ArrayLike', order: 'NotImplementedType' = 'C')",
      "doc": "",
      "arguments": [
        "a",
        "order"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.real",
      "signature": "torch._numpy.real(a: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.real_if_close",
      "signature": "torch._numpy.real_if_close(a: 'ArrayLike', tol=100)",
      "doc": "",
      "arguments": [
        "a",
        "tol"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.reciprocal",
      "signature": "torch._numpy.reciprocal(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.remainder",
      "signature": "torch._numpy.remainder(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.repeat",
      "signature": "torch._numpy.repeat(a: 'ArrayLike', repeats: 'ArrayLikeOrScalar', axis=None)",
      "doc": "",
      "arguments": [
        "a",
        "repeats",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.reshape",
      "signature": "torch._numpy.reshape(a: 'ArrayLike', newshape, order: 'NotImplementedType' = 'C')",
      "doc": "",
      "arguments": [
        "a",
        "newshape",
        "order"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.resize",
      "signature": "torch._numpy.resize(a: 'ArrayLike', new_shape=None)",
      "doc": "",
      "arguments": [
        "a",
        "new_shape"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.result_type",
      "signature": "torch._numpy.result_type(*arrays_and_dtypes)",
      "doc": "",
      "arguments": [
        "arrays_and_dtypes"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.right_shift",
      "signature": "torch._numpy.right_shift(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.rint",
      "signature": "torch._numpy.rint(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.roll",
      "signature": "torch._numpy.roll(a: 'ArrayLike', shift, axis=None)",
      "doc": "",
      "arguments": [
        "a",
        "shift",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.rollaxis",
      "signature": "torch._numpy.rollaxis(a: 'ArrayLike', axis, start=0)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "start"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.rot90",
      "signature": "torch._numpy.rot90(m: 'ArrayLike', k=1, axes=(0, 1))",
      "doc": "",
      "arguments": [
        "m",
        "k",
        "axes"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.round",
      "signature": "torch._numpy.round(a: 'ArrayLike', decimals=0, out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "decimals",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.round_",
      "signature": "torch._numpy.round_(a: 'ArrayLike', decimals=0, out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "decimals",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.row_stack",
      "signature": "torch._numpy.row_stack(tup: 'Sequence[ArrayLike]', *, dtype: 'Optional[DTypeLike]' = None, casting: 'Optional[CastingModes]' = 'same_kind')",
      "doc": "",
      "arguments": [
        "tup",
        "dtype",
        "casting"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.searchsorted",
      "signature": "torch._numpy.searchsorted(a: 'ArrayLike', v: 'ArrayLike', side='left', sorter: 'Optional[ArrayLike]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "v",
        "side",
        "sorter"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.set_default_dtype",
      "signature": "torch._numpy.set_default_dtype(fp_dtype='numpy', int_dtype='numpy')",
      "doc": "Set the (global) defaults for fp, complex, and int dtypes.\n\n    The complex dtype is inferred from the float (fp) dtype. It has\n    a width at least twice the width of the float dtype,\n    i.e., it's complex128 for float64 and complex64 for float32.\n\n    Parameters\n    ----------\n    fp_dtype\n        Allowed values are \"numpy\", \"pytorch\" or dtype_like things which\n        can be converted into a DType instance.\n        Default is \"numpy\" (i.e. float64).\n    int_dtype\n        Allowed values are \"numpy\", \"pytorch\" or dtype_like things which\n        can be converted into a DType instance.\n        Default is \"numpy\" (i.e. int64).\n\n    Returns\n    -------\n    The old default dtype state: a namedtuple with attributes ``float_dtype``,\n    ``complex_dtypes`` and ``int_dtype``. These attributes store *pytorch*\n    dtypes.\n\n    Notes\n    ------------\n    This functions has a side effect: it sets the global state with the provided dtypes.\n\n    The complex dtype has bit width of at least twice the width of the float\n    dtype, i.e. it's complex128 for float64 and complex64 for float32.\n\n    ",
      "arguments": [
        "fp_dtype",
        "int_dtype"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the (global) defaults for fp, complex, and int dtypes.\n\n    The complex dtype is inferred from the float (fp) dtype. It has\n    a width at least twice the width of the float dtype,\n    i.e., it's complex128 for float64 and complex64 for float32.\n\n    Parameters\n    ----------\n    fp_dtype\n        Allowed values are \"numpy\", \"pytorch\" or dtype_like things which\n        can be converted into a DType instance.\n        Default is \"numpy\" (i.e. float64).\n    int_dtype\n        Allowed values are \"numpy\", \"pytorch\" or dtype_like things which\n        can be converted into a DType instance.\n        Default is \"numpy\" (i.e. int64).\n\n    Returns\n    -------\n    The old default dtype state: a namedtuple with attributes ``float_dtype``,\n    ``complex_dtypes`` and ``int_dtype``. These attributes store *pytorch*\n    dtypes.\n\n    Notes\n    ------------\n    This functions has a side effect: it sets the global state with the provided dtypes.\n\n    The complex dtype has bit width of at least twice the width of the float\n    dtype, i.e. it's complex128 for float64 and complex64 for float32.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.shape",
      "signature": "torch._numpy.shape(a: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.sign",
      "signature": "torch._numpy.sign(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.signbit",
      "signature": "torch._numpy.signbit(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.sin",
      "signature": "torch._numpy.sin(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.sinc",
      "signature": "torch._numpy.sinc(x: 'ArrayLike')",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.sinh",
      "signature": "torch._numpy.sinh(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.size",
      "signature": "torch._numpy.size(a: 'ArrayLike', axis=None)",
      "doc": "",
      "arguments": [
        "a",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.sometrue",
      "signature": "torch._numpy.sometrue(a: 'ArrayLike', axis: 'AxisLike' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, *, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "out",
        "keepdims",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.sort",
      "signature": "torch._numpy.sort(a: 'ArrayLike', axis=-1, kind=None, order: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "kind",
        "order"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.split",
      "signature": "torch._numpy.split(ary: 'ArrayLike', indices_or_sections, axis=0)",
      "doc": "",
      "arguments": [
        "ary",
        "indices_or_sections",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.sqrt",
      "signature": "torch._numpy.sqrt(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.square",
      "signature": "torch._numpy.square(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.squeeze",
      "signature": "torch._numpy.squeeze(a: 'ArrayLike', axis=None)",
      "doc": "",
      "arguments": [
        "a",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.stack",
      "signature": "torch._numpy.stack(arrays: 'Sequence[ArrayLike]', axis=0, out: 'Optional[OutArray]' = None, *, dtype: 'Optional[DTypeLike]' = None, casting: 'Optional[CastingModes]' = 'same_kind')",
      "doc": "",
      "arguments": [
        "arrays",
        "axis",
        "out",
        "dtype",
        "casting"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.std",
      "signature": "torch._numpy.std(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None, ddof=0, keepdims: 'KeepDims' = False, *, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out",
        "ddof",
        "keepdims",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.subtract",
      "signature": "torch._numpy.subtract(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.sum",
      "signature": "torch._numpy.sum(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None, keepdims: 'KeepDims' = False, initial: 'NotImplementedType' = None, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out",
        "keepdims",
        "initial",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.swapaxes",
      "signature": "torch._numpy.swapaxes(a: 'ArrayLike', axis1, axis2)",
      "doc": "",
      "arguments": [
        "a",
        "axis1",
        "axis2"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.take",
      "signature": "torch._numpy.take(a: 'ArrayLike', indices: 'ArrayLike', axis=None, out: 'Optional[OutArray]' = None, mode: 'NotImplementedType' = 'raise')",
      "doc": "",
      "arguments": [
        "a",
        "indices",
        "axis",
        "out",
        "mode"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.take_along_axis",
      "signature": "torch._numpy.take_along_axis(arr: 'ArrayLike', indices: 'ArrayLike', axis)",
      "doc": "",
      "arguments": [
        "arr",
        "indices",
        "axis"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.tan",
      "signature": "torch._numpy.tan(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.tanh",
      "signature": "torch._numpy.tanh(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.tensordot",
      "signature": "torch._numpy.tensordot(a: 'ArrayLike', b: 'ArrayLike', axes=2)",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "axes"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.tile",
      "signature": "torch._numpy.tile(A: 'ArrayLike', reps)",
      "doc": "",
      "arguments": [
        "A",
        "reps"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.trace",
      "signature": "torch._numpy.trace(a: 'ArrayLike', offset=0, axis1=0, axis2=1, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None)",
      "doc": "",
      "arguments": [
        "a",
        "offset",
        "axis1",
        "axis2",
        "dtype",
        "out"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.transpose",
      "signature": "torch._numpy.transpose(a: 'ArrayLike', axes=None)",
      "doc": "",
      "arguments": [
        "a",
        "axes"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.tri",
      "signature": "torch._numpy.tri(N, M=None, k=0, dtype: 'Optional[DTypeLike]' = None, *, like: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "N",
        "M",
        "k",
        "dtype",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.tril",
      "signature": "torch._numpy.tril(m: 'ArrayLike', k=0)",
      "doc": "",
      "arguments": [
        "m",
        "k"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.tril_indices",
      "signature": "torch._numpy.tril_indices(n, k=0, m=None)",
      "doc": "",
      "arguments": [
        "n",
        "k",
        "m"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.tril_indices_from",
      "signature": "torch._numpy.tril_indices_from(arr: 'ArrayLike', k=0)",
      "doc": "",
      "arguments": [
        "arr",
        "k"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.triu",
      "signature": "torch._numpy.triu(m: 'ArrayLike', k=0)",
      "doc": "",
      "arguments": [
        "m",
        "k"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.triu_indices",
      "signature": "torch._numpy.triu_indices(n, k=0, m=None)",
      "doc": "",
      "arguments": [
        "n",
        "k",
        "m"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.triu_indices_from",
      "signature": "torch._numpy.triu_indices_from(arr: 'ArrayLike', k=0)",
      "doc": "",
      "arguments": [
        "arr",
        "k"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.true_divide",
      "signature": "torch._numpy.true_divide(x1: 'ArrayLikeOrScalar', x2: 'ArrayLikeOrScalar', /, out: 'Optional[OutArray]' = None, *, where: 'NotImplementedType' = True, casting: 'Optional[CastingModes]' = 'same_kind', order: 'NotImplementedType' = 'K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature: 'NotImplementedType' = None, extobj: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "x1",
        "x2",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.trunc",
      "signature": "torch._numpy.trunc(x: 'ArrayLike', /, out: 'Optional[OutArray]' = None, *, where=True, casting: 'Optional[CastingModes]' = 'same_kind', order='K', dtype: 'Optional[DTypeLike]' = None, subok: 'NotImplementedType' = False, signature=None, extobj=None)",
      "doc": "",
      "arguments": [
        "x",
        "out",
        "where",
        "casting",
        "order",
        "dtype",
        "subok",
        "signature",
        "extobj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.unique",
      "signature": "torch._numpy.unique(ar: 'ArrayLike', return_index: 'NotImplementedType' = False, return_inverse=False, return_counts=False, axis=None, *, equal_nan: 'NotImplementedType' = True)",
      "doc": "",
      "arguments": [
        "ar",
        "return_index",
        "return_inverse",
        "return_counts",
        "axis",
        "equal_nan"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.vander",
      "signature": "torch._numpy.vander(x: 'ArrayLike', N=None, increasing=False)",
      "doc": "",
      "arguments": [
        "x",
        "N",
        "increasing"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.var",
      "signature": "torch._numpy.var(a: 'ArrayLike', axis: 'AxisLike' = None, dtype: 'Optional[DTypeLike]' = None, out: 'Optional[OutArray]' = None, ddof=0, keepdims: 'KeepDims' = False, *, where: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "a",
        "axis",
        "dtype",
        "out",
        "ddof",
        "keepdims",
        "where"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.vdot",
      "signature": "torch._numpy.vdot(a: 'ArrayLike', b: 'ArrayLike', /)",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.vsplit",
      "signature": "torch._numpy.vsplit(ary: 'ArrayLike', indices_or_sections)",
      "doc": "",
      "arguments": [
        "ary",
        "indices_or_sections"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.vstack",
      "signature": "torch._numpy.vstack(tup: 'Sequence[ArrayLike]', *, dtype: 'Optional[DTypeLike]' = None, casting: 'Optional[CastingModes]' = 'same_kind')",
      "doc": "",
      "arguments": [
        "tup",
        "dtype",
        "casting"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.where",
      "signature": "torch._numpy.where(condition: 'ArrayLike', x: 'Optional[ArrayLikeOrScalar]' = None, y: 'Optional[ArrayLikeOrScalar]' = None, /)",
      "doc": "",
      "arguments": [
        "condition",
        "x",
        "y"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.zeros",
      "signature": "torch._numpy.zeros(shape, dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'C', *, like: 'NotImplementedType' = None)",
      "doc": "",
      "arguments": [
        "shape",
        "dtype",
        "order",
        "like"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._numpy.zeros_like",
      "signature": "torch._numpy.zeros_like(a: 'ArrayLike', dtype: 'Optional[DTypeLike]' = None, order: 'NotImplementedType' = 'K', subok: 'NotImplementedType' = False, shape=None)",
      "doc": "",
      "arguments": [
        "a",
        "dtype",
        "order",
        "subok",
        "shape"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.TensorMeta",
      "signature": "torch._prims.TensorMeta(tensorlike: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, *, shape: Union[torch.Size, list[int], tuple[int, ...], NoneType] = None, strides: Union[list[int], tuple[int, ...], NoneType] = None, dtype: Optional[torch.dtype] = None, device: Union[torch.device, str, NoneType] = None)",
      "doc": "",
      "arguments": [
        "tensorlike",
        "shape",
        "strides",
        "dtype",
        "device"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.backwards_not_supported",
      "signature": "torch._prims.backwards_not_supported(prim)",
      "doc": "",
      "arguments": [
        "prim"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.expand_dims",
      "signature": "torch._prims.expand_dims(a: torch.Tensor, dimensions: Union[list[int], tuple[int, ...]], ndim=None) -> torch.Tensor",
      "doc": "\n    Creates a view of a with a.ndim + len(dimensions) dimensions, with new\n    dimensions of length one at the dimensions specified by dimensions.\n    ",
      "arguments": [
        "a",
        "dimensions",
        "ndim"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Creates a view of a with a.ndim + len(dimensions) dimensions, with new\n    dimensions of length one at the dimensions specified by dimensions.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.handle_torch_function",
      "signature": "torch._prims.handle_torch_function(public_api: Callable, relevant_args: collections.abc.Iterable[typing.Any], *args, **kwargs) -> Any",
      "doc": "Implement a function with checks for ``__torch_function__`` overrides.\n\n    See torch::autograd::handle_torch_function for the equivalent of this\n    function in the C++ implementation.\n\n    Arguments\n    ---------\n    public_api : function\n        Function exposed by the public torch API originally called like\n        ``public_api(*args, **kwargs)`` on which arguments are now being\n        checked.\n    relevant_args : iterable\n        Iterable of arguments to check for __torch_function__ methods.\n    args : tuple\n        Arbitrary positional arguments originally passed into ``public_api``.\n    kwargs : tuple\n        Arbitrary keyword arguments originally passed into ``public_api``.\n\n    Returns\n    -------\n    object\n        Result from calling ``implementation`` or an ``__torch_function__``\n        method, as appropriate.\n\n    Raises\n    ------\n    TypeError : if no implementation is found.\n\n    Example\n    -------\n    >>> def func(a):\n    ...     if has_torch_function_unary(a):\n    ...         return handle_torch_function(func, (a,), a)\n    ...     return a + 0\n    ",
      "arguments": [
        "public_api",
        "relevant_args",
        "args",
        "kwargs"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Implement a function with checks for ``__torch_function__`` overrides.\n\n    See torch::autograd::handle_torch_function for the equivalent of this\n    function in the C++ implementation.\n\n    Arguments\n    ---------\n    public_api : function\n        Function exposed by the public torch API originally called like\n        ``public_api(*args, **kwargs)`` on which arguments are now being\n        checked.\n    relevant_args : iterable\n        Iterable of arguments to check for __torch_function__ methods.\n    args : tuple\n        Arbitrary positional arguments originally passed into ``public_api``.\n    kwargs : tuple\n        Arbitrary keyword arguments originally passed into ``public_api``.\n\n    Returns\n    -------\n    object\n        Result from calling ``implementation`` or an ``__torch_function__``\n        method, as appropriate.\n\n    Raises\n    ------\n    TypeError : if no implementation is found.\n\n    Example\n    -------\n    >>> def func(a):\n    ...     if has_torch_function_unary(a):\n    ...         return handle_torch_function(func, (a,), a)\n    ...     return a + 0\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.is_functional_schema",
      "signature": "torch._prims.is_functional_schema(schema: Any) -> bool",
      "doc": "Check if the schema is functional.\n\n    An operator is functional if:\n    - it does not mutate any of its inputs\n    - it does not return a view on any of its inputs\n    - it has at least one return\n    ",
      "arguments": [
        "schema"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if the schema is functional.\n\n    An operator is functional if:\n    - it does not mutate any of its inputs\n    - it does not return a view on any of its inputs\n    - it has at least one return\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.new_token_tensor",
      "signature": "torch._prims.new_token_tensor() -> torch.Tensor",
      "doc": "",
      "arguments": [],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.register_debug_prims",
      "signature": "torch._prims.register_debug_prims()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.register_rng_prims",
      "signature": "torch._prims.register_rng_prims()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.shift_right_logical",
      "signature": "torch._prims.shift_right_logical(*args, **kwargs)",
      "doc": "",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.sym_float",
      "signature": "torch._prims.sym_float(a)",
      "doc": "SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.torch_var",
      "signature": "torch._prims.torch_var(input, dim=None, correction=1, **kwargs)",
      "doc": "",
      "arguments": [
        "input",
        "dim",
        "correction",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.tree_flatten",
      "signature": "torch._prims.tree_flatten(tree: Any, is_leaf: Optional[Callable[[Any], bool]] = None) -> tuple[list[typing.Any], torch.utils._pytree.TreeSpec]",
      "doc": "Flattens a pytree into a list of values and a TreeSpec that can be used\n    to reconstruct the pytree.\n    ",
      "arguments": [
        "tree",
        "is_leaf"
      ],
      "return_type": "tuple[list[typing.Any], torch.utils._pytree.TreeSpec]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Flattens a pytree into a list of values and a TreeSpec that can be used\n    to reconstruct the pytree.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.tree_map",
      "signature": "torch._prims.tree_map(func: Callable[..., Any], tree: Any, *rests: Any, is_leaf: Optional[Callable[[Any], bool]] = None) -> Any",
      "doc": "Map a multi-input function over pytree args to produce a new pytree.\n\n    See also :func:`tree_map_`.\n\n    >>> tree_map(lambda x: x + 1, {'x': 7, 'y': (42, 64)})\n    {'x': 8, 'y': (43, 65)}\n    >>> tree_map(lambda x: x is None, {'x': 7, 'y': (42, 64), 'z': None})\n    {'x': False, 'y': (False, False), 'z': True}\n\n    If multiple inputs are given, the structure of the tree is taken from the first input;\n    subsequent inputs need only have ``tree`` as a prefix:\n\n    >>> tree_map(lambda x, y: [x] + y, [5, 6], [[7, 9], [1, 2]])\n    [[5, 7, 9], [6, 1, 2]]\n\n    Args:\n        func (callable): A function that takes ``1 + len(rests)`` arguments, to be applied at the\n            corresponding leaves of the pytrees.\n        tree (pytree): A pytree to be mapped over, with each leaf providing the first positional\n            argument to function ``func``.\n        rests (tuple of pytree): A tuple of pytrees, each of which has the same structure as\n            ``tree`` or has ``tree`` as a prefix.\n        is_leaf (callable, optional): An extra leaf predicate function that will be called at each\n            flattening step. The function should have a single argument with signature\n            ``is_leaf(node) -> bool``. If it returns :data:`True`, the whole subtree being treated\n            as a leaf. Otherwise, the default pytree registry will be used to determine a node is a\n            leaf or not. If the function is not specified, the default pytree registry will be used.\n\n    Returns:\n        A new pytree with the same structure as ``tree`` but with the value at each leaf given by\n        ``func(x, *xs)`` where ``x`` is the value at the corresponding leaf in ``tree`` and ``xs``\n        is the tuple of values at corresponding nodes in ``rests``.\n    ",
      "arguments": [
        "func",
        "tree",
        "rests",
        "is_leaf"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Map a multi-input function over pytree args to produce a new pytree.\n\n    See also :func:`tree_map_`.\n\n    >>> tree_map(lambda x: x + 1, {'x': 7, 'y': (42, 64)})\n    {'x': 8, 'y': (43, 65)}\n    >>> tree_map(lambda x: x is None, {'x': 7, 'y': (42, 64), 'z': None})\n    {'x': False, 'y': (False, False), 'z': True}\n\n    If multiple inputs are given, the structure of the tree is taken from the first input;\n    subsequent inputs need only have ``tree`` as a prefix:\n\n    >>> tree_map(lambda x, y: [x] + y, [5, 6], [[7, 9], [1, 2]])\n    [[5, 7, 9], [6, 1, 2]]\n\n    Args:\n        func (callable): A function that takes ``1 + len(rests)`` arguments, to be applied at the\n            corresponding leaves of the pytrees.\n        tree (pytree): A pytree to be mapped over, with each leaf providing the first positional\n            argument to function ``func``.\n        rests (tuple of pytree): A tuple of pytrees, each of which has the same structure as\n            ``tree`` or has ``tree`` as a prefix.\n        is_leaf (callable, optional): An extra leaf predicate function that will be called at each\n            flattening step. The function should have a single argument with signature\n            ``is_leaf(node) -> bool``. If it returns :data:`True`, the whole subtree being treated\n            as a leaf. Otherwise, the default pytree registry will be used to determine a node is a\n            leaf or not. If the function is not specified, the default pytree registry will be used.\n\n    Returns:\n        A new pytree with the same structure as ``tree`` but with the value at each leaf given by\n        ``func(x, *xs)`` where ``x`` is the value at the corresponding leaf in ``tree`` and ``xs``\n        is the tuple of values at corresponding nodes in ``rests``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.tree_unflatten",
      "signature": "torch._prims.tree_unflatten(leaves: collections.abc.Iterable[typing.Any], treespec: torch.utils._pytree.TreeSpec) -> Any",
      "doc": "Given a list of values and a TreeSpec, builds a pytree.\n    This is the inverse operation of `tree_flatten`.\n    ",
      "arguments": [
        "leaves",
        "treespec"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Given a list of values and a TreeSpec, builds a pytree.\n    This is the inverse operation of `tree_flatten`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims.type_to_dtype",
      "signature": "torch._prims.type_to_dtype(typ: 'type') -> 'torch.dtype'",
      "doc": "\n    Computes the corresponding dtype for a Number type.\n    ",
      "arguments": [
        "typ"
      ],
      "return_type": "torch.dtype",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the corresponding dtype for a Number type.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.NamedTuple",
      "signature": "torch._prims_common.NamedTuple(typename, fields=None, /, **kwargs)",
      "doc": "Typed version of namedtuple.\n\n    Usage::\n\n        class Employee(NamedTuple):\n            name: str\n            id: int\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n    The resulting class has an extra __annotations__ attribute, giving a\n    dict that maps field names to types.  (The field names are also in\n    the _fields attribute, which is part of the namedtuple API.)\n    An alternative equivalent functional syntax is also accepted::\n\n        Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n    ",
      "arguments": [
        "typename",
        "fields",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Typed version of namedtuple.\n\n    Usage::\n\n        class Employee(NamedTuple):\n            name: str\n            id: int\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n    The resulting class has an extra __annotations__ attribute, giving a\n    dict that maps field names to types.  (The field names are also in\n    the _fields attribute, which is part of the namedtuple API.)\n    An alternative equivalent functional syntax is also accepted::\n\n        Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.alert_not_deterministic",
      "signature": "torch._prims_common.alert_not_deterministic(caller: 'str')",
      "doc": "",
      "arguments": [
        "caller"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.apply_perm",
      "signature": "torch._prims_common.apply_perm(inp, perm)",
      "doc": "",
      "arguments": [
        "inp",
        "perm"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.are_strides_like_channels_last",
      "signature": "torch._prims_common.are_strides_like_channels_last(shape: 'Sequence[int]', strides: 'Sequence[int]') -> 'bool'",
      "doc": "",
      "arguments": [
        "shape",
        "strides"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.can_safe_cast_to",
      "signature": "torch._prims_common.can_safe_cast_to(*, cast_to: 'torch.dtype', cast_from: 'torch.dtype') -> 'bool'",
      "doc": "",
      "arguments": [
        "cast_to",
        "cast_from"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.canonicalize_device",
      "signature": "torch._prims_common.canonicalize_device(device: 'DeviceLikeType') -> 'torch.device'",
      "doc": "",
      "arguments": [
        "device"
      ],
      "return_type": "torch.device",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.canonicalize_dim",
      "signature": "torch._prims_common.canonicalize_dim(rank: 'int', idx: 'int', wrap_scalar: 'bool' = True) -> 'int'",
      "doc": "",
      "arguments": [
        "rank",
        "idx",
        "wrap_scalar"
      ],
      "return_type": "int",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.canonicalize_dims",
      "signature": "torch._prims_common.canonicalize_dims(rank, indices, wrap_scalar=True)",
      "doc": "",
      "arguments": [
        "rank",
        "indices",
        "wrap_scalar"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.cast",
      "signature": "torch._prims_common.cast(typ, val)",
      "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
      "arguments": [
        "typ",
        "val"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check",
      "signature": "torch._prims_common.check(b: 'bool', s: 'Callable[[], str]', exc_type: 'type[Exception]' = <class 'RuntimeError'>) -> 'None'",
      "doc": "\n    Helper function for raising an error_type (default: RuntimeError) if a boolean condition fails.\n    Error message is a callable producing a string (to avoid wasting time\n    string formatting in non-error case, and also to make it easier for torchdynamo\n    to trace.)\n\n    .. note:: This function is planned for removal in the future. Please use\n        `torch._check*` functions instead.\n    ",
      "arguments": [
        "b",
        "s",
        "exc_type"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Helper function for raising an error_type (default: RuntimeError) if a boolean condition fails.\n    Error message is a callable producing a string (to avoid wasting time\n    string formatting in non-error case, and also to make it easier for torchdynamo\n    to trace.)\n\n    .. note:: This function is planned for removal in the future. Please use\n        `torch._check*` functions instead.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_all_strides",
      "signature": "torch._prims_common.check_all_strides(a: 'TensorLikeType', b: 'TensorLikeType', *, only_cuda=True) -> 'tuple[bool, Optional[int]]'",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "only_cuda"
      ],
      "return_type": "tuple[bool, Optional[int]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_fp_or_complex",
      "signature": "torch._prims_common.check_fp_or_complex(dtype: 'torch.dtype', fn_name: 'str', allow_low_precision_dtypes: 'bool' = True)",
      "doc": "\n    Checks whether the input is floating point or complex.\n    If allow_low_precision_dtypes is True, it allows having float16, bfloat16, and complex32\n    ",
      "arguments": [
        "dtype",
        "fn_name",
        "allow_low_precision_dtypes"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Checks whether the input is floating point or complex.\n    If allow_low_precision_dtypes is True, it allows having float16, bfloat16, and complex32\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_in_bounds_for_storage",
      "signature": "torch._prims_common.check_in_bounds_for_storage(a: 'torch.TypedStorage', shape: 'ShapeType', strides: 'StrideType', storage_offset: 'int')",
      "doc": "\n    Determines if the given shape, strides, and offset are valid for the given storage.\n    ",
      "arguments": [
        "a",
        "shape",
        "strides",
        "storage_offset"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Determines if the given shape, strides, and offset are valid for the given storage.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_is_matrix",
      "signature": "torch._prims_common.check_is_matrix(A: 'TensorLikeType', f_name: 'str', arg_name: 'str' = 'A')",
      "doc": "",
      "arguments": [
        "A",
        "f_name",
        "arg_name"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_layout",
      "signature": "torch._prims_common.check_layout(layout: 'torch.layout')",
      "doc": "",
      "arguments": [
        "layout"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_pin_memory",
      "signature": "torch._prims_common.check_pin_memory(pin_memory: 'bool')",
      "doc": "",
      "arguments": [
        "pin_memory"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_same_device",
      "signature": "torch._prims_common.check_same_device(*args, allow_cpu_scalar_tensors)",
      "doc": "\n    Checks that all Tensors in args have the same device.\n\n    Raises a RuntimeError when:\n      - args contains an object whose type is not Tensor or Number\n      - two Tensor objects in args have different devices, unless one is a CPU scalar tensor and allow_cpu_scalar_tensors is True\n    ",
      "arguments": [
        "args",
        "allow_cpu_scalar_tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Checks that all Tensors in args have the same device.\n\n    Raises a RuntimeError when:\n      - args contains an object whose type is not Tensor or Number\n      - two Tensor objects in args have different devices, unless one is a CPU scalar tensor and allow_cpu_scalar_tensors is True\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_same_dtype",
      "signature": "torch._prims_common.check_same_dtype(*args)",
      "doc": "\n    Checks that all Tensors in args have the same device and that all Numbers have the\n    same corresponding Python type.\n\n    Raises a RuntimeError when:\n      - args contains an object whose type is not Tensor or Number\n      - two Tensors objects in args have different dtypes\n      - two Number objects in args have different types\n      - there are Tensors and Numbers in args, and one of those Tensors corresponding\n          Python types is different from the type of one of those Numbers\n    ",
      "arguments": [
        "args"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Checks that all Tensors in args have the same device and that all Numbers have the\n    same corresponding Python type.\n\n    Raises a RuntimeError when:\n      - args contains an object whose type is not Tensor or Number\n      - two Tensors objects in args have different dtypes\n      - two Number objects in args have different types\n      - there are Tensors and Numbers in args, and one of those Tensors corresponding\n          Python types is different from the type of one of those Numbers\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_same_shape",
      "signature": "torch._prims_common.check_same_shape(*args, allow_cpu_scalar_tensors: 'bool')",
      "doc": "\n    Checks that all Tensors in args have the same shape.\n\n    Raises a RuntimeError when:\n      - args contains an object whose type is not Tensor or Number\n      - two Tensor objects in args have different devices\n    ",
      "arguments": [
        "args",
        "allow_cpu_scalar_tensors"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Checks that all Tensors in args have the same shape.\n\n    Raises a RuntimeError when:\n      - args contains an object whose type is not Tensor or Number\n      - two Tensor objects in args have different devices\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.check_significant_strides",
      "signature": "torch._prims_common.check_significant_strides(a: 'TensorLikeType', b: 'TensorLikeType', *, only_cuda=True, allow_rhs_unbacked=False) -> 'tuple[bool, Optional[int]]'",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "only_cuda",
        "allow_rhs_unbacked"
      ],
      "return_type": "tuple[bool, Optional[int]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.clone_preserve_strides",
      "signature": "torch._prims_common.clone_preserve_strides(x)",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.compare_tensor_meta",
      "signature": "torch._prims_common.compare_tensor_meta(a: 'TensorLikeType', b: 'TensorLikeType', check_sizes=True, check_strides=False, *, allow_rhs_unbacked=False, check_conj=True)",
      "doc": "\n    Checks that two tensor likes have the same shape,\n    dtype and device.\n\n    In the future this will validate additional metadata, like\n    strides.\n    ",
      "arguments": [
        "a",
        "b",
        "check_sizes",
        "check_strides",
        "allow_rhs_unbacked",
        "check_conj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Checks that two tensor likes have the same shape,\n    dtype and device.\n\n    In the future this will validate additional metadata, like\n    strides.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.compute_elementwise_output_logical_to_physical_perm",
      "signature": "torch._prims_common.compute_elementwise_output_logical_to_physical_perm(*tensors, _skip_checks=False) -> 'list[int]'",
      "doc": "",
      "arguments": [
        "tensors",
        "_skip_checks"
      ],
      "return_type": "list[int]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.compute_elementwise_output_strides",
      "signature": "torch._prims_common.compute_elementwise_output_strides(*tensors) -> 'tuple[int, ...]'",
      "doc": "\n    Computes the output strides for elementwise operations.\n    ",
      "arguments": [
        "tensors"
      ],
      "return_type": "tuple[int, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the output strides for elementwise operations.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.compute_reduction_output_shape",
      "signature": "torch._prims_common.compute_reduction_output_shape(shape: 'ShapeType', dimensions: 'Sequence') -> 'tuple[int, ...]'",
      "doc": "",
      "arguments": [
        "shape",
        "dimensions"
      ],
      "return_type": "tuple[int, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.compute_required_storage_length",
      "signature": "torch._prims_common.compute_required_storage_length(shape: 'ShapeType', strides: 'StrideType', storage_offset: 'int') -> 'int'",
      "doc": "Computes the minimum storage size to hold the given tensor geometry.\n\n    Example\n    =======\n\n    This is the size of a newly allocated tensor's storage, in units of elements\n\n    >>> t = torch.empty((10, 20))\n    >>> compute_required_storage_length(t.shape, t.stride(), t.storage_offset())\n    200\n\n    >>> # xdoctest: +SKIP(failing)\n    >>> t2 = torch.empty_strided((1, 2, 3), (5, 7, 11))\n    >>> size = compute_required_storage_length(t2.shape, t2.stride(), t2.storage_offset())\n    >>> size == t.storage().size()\n    True\n\n    A valid tensor may have a larger storage size, but never smaller\n\n    >>> slice = torch.empty(100)[20:40]\n    >>> slice.storage().size()\n    100\n\n    >>> compute_required_storage_length(slice.shape, slice.stride(), slice.storage_offset())\n    40\n\n    ",
      "arguments": [
        "shape",
        "strides",
        "storage_offset"
      ],
      "return_type": "int",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Computes the minimum storage size to hold the given tensor geometry.\n\n    Example\n    =======\n\n    This is the size of a newly allocated tensor's storage, in units of elements\n\n    >>> t = torch.empty((10, 20))\n    >>> compute_required_storage_length(t.shape, t.stride(), t.storage_offset())\n    200\n\n    >>> # xdoctest: +SKIP(failing)\n    >>> t2 = torch.empty_strided((1, 2, 3), (5, 7, 11))\n    >>> size = compute_required_storage_length(t2.shape, t2.stride(), t2.storage_offset())\n    >>> size == t.storage().size()\n    True\n\n    A valid tensor may have a larger storage size, but never smaller\n\n    >>> slice = torch.empty(100)[20:40]\n    >>> slice.storage().size()\n    100\n\n    >>> compute_required_storage_length(slice.shape, slice.stride(), slice.storage_offset())\n    40\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.corresponding_complex_dtype",
      "signature": "torch._prims_common.corresponding_complex_dtype(dtype: 'torch.dtype') -> 'torch.dtype'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "torch.dtype",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.corresponding_real_dtype",
      "signature": "torch._prims_common.corresponding_real_dtype(dtype: 'torch.dtype') -> 'torch.dtype'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "torch.dtype",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.device_or_default",
      "signature": "torch._prims_common.device_or_default(device: 'Optional[DeviceLikeType]') -> 'DeviceLikeType'",
      "doc": "",
      "arguments": [
        "device"
      ],
      "return_type": "DeviceLikeType",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.dtype_or_default",
      "signature": "torch._prims_common.dtype_or_default(dtype: 'Optional[torch.dtype]') -> 'torch.dtype'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "torch.dtype",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.dtype_to_type",
      "signature": "torch._prims_common.dtype_to_type(dtype: 'torch.dtype') -> 'type'",
      "doc": "\n    Computes the corresponding Python type (AKA \"type kind\") for the\n    given dtype.\n    ",
      "arguments": [
        "dtype"
      ],
      "return_type": "type",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the corresponding Python type (AKA \"type kind\") for the\n    given dtype.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.dtype_to_type_ctor",
      "signature": "torch._prims_common.dtype_to_type_ctor(dtype: 'torch.dtype') -> 'Callable[[NumberType], NumberType]'",
      "doc": "\n    Computes the corresponding Python type constructor for the\n    given dtype.\n    ",
      "arguments": [
        "dtype"
      ],
      "return_type": "Callable[[NumberType], NumberType]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the corresponding Python type constructor for the\n    given dtype.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.elementwise_dtypes",
      "signature": "torch._prims_common.elementwise_dtypes(*_args, type_promotion_kind: 'ELEMENTWISE_TYPE_PROMOTION_KIND') -> 'tuple[torch.dtype, torch.dtype]'",
      "doc": "\n    Computes the computation and result dtypes for elementwise type promotion\n    on the given arguments and with the given elementwise type promotion kind.\n\n    Note that not all inputs to an elementwise operation necessarily participate in type promotion.\n    For example, the \"alpha\" parameter of torch.add does not participate in type promotion,\n    although it may be cast to the Python type corresponding to the computation dtype that\n    the type promotion algorithm determines.\n\n    Default elementwise type promotion, which all other type promotion kinds tweak (see below),\n    first decides which of four ordered types to use:\n\n    bool -> integer -> floating point -> complex\n\n    The selected type is the \"lowest\" type in the above list such that all number arguments\n    have a weakly \"lower\" type and all tensor arguments have a weakly lower corresponding\n    type for their dtype.\n\n    Once the type is determined, the particular result dtype is found. The dtypes are\n    partially ordered as follows:\n\n    bool -> uint8, int8 -> int16 -> int32 -> int64 ->\n      float16, bfloat16 -> float32 -> float64 -> complex32 -> complex64 -> complex128\n\n    The result dtype is selected by:\n      - if no tensor's dtype has the same corresponding type as the one selected,\n          then the result dtype is the (default) dtype corresponding to the selected type\n          (for example, 1.5 + an integer tensor has a result dtype of the default floating point dtype)\n      - if the result type is complex then the dtype is:\n        -  the default complex dtype if there are no floating point or complex tensors\n        -  if there are floating point or complex tensors with one or more dimensions, then\n            the complex dtype corresponding to the highest corresponding complex dtype among those tensors\n            (for example, double + cfloat -> cdouble)\n        -  if there are only floating point or complex tensors with zero dimensions, then\n            the complex dtype corresponding to the highest corresponding complex dtype among those tensors\n      - if the first two cases do not apply, the result dtype is the highest dtype among\n          all tensors with one or more dimensions of the output type, and if there are no such\n          tensors then it's the highest dtype among all tensors with zero dimensions of the output type\n          (for example, long + half -> half, even if the half tensor has zero dimensions)\n\n    The \"corresponding complex dtypes\" are:\n      float16    -> complex32\n      bfloat16   -> complex64\n      float32    -> complex64\n      float64    -> complex128\n      complex32  -> complex32\n      complex64  -> complex64\n      complex128 -> complex128\n\n    The DEFAULT type promotion kind computes per above, and then uses the result dtype to pick a computation\n    dtype by mapping low precision floating point and complex dtypes as follows:\n\n      float16   -> float32\n      bfloat16  -> float32\n      complex32 -> complex64\n\n    This is referred to as \"op math\", and the NO_OPMATH type promotion kind disables this mapping, making the\n    computation dtype the same as the result dtype when it's selected. NO_OPMATH is appropriate for kernels\n    which perform no mathematical operations on their tensors (see below for examples).\n\n    The INT_TO_FLOAT type promotion kind maps boolean and integer result dtypes to the default floating point dtype,\n    and computation dtypes to the appropriate op math dtype.\n\n    The COMPLEX_TO_FLOAT type promotion kind maps complex result dtypes to the corresponding float dtype, following this\n    mapping:\n\n        complex32  -> float16\n        complex64  -> float32\n        complex128 -> float64\n\n    Note that COMPLEX_TO_FLOAT derives the computation dtype as the DEFAULT setting does.\n\n    The BOOL_TO_LONG type promotion kind maps boolean computation and result dtypes to long.\n\n    The ALWAYS_BOOL type promotion kind always sets the result dtype to bool.\n\n    Example operators for each type promotion option:\n      DEFAULT                 : add\n      NO_OPMATH               : where, nextafter, cat\n      INT_TO_FLOAT            : sin\n      COMPLEX_TO_FLOAT        : abs\n      BOOL_TO_LONG            : pow\n      ALWAYS_BOOL             : eq\n\n    ",
      "arguments": [
        "_args",
        "type_promotion_kind"
      ],
      "return_type": "tuple[torch.dtype, torch.dtype]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the computation and result dtypes for elementwise type promotion\n    on the given arguments and with the given elementwise type promotion kind.\n\n    Note that not all inputs to an elementwise operation necessarily participate in type promotion.\n    For example, the \"alpha\" parameter of torch.add does not participate in type promotion,\n    although it may be cast to the Python type corresponding to the computation dtype that\n    the type promotion algorithm determines.\n\n    Default elementwise type promotion, which all other type promotion kinds tweak (see below),\n    first decides which of four ordered types to use:\n\n    bool -> integer -> floating point -> complex\n\n    The selected type is the \"lowest\" type in the above list such that all number arguments\n    have a weakly \"lower\" type and all tensor arguments have a weakly lower corresponding\n    type for their dtype.\n\n    Once the type is determined, the particular result dtype is found. The dtypes are\n    partially ordered as follows:\n\n    bool -> uint8, int8 -> int16 -> int32 -> int64 ->\n      float16, bfloat16 -> float32 -> float64 -> complex32 -> complex64 -> complex128\n\n    The result dtype is selected by:\n      - if no tensor's dtype has the same corresponding type as the one selected,\n          then the result dtype is the (default) dtype corresponding to the selected type\n          (for example, 1.5 + an integer tensor has a result dtype of the default floating point dtype)\n      - if the result type is complex then the dtype is:\n        -  the default complex dtype if there are no floating point or complex tensors\n        -  if there are floating point or complex tensors with one or more dimensions, then\n            the complex dtype corresponding to the highest corresponding complex dtype among those tensors\n            (for example, double + cfloat -> cdouble)\n        -  if there are only floating point or complex tensors with zero dimensions, then\n            the complex dtype corresponding to the highest corresponding complex dtype among those tensors\n      - if the first two cases do not apply, the result dtype is the highest dtype among\n          all tensors with one or more dimensions of the output type, and if there are no such\n          tensors then it's the highest dtype among all tensors with zero dimensions of the output type\n          (for example, long + half -> half, even if the half tensor has zero dimensions)\n\n    The \"corresponding complex dtypes\" are:\n      float16    -> complex32\n      bfloat16   -> complex64\n      float32    -> complex64\n      float64    -> complex128\n      complex32  -> complex32\n      complex64  -> complex64\n      complex128 -> complex128\n\n    The DEFAULT type promotion kind computes per above, and then uses the result dtype to pick a computation\n    dtype by mapping low precision floating point and complex dtypes as follows:\n\n      float16   -> float32\n      bfloat16  -> float32\n      complex32 -> complex64\n\n    This is referred to as \"op math\", and the NO_OPMATH type promotion kind disables this mapping, making the\n    computation dtype the same as the result dtype when it's selected. NO_OPMATH is appropriate for kernels\n    which perform no mathematical operations on their tensors (see below for examples).\n\n    The INT_TO_FLOAT type promotion kind maps boolean and integer result dtypes to the default floating point dtype,\n    and computation dtypes to the appropriate op math dtype.\n\n    The COMPLEX_TO_FLOAT type promotion kind maps complex result dtypes to the corresponding float dtype, following this\n    mapping:\n\n        complex32  -> float16\n        complex64  -> float32\n        complex128 -> float64\n\n    Note that COMPLEX_TO_FLOAT derives the computation dtype as the DEFAULT setting does.\n\n    The BOOL_TO_LONG type promotion kind maps boolean computation and result dtypes to long.\n\n    The ALWAYS_BOOL type promotion kind always sets the result dtype to bool.\n\n    Example operators for each type promotion option:\n      DEFAULT                 : add\n      NO_OPMATH               : where, nextafter, cat\n      INT_TO_FLOAT            : sin\n      COMPLEX_TO_FLOAT        : abs\n      BOOL_TO_LONG            : pow\n      ALWAYS_BOOL             : eq\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.expr_type",
      "signature": "torch._prims_common.expr_type(x: 'sympy.Basic') -> 'type'",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "type",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.extract_dims_from_varargs",
      "signature": "torch._prims_common.extract_dims_from_varargs(dims: 'Union[DimsSequenceType, tuple[DimsSequenceType, ...]]') -> 'DimsSequenceType'",
      "doc": "",
      "arguments": [
        "dims"
      ],
      "return_type": "DimsSequenceType",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.extract_shape",
      "signature": "torch._prims_common.extract_shape(*args, allow_cpu_scalar_tensors: 'bool') -> 'Optional[ShapeType]'",
      "doc": "",
      "arguments": [
        "args",
        "allow_cpu_scalar_tensors"
      ],
      "return_type": "Optional[ShapeType]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.extract_shape_from_varargs",
      "signature": "torch._prims_common.extract_shape_from_varargs(shape: 'Union[ShapeType, tuple[ShapeType]]', validate=True) -> 'tuple[int, ...]'",
      "doc": "\n    Returns a shape from varargs.\n\n    In PyTorch, operations that accept shapes often accept them as varargs, like\n    foo(*shape). However a user can pass the shape as a sequence of integers,\n    like this:\n\n      foo(1, 2, 3)\n\n    or as a sequence of integers\n\n      foo((1, 2, 3))\n\n    In the first case shape will be a tuple of integers, and in the second case it's a tuple\n    containing a tuple of integers. This validates those inputs and canonicalizes them\n    to a tuple of integers.\n    ",
      "arguments": [
        "shape",
        "validate"
      ],
      "return_type": "tuple[int, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns a shape from varargs.\n\n    In PyTorch, operations that accept shapes often accept them as varargs, like\n    foo(*shape). However a user can pass the shape as a sequence of integers,\n    like this:\n\n      foo(1, 2, 3)\n\n    or as a sequence of integers\n\n      foo((1, 2, 3))\n\n    In the first case shape will be a tuple of integers, and in the second case it's a tuple\n    containing a tuple of integers. This validates those inputs and canonicalizes them\n    to a tuple of integers.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.get_acc_type",
      "signature": "torch._prims_common.get_acc_type(dtype: 'torch.dtype', device: 'torch.device') -> 'torch.dtype'",
      "doc": "",
      "arguments": [
        "dtype",
        "device"
      ],
      "return_type": "torch.dtype",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.get_aten_op",
      "signature": "torch._prims_common.get_aten_op(fn: 'Callable', name: 'str')",
      "doc": "\n    Given the __module__ of reference and its name, it returns\n    (our best guess of) the ATen name of the associated operation\n\n    Note: In ATen, the __name__ of a function within a module often\n    starts by the module name. E.g. linalg_eigh, or special_zeta\n    ",
      "arguments": [
        "fn",
        "name"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Given the __module__ of reference and its name, it returns\n    (our best guess of) the ATen name of the associated operation\n\n    Note: In ATen, the __name__ of a function within a module often\n    starts by the module name. E.g. linalg_eigh, or special_zeta\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.get_computation_dtype",
      "signature": "torch._prims_common.get_computation_dtype(dtype: 'torch.dtype') -> 'torch.dtype'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "torch.dtype",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.get_dtype",
      "signature": "torch._prims_common.get_dtype(x: 'Union[torch.Tensor, NumberType]')",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.get_higher_dtype",
      "signature": "torch._prims_common.get_higher_dtype(a: 'Optional[Union[torch.dtype, TensorLikeType, NumberType]]', b: 'Optional[Union[torch.dtype, TensorLikeType, NumberType]]') -> 'Optional[torch.dtype]'",
      "doc": "\n    Computes the \"lowest\" datatype that is weakly\n    \"higher\" than both a and b.\n    ",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "Optional[torch.dtype]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the \"lowest\" datatype that is weakly\n    \"higher\" than both a and b.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.get_higher_type",
      "signature": "torch._prims_common.get_higher_type(a: 'type', b: 'type') -> 'type'",
      "doc": "\n    Returns the higher of the two given Number types.\n\n    The types are ordered bool -> int -> float -> complex.\n    ",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "type",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns the higher of the two given Number types.\n\n    The types are ordered bool -> int -> float -> complex.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.infer_size",
      "signature": "torch._prims_common.infer_size(shape: 'ShapeType', numel: 'int') -> 'tuple[int, ...]'",
      "doc": "\n    Infers the size of a dim with size -1, if it exists.\n    Also checks that new shape is compatible with the number of elements.\n    ",
      "arguments": [
        "shape",
        "numel"
      ],
      "return_type": "tuple[int, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Infers the size of a dim with size -1, if it exists.\n    Also checks that new shape is compatible with the number of elements.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.infer_size_shapes",
      "signature": "torch._prims_common.infer_size_shapes(a: 'ShapeType', b: 'ShapeType') -> 'tuple[int, ...]'",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "tuple[int, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.invert_perm",
      "signature": "torch._prims_common.invert_perm(perm)",
      "doc": "",
      "arguments": [
        "perm"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_boolean_dtype",
      "signature": "torch._prims_common.is_boolean_dtype(dtype: 'torch.dtype') -> 'bool'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_channels_last_contiguous",
      "signature": "torch._prims_common.is_channels_last_contiguous(a: 'Tensor') -> 'bool'",
      "doc": "\n    True when a tensor is channels-last contiguous.\n\n    This requires that:\n\n      - the tensor is conceptually either 4 (NHWC) or 5 (NDHWC) dimensions\n      - if we name the tensor's dimensions NCHW or NCDHW, then the strides are such that the\n        stride of the 'C' dimension (Cs) is 1 and the strides corresponding to\n        each dimension (Xs) can be ordered Cs <= Ws <= Hs <= (Ds) <= Ns and are\n        \"nested\" -- so Ws = Cs * Cl, where Cl is the length of the 'C' dimension,\n        for example.\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    True when a tensor is channels-last contiguous.\n\n    This requires that:\n\n      - the tensor is conceptually either 4 (NHWC) or 5 (NDHWC) dimensions\n      - if we name the tensor's dimensions NCHW or NCDHW, then the strides are such that the\n        stride of the 'C' dimension (Cs) is 1 and the strides corresponding to\n        each dimension (Xs) can be ordered Cs <= Ws <= Hs <= (Ds) <= Ns and are\n        \"nested\" -- so Ws = Cs * Cl, where Cl is the length of the 'C' dimension,\n        for example.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_channels_last_contiguous_2d",
      "signature": "torch._prims_common.is_channels_last_contiguous_2d(a: 'Tensor') -> 'bool'",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_channels_last_contiguous_3d",
      "signature": "torch._prims_common.is_channels_last_contiguous_3d(a: 'Tensor') -> 'bool'",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_complex_dtype",
      "signature": "torch._prims_common.is_complex_dtype(dtype: 'torch.dtype') -> 'bool'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_contiguous",
      "signature": "torch._prims_common.is_contiguous(a: 'TensorLikeType') -> 'bool'",
      "doc": "\n    Tests whether a tensor is contiguous or not.\n\n    Tensors are contiguous when they have no elements,\n    one element, or when they have \"nested\" strides.\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Tests whether a tensor is contiguous or not.\n\n    Tensors are contiguous when they have no elements,\n    one element, or when they have \"nested\" strides.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_contiguous_for_memory_format",
      "signature": "torch._prims_common.is_contiguous_for_memory_format(a: 'Tensor', *, memory_format: 'torch.memory_format') -> 'bool'",
      "doc": "",
      "arguments": [
        "a",
        "memory_format"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_cpu_scalar_tensor",
      "signature": "torch._prims_common.is_cpu_scalar_tensor(a: 'Any') -> 'bool'",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_expandable_to",
      "signature": "torch._prims_common.is_expandable_to(shape: 'ShapeType', desired: 'ShapeType') -> 'bool'",
      "doc": "Checks if a shape can be expanded to another shape.\n    This is equivalent to checking if the two shapes are broadcastable.\n    ",
      "arguments": [
        "shape",
        "desired"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Checks if a shape can be expanded to another shape.\n    This is equivalent to checking if the two shapes are broadcastable.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_float_dtype",
      "signature": "torch._prims_common.is_float_dtype(dtype: 'torch.dtype') -> 'bool'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_grad_dtype",
      "signature": "torch._prims_common.is_grad_dtype(dtype: 'torch.dtype') -> 'bool'",
      "doc": "\n    Checks if the dtype can require a gradient.\n    ",
      "arguments": [
        "dtype"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Checks if the dtype can require a gradient.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_integer_dtype",
      "signature": "torch._prims_common.is_integer_dtype(dtype: 'torch.dtype') -> 'bool'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_low_precision_dtype",
      "signature": "torch._prims_common.is_low_precision_dtype(dtype: 'torch.dtype') -> 'bool'",
      "doc": "",
      "arguments": [
        "dtype"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_non_overlapping_and_dense",
      "signature": "torch._prims_common.is_non_overlapping_and_dense(a: 'Tensor') -> 'bool'",
      "doc": "\n    True when a tensor is non-overlapping and dense.\n\n    A tensor is non-overlapping and dense when there exists a permutation of\n    its dimensions that is contiguous.\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    True when a tensor is non-overlapping and dense.\n\n    A tensor is non-overlapping and dense when there exists a permutation of\n    its dimensions that is contiguous.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_same_shape",
      "signature": "torch._prims_common.is_same_shape(a: 'Sequence', b: 'Sequence') -> 'bool'",
      "doc": "\n    Compares two shapes a and b, returning True if they are the same\n    (their ranks and corresponding lengths match) and False otherwise.\n    ",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Compares two shapes a and b, returning True if they are the same\n    (their ranks and corresponding lengths match) and False otherwise.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_valid_permutation",
      "signature": "torch._prims_common.is_valid_permutation(rank: 'int', perm: 'DimsSequenceType') -> 'bool'",
      "doc": "\n    Validates that perm is a permutation of length rank.\n    ",
      "arguments": [
        "rank",
        "perm"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Validates that perm is a permutation of length rank.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.is_weakly_lesser_type",
      "signature": "torch._prims_common.is_weakly_lesser_type(a: 'type', b: 'type') -> 'bool'",
      "doc": "\n    Compares two types, a and b, returning True if a is weakly \"less\" than b.\n\n    The comparison is determined by the following type ordering: bool, int, float, complex.\n    ",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Compares two types, a and b, returning True if a is weakly \"less\" than b.\n\n    The comparison is determined by the following type ordering: bool, int, float, complex.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.layout_or_default",
      "signature": "torch._prims_common.layout_or_default(layout: 'Optional[torch.layout]') -> 'torch.layout'",
      "doc": "",
      "arguments": [
        "layout"
      ],
      "return_type": "torch.layout",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.make_channels_last_1d_strides_for",
      "signature": "torch._prims_common.make_channels_last_1d_strides_for(shape: 'Sequence[_IntLikeT]') -> 'tuple[Union[_IntLikeT, int], ...]'",
      "doc": "",
      "arguments": [
        "shape"
      ],
      "return_type": "tuple[Union[_IntLikeT, int], ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.make_channels_last_2d_strides_for",
      "signature": "torch._prims_common.make_channels_last_2d_strides_for(shape: 'Sequence[_IntLikeT]') -> 'tuple[Union[_IntLikeT, int], ...]'",
      "doc": "",
      "arguments": [
        "shape"
      ],
      "return_type": "tuple[Union[_IntLikeT, int], ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.make_channels_last_3d_strides_for",
      "signature": "torch._prims_common.make_channels_last_3d_strides_for(shape: 'Sequence[_IntLikeT]') -> 'tuple[Union[_IntLikeT, int], ...]'",
      "doc": "",
      "arguments": [
        "shape"
      ],
      "return_type": "tuple[Union[_IntLikeT, int], ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.make_channels_last_strides_for",
      "signature": "torch._prims_common.make_channels_last_strides_for(shape: 'Sequence[_IntLikeT]') -> 'tuple[Union[_IntLikeT, int], ...]'",
      "doc": "",
      "arguments": [
        "shape"
      ],
      "return_type": "tuple[Union[_IntLikeT, int], ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.make_contiguous_strides_for",
      "signature": "torch._prims_common.make_contiguous_strides_for(shape: 'ShapeType', row_major: 'bool' = True) -> 'tuple[Union[_IntLikeT, int], ...]'",
      "doc": "\n    Returns the strides of a contiguous tensor if row_major\n    If row_major=True, it returns the strides of a contiguous batch of Fortran-contiguous matrices\n    This is often used when calling external libraries like BLAS/LAPACK/cuSolver...\n    ",
      "arguments": [
        "shape",
        "row_major"
      ],
      "return_type": "tuple[Union[_IntLikeT, int], ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns the strides of a contiguous tensor if row_major\n    If row_major=True, it returns the strides of a contiguous batch of Fortran-contiguous matrices\n    This is often used when calling external libraries like BLAS/LAPACK/cuSolver...\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.mask_tensor",
      "signature": "torch._prims_common.mask_tensor(mask: 'TensorLikeType', t: 'TensorLikeType')",
      "doc": "\n    Similar to torch.where(mask, t, 0) but if t is boolean,\n    result is also boolean and not promoted to int.\n    ",
      "arguments": [
        "mask",
        "t"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Similar to torch.where(mask, t, 0) but if t is boolean,\n    result is also boolean and not promoted to int.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.number_type",
      "signature": "torch._prims_common.number_type(x: 'Union[NumberType, torch.SymInt, torch.SymFloat, torch.SymBool]') -> 'type'",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "type",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.overload",
      "signature": "torch._prims_common.overload(func)",
      "doc": "Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.\n\n    For example::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            ...  # implementation goes here\n\n    The overloads for a function can be retrieved at runtime using the\n    get_overloads() function.\n    ",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.\n\n    For example::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            ...  # implementation goes here\n\n    The overloads for a function can be retrieved at runtime using the\n    get_overloads() function.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.prod",
      "signature": "torch._prims_common.prod(xs: 'Sequence[NumberType]') -> 'NumberType'",
      "doc": "Product of elements in input sequence. Returns 1 for empty sequence",
      "arguments": [
        "xs"
      ],
      "return_type": "NumberType",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Product of elements in input sequence. Returns 1 for empty sequence",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.reduction_dims",
      "signature": "torch._prims_common.reduction_dims(shape: 'ShapeType', dims: 'Optional[Sequence]') -> 'tuple[int, ...]'",
      "doc": "",
      "arguments": [
        "shape",
        "dims"
      ],
      "return_type": "tuple[int, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.reduction_dtypes",
      "signature": "torch._prims_common.reduction_dtypes(arg, output_dtype_kind: 'REDUCTION_OUTPUT_TYPE_KIND', dtype: 'Optional[torch.dtype]' = None) -> 'tuple[torch.dtype, Optional[torch.dtype]]'",
      "doc": "",
      "arguments": [
        "arg",
        "output_dtype_kind",
        "dtype"
      ],
      "return_type": "tuple[torch.dtype, Optional[torch.dtype]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.same_shape",
      "signature": "torch._prims_common.same_shape(a: 'ShapeType', b: 'ShapeType', *, allow_rhs_unbacked=False) -> 'bool'",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "allow_rhs_unbacked"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.set_correction",
      "signature": "torch._prims_common.set_correction(unbiased: 'Optional[bool]' = None, correction: 'Optional[NumberType]' = None) -> 'float'",
      "doc": "",
      "arguments": [
        "unbiased",
        "correction"
      ],
      "return_type": "float",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.suggest_memory_format",
      "signature": "torch._prims_common.suggest_memory_format(x: 'TensorLikeType') -> 'torch.memory_format'",
      "doc": "",
      "arguments": [
        "x"
      ],
      "return_type": "torch.memory_format",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.sym_float",
      "signature": "torch._prims_common.sym_float(a)",
      "doc": "SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.sym_int",
      "signature": "torch._prims_common.sym_int(a)",
      "doc": "SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.sym_max",
      "signature": "torch._prims_common.sym_max(a, b)",
      "doc": "\n    SymInt-aware utility for max which avoids branching on a < b.\n    Unlike builtins.max(), this only works for int/float, and it always\n    promotes to float if any argument is float (unlike builtins.max, which\n    will faithfully preserve the type of the input argument).\n    ",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    SymInt-aware utility for max which avoids branching on a < b.\n    Unlike builtins.max(), this only works for int/float, and it always\n    promotes to float if any argument is float (unlike builtins.max, which\n    will faithfully preserve the type of the input argument).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.type_to_dtype",
      "signature": "torch._prims_common.type_to_dtype(typ: 'type') -> 'torch.dtype'",
      "doc": "\n    Computes the corresponding dtype for a Number type.\n    ",
      "arguments": [
        "typ"
      ],
      "return_type": "torch.dtype",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the corresponding dtype for a Number type.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.validate_dim_length",
      "signature": "torch._prims_common.validate_dim_length(length: 'int')",
      "doc": "\n    Validates that an object represents a valid\n    dimension length.\n    ",
      "arguments": [
        "length"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Validates that an object represents a valid\n    dimension length.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.validate_dimension_indices",
      "signature": "torch._prims_common.validate_dimension_indices(rank: 'int', indices: 'DimsSequenceType')",
      "doc": "",
      "arguments": [
        "rank",
        "indices"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.validate_exclusive_idx",
      "signature": "torch._prims_common.validate_exclusive_idx(rank: 'int', ex_idx: 'int')",
      "doc": "\n    Validates that ex_idx is a valid exclusive index\n    for the given shape.\n    ",
      "arguments": [
        "rank",
        "ex_idx"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Validates that ex_idx is a valid exclusive index\n    for the given shape.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.validate_idx",
      "signature": "torch._prims_common.validate_idx(rank: 'int', idx: 'int')",
      "doc": "\n    Validates that idx is a valid index for the given shape.\n    Assumes the index is already canonicalized.\n    ",
      "arguments": [
        "rank",
        "idx"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Validates that idx is a valid index for the given shape.\n    Assumes the index is already canonicalized.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.validate_memory_format",
      "signature": "torch._prims_common.validate_memory_format(memory_format: 'torch.memory_format')",
      "doc": "",
      "arguments": [
        "memory_format"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.validate_no_repeating_dims",
      "signature": "torch._prims_common.validate_no_repeating_dims(dims: 'Sequence')",
      "doc": "",
      "arguments": [
        "dims"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.validate_shape",
      "signature": "torch._prims_common.validate_shape(shape: 'ShapeType')",
      "doc": "\n    Validates that a sequence represents a valid shape.\n    ",
      "arguments": [
        "shape"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Validates that a sequence represents a valid shape.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._prims_common.validate_strides",
      "signature": "torch._prims_common.validate_strides(strides: 'StrideType')",
      "doc": "\n    Verifies the object specifies valid strides.\n    ",
      "arguments": [
        "strides"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Verifies the object specifies valid strides.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.T",
      "signature": "torch._refs.T(a: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.abs",
      "signature": "torch._refs.abs(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.abs_",
      "signature": "torch._refs.abs_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.acos",
      "signature": "torch._refs.acos(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.acos_",
      "signature": "torch._refs.acos_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.acosh",
      "signature": "torch._refs.acosh(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.acosh_",
      "signature": "torch._refs.acosh_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.add",
      "signature": "torch._refs.add(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, alpha: Union[bool, int, float, complex, NoneType] = None, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.add\n    ",
      "arguments": [
        "a",
        "b",
        "alpha",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.add\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.add_",
      "signature": "torch._refs.add_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, alpha: Union[bool, int, float, complex, NoneType] = None, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.add\n    ",
      "arguments": [
        "a",
        "b",
        "alpha",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.add\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.addcdiv",
      "signature": "torch._refs.addcdiv(self: torch.Tensor, tensor1: torch.Tensor, tensor2: torch.Tensor, *, value: Union[bool, int, float, complex] = 1, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.addcdiv\n    ",
      "arguments": [
        "self",
        "tensor1",
        "tensor2",
        "value",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.addcdiv\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.addcdiv_",
      "signature": "torch._refs.addcdiv_(self: torch.Tensor, tensor1: torch.Tensor, tensor2: torch.Tensor, *, value: Union[bool, int, float, complex] = 1, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.addcdiv\n    ",
      "arguments": [
        "self",
        "tensor1",
        "tensor2",
        "value",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.addcdiv\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.addcmul",
      "signature": "torch._refs.addcmul(self: torch.Tensor, tensor1: torch.Tensor, tensor2: torch.Tensor, *, value: Union[bool, int, float, complex] = 1, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.addcmul\n    ",
      "arguments": [
        "self",
        "tensor1",
        "tensor2",
        "value",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.addcmul\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.addcmul_",
      "signature": "torch._refs.addcmul_(self: torch.Tensor, tensor1: torch.Tensor, tensor2: torch.Tensor, *, value: Union[bool, int, float, complex] = 1, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.addcmul\n    ",
      "arguments": [
        "self",
        "tensor1",
        "tensor2",
        "value",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.addcmul\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.addr",
      "signature": "torch._refs.addr(self: torch.Tensor, vec1: torch.Tensor, vec2: torch.Tensor, *, beta: Union[bool, int, float, complex] = 1, alpha: Union[bool, int, float, complex] = 1, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "vec1",
        "vec2",
        "beta",
        "alpha",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.alias",
      "signature": "torch._refs.alias(a: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.alias_copy",
      "signature": "torch._refs.alias_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.all",
      "signature": "torch._refs.all(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, keepdim: bool = False, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "keepdim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.allclose",
      "signature": "torch._refs.allclose(a: torch.Tensor, b: torch.Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> bool",
      "doc": "\n    Reference implementation of torch.allclose\n    ",
      "arguments": [
        "a",
        "b",
        "rtol",
        "atol",
        "equal_nan"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.allclose\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.amax",
      "signature": "torch._refs.amax(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, keepdim: bool = False, *, out: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "keepdim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.amin",
      "signature": "torch._refs.amin(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, keepdim: bool = False, *, out: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "keepdim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.any",
      "signature": "torch._refs.any(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, keepdim: bool = False, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "keepdim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.arange",
      "signature": "torch._refs.arange(start: Union[bool, int, float, complex] = 0, end: Union[bool, int, float, complex, NoneType] = None, step: Union[bool, int, float, complex] = 1, *, dtype: Optional[torch.dtype] = None, layout: torch.layout = torch.strided, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "start",
        "end",
        "step",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.as_strided",
      "signature": "torch._refs.as_strided(a: torch.Tensor, size: Union[torch.Size, list[int], tuple[int, ...]], stride: Union[list[int], tuple[int, ...]], storage_offset: Optional[int] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "size",
        "stride",
        "storage_offset"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.as_strided_copy",
      "signature": "torch._refs.as_strided_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.as_strided_scatter",
      "signature": "torch._refs.as_strided_scatter(input: torch.Tensor, src: torch.Tensor, size: Union[torch.Size, list[int], tuple[int, ...]], stride: Union[list[int], tuple[int, ...]], storage_offset: Optional[int] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "src",
        "size",
        "stride",
        "storage_offset",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.asin",
      "signature": "torch._refs.asin(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.asin_",
      "signature": "torch._refs.asin_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.asinh",
      "signature": "torch._refs.asinh(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.asinh_",
      "signature": "torch._refs.asinh_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atan",
      "signature": "torch._refs.atan(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atan2",
      "signature": "torch._refs.atan2(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atan2_",
      "signature": "torch._refs.atan2_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atan_",
      "signature": "torch._refs.atan_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atanh",
      "signature": "torch._refs.atanh(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atanh_",
      "signature": "torch._refs.atanh_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atleast_1d",
      "signature": "torch._refs.atleast_1d(arg: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor]], *args: torch.Tensor) -> Union[torch.Tensor, tuple[torch.Tensor, ...]]",
      "doc": "Reference implementation of :func:`torch.atleast_1d`.",
      "arguments": [
        "arg",
        "args"
      ],
      "return_type": "typing.Union[torch.Tensor, tuple[torch.Tensor, ...]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reference implementation of :func:`torch.atleast_1d`.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atleast_2d",
      "signature": "torch._refs.atleast_2d(arg: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor]], *args: torch.Tensor) -> Union[torch.Tensor, tuple[torch.Tensor, ...]]",
      "doc": "Reference implementation of :func:`torch.atleast_2d`.",
      "arguments": [
        "arg",
        "args"
      ],
      "return_type": "typing.Union[torch.Tensor, tuple[torch.Tensor, ...]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reference implementation of :func:`torch.atleast_2d`.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.atleast_3d",
      "signature": "torch._refs.atleast_3d(arg: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor]], *args: torch.Tensor) -> Union[torch.Tensor, tuple[torch.Tensor, ...]]",
      "doc": "Reference implementation of :func:`torch.atleast_3d`.",
      "arguments": [
        "arg",
        "args"
      ],
      "return_type": "typing.Union[torch.Tensor, tuple[torch.Tensor, ...]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reference implementation of :func:`torch.atleast_3d`.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_and",
      "signature": "torch._refs.bitwise_and(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_and_",
      "signature": "torch._refs.bitwise_and_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_left_shift",
      "signature": "torch._refs.bitwise_left_shift(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_left_shift_",
      "signature": "torch._refs.bitwise_left_shift_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_not",
      "signature": "torch._refs.bitwise_not(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_not_",
      "signature": "torch._refs.bitwise_not_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_or",
      "signature": "torch._refs.bitwise_or(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_or_",
      "signature": "torch._refs.bitwise_or_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_right_shift",
      "signature": "torch._refs.bitwise_right_shift(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_right_shift_",
      "signature": "torch._refs.bitwise_right_shift_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_xor",
      "signature": "torch._refs.bitwise_xor(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bitwise_xor_",
      "signature": "torch._refs.bitwise_xor_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.block_diag",
      "signature": "torch._refs.block_diag(*tensors: list[torch.Tensor]) -> torch.Tensor",
      "doc": "\n    This is used as an input to PythonRefInfo. `torch.block_diag`\n    expects arguments splatted, but `aten.block_diag` expects only\n    one argument that is a list of Tensors.\n    ",
      "arguments": [
        "tensors"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This is used as an input to PythonRefInfo. `torch.block_diag`\n    expects arguments splatted, but `aten.block_diag` expects only\n    one argument that is a list of Tensors.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.broadcast_shapes",
      "signature": "torch._refs.broadcast_shapes(*shapes) -> Union[torch.Size, list[int], tuple[int, ...]]",
      "doc": "",
      "arguments": [
        "shapes"
      ],
      "return_type": "typing.Union[torch.Size, list[int], tuple[int, ...]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.broadcast_tensors",
      "signature": "torch._refs.broadcast_tensors(*tensors) -> list[torch.Tensor]",
      "doc": "",
      "arguments": [
        "tensors"
      ],
      "return_type": "list[torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.broadcast_to",
      "signature": "torch._refs.broadcast_to(a: torch.Tensor, size: Union[torch.Size, list[int], tuple[int, ...]]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "size"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.bucketize",
      "signature": "torch._refs.bucketize(a: Union[torch.Tensor, bool, int, float, complex], boundaries: torch.Tensor, *, out_int32: bool = False, right: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "boundaries",
        "out_int32",
        "right",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cast",
      "signature": "torch._refs.cast(typ, val)",
      "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
      "arguments": [
        "typ",
        "val"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cat",
      "signature": "torch._refs.cat(tensors: Union[list[torch.Tensor], tuple[torch.Tensor, ...]], dim: int = 0, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "tensors",
        "dim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cauchy",
      "signature": "torch._refs.cauchy(self, median=0, sigma=1, generator=None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "median",
        "sigma",
        "generator",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cauchy_",
      "signature": "torch._refs.cauchy_(self, median=0, sigma=1, generator=None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "median",
        "sigma",
        "generator",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ceil",
      "signature": "torch._refs.ceil(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ceil_",
      "signature": "torch._refs.ceil_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.chunk",
      "signature": "torch._refs.chunk(a: torch.Tensor, chunks: int, dim: int = 0) -> tuple[torch.Tensor, ...]",
      "doc": "",
      "arguments": [
        "a",
        "chunks",
        "dim"
      ],
      "return_type": "tuple[torch.Tensor, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.clamp",
      "signature": "torch._refs.clamp(a: torch.Tensor, min: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, max: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "min",
        "max",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.clamp_",
      "signature": "torch._refs.clamp_(a: torch.Tensor, min: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, max: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "min",
        "max",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.clamp_max",
      "signature": "torch._refs.clamp_max(self: torch.Tensor, max: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "max",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.clamp_max_",
      "signature": "torch._refs.clamp_max_(self: torch.Tensor, max: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "max",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.clamp_min",
      "signature": "torch._refs.clamp_min(self: torch.Tensor, min: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "min",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.clamp_min_",
      "signature": "torch._refs.clamp_min_(self: torch.Tensor, min: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "min",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.clone",
      "signature": "torch._refs.clone(a: torch.Tensor, *, memory_format: torch.memory_format = torch.preserve_format, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "memory_format",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.column_stack",
      "signature": "torch._refs.column_stack(tensors: Union[list[torch.Tensor], tuple[torch.Tensor, ...]], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "tensors",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.conj",
      "signature": "torch._refs.conj(input: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.conj_physical",
      "signature": "torch._refs.conj_physical(input: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.conj_physical_",
      "signature": "torch._refs.conj_physical_(input: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.constant_pad_nd",
      "signature": "torch._refs.constant_pad_nd(input: torch.Tensor, pad: list[int], value: Union[bool, int, float, complex] = 0, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "pad",
        "value",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.contiguous",
      "signature": "torch._refs.contiguous(a: torch.Tensor, *, memory_format: torch.memory_format = torch.contiguous_format) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "memory_format"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.copy_to",
      "signature": "torch._refs.copy_to(a: torch.Tensor, b: torch.Tensor, *, allow_cross_device=True)",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "allow_cross_device"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.copysign",
      "signature": "torch._refs.copysign(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.copysign_",
      "signature": "torch._refs.copysign_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cos",
      "signature": "torch._refs.cos(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cos_",
      "signature": "torch._refs.cos_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cosh",
      "signature": "torch._refs.cosh(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cosh_",
      "signature": "torch._refs.cosh_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.count_nonzero",
      "signature": "torch._refs.count_nonzero(self, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "dim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cumprod",
      "signature": "torch._refs.cumprod(a: torch.Tensor, dim: int, *, dtype: Optional[torch.dtype] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cumprod_",
      "signature": "torch._refs.cumprod_(a: torch.Tensor, dim: int, *, dtype: Optional[torch.dtype] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cumsum",
      "signature": "torch._refs.cumsum(a: torch.Tensor, dim: int, *, dtype: Optional[torch.dtype] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.cumsum_",
      "signature": "torch._refs.cumsum_(a: torch.Tensor, dim: int, *, dtype: Optional[torch.dtype] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.deg2rad",
      "signature": "torch._refs.deg2rad(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.deg2rad_",
      "signature": "torch._refs.deg2rad_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.diag",
      "signature": "torch._refs.diag(self: torch.Tensor, offset: int = 0, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "offset",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.diag_embed",
      "signature": "torch._refs.diag_embed(t: torch.Tensor, offset: int = 0, dim1: int = -2, dim2: int = -1, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.diag_embed\n    ",
      "arguments": [
        "t",
        "offset",
        "dim1",
        "dim2",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.diag_embed\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.diagonal",
      "signature": "torch._refs.diagonal(self: torch.Tensor, offset: int = 0, dim1: int = 0, dim2: int = 1) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.diagonal\n    ",
      "arguments": [
        "self",
        "offset",
        "dim1",
        "dim2"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.diagonal\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.diagonal_copy",
      "signature": "torch._refs.diagonal_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.diagonal_scatter",
      "signature": "torch._refs.diagonal_scatter(input: torch.Tensor, src: torch.Tensor, offset: int = 0, dim1: int = 0, dim2: int = 1, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "src",
        "offset",
        "dim1",
        "dim2",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.digamma",
      "signature": "torch._refs.digamma(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.digamma_",
      "signature": "torch._refs.digamma_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.div",
      "signature": "torch._refs.div(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, rounding_mode: Optional[str] = None, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.div\n    ",
      "arguments": [
        "a",
        "b",
        "rounding_mode",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.div\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.div_",
      "signature": "torch._refs.div_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, rounding_mode: Optional[str] = None, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.div\n    ",
      "arguments": [
        "a",
        "b",
        "rounding_mode",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.div\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.dot",
      "signature": "torch._refs.dot(self, other, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "other",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.dsplit",
      "signature": "torch._refs.dsplit(a: torch.Tensor, sections: Union[int, list[int], tuple[int, ...]]) -> Union[list[torch.Tensor], tuple[torch.Tensor, ...]]",
      "doc": "",
      "arguments": [
        "a",
        "sections"
      ],
      "return_type": "typing.Union[list[torch.Tensor], tuple[torch.Tensor, ...]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.dstack",
      "signature": "torch._refs.dstack(tensors: Union[list[torch.Tensor], tuple[torch.Tensor, ...]], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "tensors",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.dtype_to_type",
      "signature": "torch._refs.dtype_to_type(dtype: 'torch.dtype') -> 'type'",
      "doc": "\n    Computes the corresponding Python type (AKA \"type kind\") for the\n    given dtype.\n    ",
      "arguments": [
        "dtype"
      ],
      "return_type": "type",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the corresponding Python type (AKA \"type kind\") for the\n    given dtype.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.elementwise_unary_scalar_wrapper",
      "signature": "torch._refs.elementwise_unary_scalar_wrapper(fn: Callable[~_P, ~_T]) -> Callable[~_P, Union[~_T, bool, int, float, complex]]",
      "doc": "\n    Allows unary operators that accept tensors to work with Python numbers.\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "typing.Callable[~_P, typing.Union[~_T, bool, int, float, complex]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Allows unary operators that accept tensors to work with Python numbers.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.empty",
      "signature": "torch._refs.empty(*shape, dtype: Optional[torch.dtype] = None, layout: torch.layout = torch.strided, device: Union[str, torch.device, int, NoneType] = None, requires_grad: bool = False, pin_memory: bool = False, memory_format: torch.memory_format = torch.contiguous_format, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "shape",
        "dtype",
        "layout",
        "device",
        "requires_grad",
        "pin_memory",
        "memory_format",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.empty_like",
      "signature": "torch._refs.empty_like(a: torch.Tensor, *, dtype: Optional[torch.dtype] = None, device: Union[str, torch.device, int, NoneType] = None, layout: Optional[torch.layout] = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = torch.preserve_format, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dtype",
        "device",
        "layout",
        "pin_memory",
        "requires_grad",
        "memory_format",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.empty_out",
      "signature": "torch._refs.empty_out(size: torch.Tensor, out: torch.Tensor, memory_format: Optional[torch.memory_format] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "size",
        "out",
        "memory_format"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.empty_permuted",
      "signature": "torch._refs.empty_permuted(shape, physical_layout, dtype: Optional[torch.dtype] = None, layout: torch.layout = torch.strided, device: Union[str, torch.device, int, NoneType] = None, requires_grad: bool = False, pin_memory: bool = False, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "shape",
        "physical_layout",
        "dtype",
        "layout",
        "device",
        "requires_grad",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.empty_strided",
      "signature": "torch._refs.empty_strided(shape: Union[torch.Size, list[int], tuple[int, ...], tuple[Union[torch.Size, list[int], tuple[int, ...]]]], strides: Union[list[int], tuple[int, ...]], *, dtype: Optional[torch.dtype] = None, device: Union[str, torch.device, int, NoneType] = None, layout: torch.layout = torch.strided, requires_grad: bool = False, pin_memory: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "shape",
        "strides",
        "dtype",
        "device",
        "layout",
        "requires_grad",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.eq",
      "signature": "torch._refs.eq(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.eq_",
      "signature": "torch._refs.eq_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.equal",
      "signature": "torch._refs.equal(a: torch.Tensor, b: torch.Tensor) -> bool",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.erf",
      "signature": "torch._refs.erf(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.erf_",
      "signature": "torch._refs.erf_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.erfc",
      "signature": "torch._refs.erfc(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.erfc_",
      "signature": "torch._refs.erfc_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.erfinv",
      "signature": "torch._refs.erfinv(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.erfinv_",
      "signature": "torch._refs.erfinv_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.exp",
      "signature": "torch._refs.exp(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.exp2",
      "signature": "torch._refs.exp2(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.exp2_",
      "signature": "torch._refs.exp2_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.exp_",
      "signature": "torch._refs.exp_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.expand",
      "signature": "torch._refs.expand(a: torch.Tensor, *shape) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "shape"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.expand_as",
      "signature": "torch._refs.expand_as(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.expand_copy",
      "signature": "torch._refs.expand_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.expm1",
      "signature": "torch._refs.expm1(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.expm1_",
      "signature": "torch._refs.expm1_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.exponential",
      "signature": "torch._refs.exponential(self, rate=1, generator=None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "rate",
        "generator",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.exponential_",
      "signature": "torch._refs.exponential_(self, rate=1, generator=None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "rate",
        "generator",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.eye",
      "signature": "torch._refs.eye(n: int, m: Optional[int] = None, *, dtype: Optional[torch.dtype] = None, layout: torch.layout = torch.strided, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.eye\n    ",
      "arguments": [
        "n",
        "m",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.eye\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.fill",
      "signature": "torch._refs.fill(a: torch.Tensor, value: Union[bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "value",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.fill_",
      "signature": "torch._refs.fill_(a: torch.Tensor, value: Union[bool, int, float, complex]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "value"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.flatten",
      "signature": "torch._refs.flatten(a: torch.Tensor, start_dim: int = 0, end_dim: int = -1) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "start_dim",
        "end_dim"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.flip",
      "signature": "torch._refs.flip(a: torch.Tensor, dims: Union[list[int], tuple[int, ...]], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dims",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.fliplr",
      "signature": "torch._refs.fliplr(a: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.flipud",
      "signature": "torch._refs.flipud(a: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.float_power",
      "signature": "torch._refs.float_power(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.float_power_",
      "signature": "torch._refs.float_power_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.floor",
      "signature": "torch._refs.floor(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.floor_",
      "signature": "torch._refs.floor_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.floor_divide",
      "signature": "torch._refs.floor_divide(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.floor_divide_",
      "signature": "torch._refs.floor_divide_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.fmax",
      "signature": "torch._refs.fmax(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.fmin",
      "signature": "torch._refs.fmin(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.fmod",
      "signature": "torch._refs.fmod(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.fmod_",
      "signature": "torch._refs.fmod_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.frac",
      "signature": "torch._refs.frac(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.frac_",
      "signature": "torch._refs.frac_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.frexp",
      "signature": "torch._refs.frexp(self: torch.Tensor, *, out: tuple[torch.Tensor, torch.Tensor] = None) -> torch._prims_common.wrappers.return_types_frexp",
      "doc": "",
      "arguments": [
        "self",
        "out"
      ],
      "return_type": "<class 'torch._prims_common.wrappers.return_types_frexp'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.full",
      "signature": "torch._refs.full(shape: Union[torch.Size, list[int], tuple[int, ...]], fill_value: Union[bool, int, float, complex], *, dtype: Optional[torch.dtype] = None, layout: torch.layout = torch.strided, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "shape",
        "fill_value",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.full_like",
      "signature": "torch._refs.full_like(a: torch.Tensor, fill_value: Union[bool, int, float, complex], *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = None, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = torch.preserve_format) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "fill_value",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "memory_format"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.gcd",
      "signature": "torch._refs.gcd(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.gcd_",
      "signature": "torch._refs.gcd_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ge",
      "signature": "torch._refs.ge(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ge_",
      "signature": "torch._refs.ge_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.geometric",
      "signature": "torch._refs.geometric(self, p, generator=None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "p",
        "generator",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.geometric_",
      "signature": "torch._refs.geometric_(self, p, generator=None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "p",
        "generator",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.gt",
      "signature": "torch._refs.gt(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.gt_",
      "signature": "torch._refs.gt_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.handle_noncontiguous_outputs",
      "signature": "torch._refs.handle_noncontiguous_outputs(input_tlist, output)",
      "doc": "",
      "arguments": [
        "input_tlist",
        "output"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.heaviside",
      "signature": "torch._refs.heaviside(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.heaviside_",
      "signature": "torch._refs.heaviside_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.hsplit",
      "signature": "torch._refs.hsplit(a: torch.Tensor, indices_or_sections: Union[int, list[int], tuple[int, ...]]) -> tuple[torch.Tensor, ...]",
      "doc": "",
      "arguments": [
        "a",
        "indices_or_sections"
      ],
      "return_type": "tuple[torch.Tensor, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.hstack",
      "signature": "torch._refs.hstack(tensors: Union[list[torch.Tensor], tuple[torch.Tensor, ...]], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "tensors",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.hypot",
      "signature": "torch._refs.hypot(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.hypot_",
      "signature": "torch._refs.hypot_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.i0",
      "signature": "torch._refs.i0(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.i0_",
      "signature": "torch._refs.i0_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.igamma",
      "signature": "torch._refs.igamma(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.igamma_",
      "signature": "torch._refs.igamma_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.igammac",
      "signature": "torch._refs.igammac(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.igammac_",
      "signature": "torch._refs.igammac_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.imag",
      "signature": "torch._refs.imag(a: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.index_add",
      "signature": "torch._refs.index_add(x: torch.Tensor, dim: int, index: torch.Tensor, tensor: torch.Tensor, *, alpha: Union[bool, int, float, complex] = 1, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "x",
        "dim",
        "index",
        "tensor",
        "alpha",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.index_copy",
      "signature": "torch._refs.index_copy(x: torch.Tensor, dim: int, index: torch.Tensor, tensor: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "x",
        "dim",
        "index",
        "tensor",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.index_copy_",
      "signature": "torch._refs.index_copy_(x: torch.Tensor, dim: int, index: torch.Tensor, tensor: torch.Tensor)",
      "doc": "",
      "arguments": [
        "x",
        "dim",
        "index",
        "tensor"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.index_fill",
      "signature": "torch._refs.index_fill(x: torch.Tensor, dim: int, index: torch.Tensor, value: Union[bool, int, float, complex, torch.Tensor], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "x",
        "dim",
        "index",
        "value",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.index_fill_",
      "signature": "torch._refs.index_fill_(x: torch.Tensor, dim: int, index: torch.Tensor, value: Union[bool, int, float, complex, torch.Tensor])",
      "doc": "",
      "arguments": [
        "x",
        "dim",
        "index",
        "value"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.index_select",
      "signature": "torch._refs.index_select(x: torch.Tensor, dim: int, index: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "x",
        "dim",
        "index",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.is_complex",
      "signature": "torch._refs.is_complex(input: torch.Tensor)",
      "doc": "",
      "arguments": [
        "input"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.is_noncontiguous_supported",
      "signature": "torch._refs.is_noncontiguous_supported(device)",
      "doc": "",
      "arguments": [
        "device"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.is_weakly_lesser_type",
      "signature": "torch._refs.is_weakly_lesser_type(a: 'type', b: 'type') -> 'bool'",
      "doc": "\n    Compares two types, a and b, returning True if a is weakly \"less\" than b.\n\n    The comparison is determined by the following type ordering: bool, int, float, complex.\n    ",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Compares two types, a and b, returning True if a is weakly \"less\" than b.\n\n    The comparison is determined by the following type ordering: bool, int, float, complex.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.isclose",
      "signature": "torch._refs.isclose(a: torch.Tensor, b: torch.Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "rtol",
        "atol",
        "equal_nan"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.isfinite",
      "signature": "torch._refs.isfinite(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.isinf",
      "signature": "torch._refs.isinf(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.isnan",
      "signature": "torch._refs.isnan(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.isneginf",
      "signature": "torch._refs.isneginf(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.isposinf",
      "signature": "torch._refs.isposinf(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.isreal",
      "signature": "torch._refs.isreal(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.istft",
      "signature": "torch._refs.istft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex=False) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "n_fft",
        "hop_length",
        "win_length",
        "window",
        "center",
        "normalized",
        "onesided",
        "length",
        "return_complex"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.item",
      "signature": "torch._refs.item(a: torch.Tensor) -> Union[bool, int, float, complex]",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "typing.Union[bool, int, float, complex]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.lcm",
      "signature": "torch._refs.lcm(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.lcm_",
      "signature": "torch._refs.lcm_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.le",
      "signature": "torch._refs.le(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.le_",
      "signature": "torch._refs.le_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.lerp",
      "signature": "torch._refs.lerp(start: torch.Tensor, end: torch.Tensor, weight: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "start",
        "end",
        "weight",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.lerp_",
      "signature": "torch._refs.lerp_(start: torch.Tensor, end: torch.Tensor, weight: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "start",
        "end",
        "weight",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.lgamma",
      "signature": "torch._refs.lgamma(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.lgamma_",
      "signature": "torch._refs.lgamma_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.linspace",
      "signature": "torch._refs.linspace(start: Union[bool, int, float, complex, torch.Tensor], end: Union[bool, int, float, complex, torch.Tensor], steps: Union[bool, int, float, complex], *, dtype: Optional[torch.dtype] = None, device: Union[str, torch.device, int, NoneType] = None, layout: torch.layout = torch.strided, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "start",
        "end",
        "steps",
        "dtype",
        "device",
        "layout",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log",
      "signature": "torch._refs.log(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log10",
      "signature": "torch._refs.log10(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log10_",
      "signature": "torch._refs.log10_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log1p",
      "signature": "torch._refs.log1p(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log1p_",
      "signature": "torch._refs.log1p_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log2",
      "signature": "torch._refs.log2(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log2_",
      "signature": "torch._refs.log2_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log_",
      "signature": "torch._refs.log_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log_normal",
      "signature": "torch._refs.log_normal(self, mean=1, std=2, generator=None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "mean",
        "std",
        "generator",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log_normal_",
      "signature": "torch._refs.log_normal_(self, mean=1, std=2, generator=None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "mean",
        "std",
        "generator",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.log_softmax",
      "signature": "torch._refs.log_softmax(a: torch.Tensor, dim: int, dtype: Optional[torch.dtype] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logaddexp",
      "signature": "torch._refs.logaddexp(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logaddexp2",
      "signature": "torch._refs.logaddexp2(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logical_and",
      "signature": "torch._refs.logical_and(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logical_and_",
      "signature": "torch._refs.logical_and_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logical_not",
      "signature": "torch._refs.logical_not(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logical_not_",
      "signature": "torch._refs.logical_not_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logical_or",
      "signature": "torch._refs.logical_or(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logical_or_",
      "signature": "torch._refs.logical_or_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logical_xor",
      "signature": "torch._refs.logical_xor(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logical_xor_",
      "signature": "torch._refs.logical_xor_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logspace",
      "signature": "torch._refs.logspace(start: Union[bool, int, float, complex, torch.Tensor], end: Union[bool, int, float, complex, torch.Tensor], steps: Union[bool, int, float, complex], base: Union[bool, int, float, complex] = 10, *, dtype: Optional[torch.dtype] = None, device: Union[str, torch.device, int, NoneType] = None, layout: torch.layout = torch.strided, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "start",
        "end",
        "steps",
        "base",
        "dtype",
        "device",
        "layout",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.logsumexp",
      "signature": "torch._refs.logsumexp(self: torch.Tensor, dim: Union[int, list[int], tuple[int, ...]], keepdim: bool = False, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "dim",
        "keepdim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.lt",
      "signature": "torch._refs.lt(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.lt_",
      "signature": "torch._refs.lt_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.masked_fill",
      "signature": "torch._refs.masked_fill(a: torch.Tensor, mask: torch.Tensor, value: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "mask",
        "value",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.masked_fill_",
      "signature": "torch._refs.masked_fill_(a: torch.Tensor, mask: torch.Tensor, value: Union[torch.Tensor, bool, int, float, complex]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "mask",
        "value"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.maximum",
      "signature": "torch._refs.maximum(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.mean",
      "signature": "torch._refs.mean(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, keepdim: bool = False, *, dtype=None, out=None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "keepdim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.meshgrid",
      "signature": "torch._refs.meshgrid(*tensors: Union[torch.Tensor, list[torch.Tensor], tuple[torch.Tensor]], indexing: str) -> list[torch.Tensor]",
      "doc": "",
      "arguments": [
        "tensors",
        "indexing"
      ],
      "return_type": "list[torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.minimum",
      "signature": "torch._refs.minimum(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.movedim",
      "signature": "torch._refs.movedim(input: torch.Tensor, source: Union[int, list[int], tuple[int, ...]], destination: Union[int, list[int], tuple[int, ...]]) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.movedim\n    ",
      "arguments": [
        "input",
        "source",
        "destination"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.movedim\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.mul",
      "signature": "torch._refs.mul(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.mul_",
      "signature": "torch._refs.mul_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.mvlgamma",
      "signature": "torch._refs.mvlgamma(*args, **kwargs)",
      "doc": "",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.mvlgamma_",
      "signature": "torch._refs.mvlgamma_(*args, **kwargs)",
      "doc": "",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.nan_to_num",
      "signature": "torch._refs.nan_to_num(a: torch.Tensor, nan: Union[bool, int, float, complex, NoneType] = 0.0, posinf: Union[bool, int, float, complex, NoneType] = None, neginf: Union[bool, int, float, complex, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "nan",
        "posinf",
        "neginf",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.nan_to_num_",
      "signature": "torch._refs.nan_to_num_(a: torch.Tensor, nan: Union[bool, int, float, complex, NoneType] = 0.0, posinf: Union[bool, int, float, complex, NoneType] = None, neginf: Union[bool, int, float, complex, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "nan",
        "posinf",
        "neginf",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.narrow",
      "signature": "torch._refs.narrow(a: torch.Tensor, dim: int, start: Union[int, torch.Tensor], length: int) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "start",
        "length"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.narrow_copy",
      "signature": "torch._refs.narrow_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.native_group_norm",
      "signature": "torch._refs.native_group_norm(input: torch.Tensor, weight: Optional[torch.Tensor], bias: Optional[torch.Tensor], batch_size: int, num_channels: int, flattened_inner_size: int, num_groups: int, eps: float) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
      "doc": "",
      "arguments": [
        "input",
        "weight",
        "bias",
        "batch_size",
        "num_channels",
        "flattened_inner_size",
        "num_groups",
        "eps"
      ],
      "return_type": "tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.native_layer_norm",
      "signature": "torch._refs.native_layer_norm(input: torch.Tensor, normalized_shape: Union[torch.Size, list[int], tuple[int, ...]], weight: Optional[torch.Tensor], bias: Optional[torch.Tensor], eps: float, *, out: tuple[torch.Tensor, torch.Tensor, torch.Tensor] = None) -> torch._prims_common.wrappers.return_types_native_layer_norm",
      "doc": "",
      "arguments": [
        "input",
        "normalized_shape",
        "weight",
        "bias",
        "eps",
        "out"
      ],
      "return_type": "<class 'torch._prims_common.wrappers.return_types_native_layer_norm'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.native_layer_norm_fake",
      "signature": "torch._refs.native_layer_norm_fake(fake_mode, func, *args, **kwargs)",
      "doc": "",
      "arguments": [
        "fake_mode",
        "func",
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ne",
      "signature": "torch._refs.ne(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ne_",
      "signature": "torch._refs.ne_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.neg",
      "signature": "torch._refs.neg(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.neg_",
      "signature": "torch._refs.neg_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.new_empty",
      "signature": "torch._refs.new_empty(a: torch.Tensor, size: Union[torch.Size, list[int], tuple[int, ...]], *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = None, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "size",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.new_empty_strided",
      "signature": "torch._refs.new_empty_strided(a: torch.Tensor, size: Union[torch.Size, list[int], tuple[int, ...]], stride: Union[list[int], tuple[int, ...]], *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = None, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.Tensor.new_empty_strided\n    ",
      "arguments": [
        "a",
        "size",
        "stride",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.Tensor.new_empty_strided\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.new_full",
      "signature": "torch._refs.new_full(a: torch.Tensor, size: Union[torch.Size, list[int], tuple[int, ...]], fill_value: Union[bool, int, float, complex], *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = None, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "size",
        "fill_value",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.new_ones",
      "signature": "torch._refs.new_ones(a: torch.Tensor, size: Union[torch.Size, list[int], tuple[int, ...]], *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = None, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "size",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.new_zeros",
      "signature": "torch._refs.new_zeros(a: torch.Tensor, size: Union[torch.Size, list[int], tuple[int, ...]], *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = None, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "size",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.nextafter",
      "signature": "torch._refs.nextafter(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.nextafter_",
      "signature": "torch._refs.nextafter_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.norm",
      "signature": "torch._refs.norm(input: torch.Tensor, p: Union[float, str, NoneType] = 'fro', dim: Union[int, list[int], tuple[int, ...], NoneType] = None, keepdim: bool = False, *, dtype: Optional[torch.dtype] = None, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "p",
        "dim",
        "keepdim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.normal",
      "signature": "torch._refs.normal(mean=0, std=1, size=None, *, generator=None, dtype=None, layout=None, device=None, pin_memory=None, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "mean",
        "std",
        "size",
        "generator",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.normal_",
      "signature": "torch._refs.normal_(self, mean=0, std=1, *, generator=None)",
      "doc": "",
      "arguments": [
        "self",
        "mean",
        "std",
        "generator"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ones",
      "signature": "torch._refs.ones(*size, dtype: Optional[torch.dtype] = None, layout: torch.layout = torch.strided, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "size",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ones_like",
      "signature": "torch._refs.ones_like(a: torch.Tensor, *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = None, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = torch.preserve_format, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "memory_format",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.out_wrapper",
      "signature": "torch._refs.out_wrapper(*out_names: str, exact_dtype: bool = False, pass_is_out: bool = False, preserve_memory_format: bool = False) -> Callable[[Callable[~_P, ~_T]], Callable[~_P, ~_T]]",
      "doc": "",
      "arguments": [
        "out_names",
        "exact_dtype",
        "pass_is_out",
        "preserve_memory_format"
      ],
      "return_type": "typing.Callable[[typing.Callable[~_P, ~_T]], typing.Callable[~_P, ~_T]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.overload",
      "signature": "torch._refs.overload(func)",
      "doc": "Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.\n\n    For example::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            ...  # implementation goes here\n\n    The overloads for a function can be retrieved at runtime using the\n    get_overloads() function.\n    ",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.\n\n    For example::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            ...  # implementation goes here\n\n    The overloads for a function can be retrieved at runtime using the\n    get_overloads() function.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.permute",
      "signature": "torch._refs.permute(a: torch.Tensor, *dims) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dims"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.permute_copy",
      "signature": "torch._refs.permute_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.positive",
      "signature": "torch._refs.positive(a: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.pow",
      "signature": "torch._refs.pow(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.pow_",
      "signature": "torch._refs.pow_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.prod",
      "signature": "torch._refs.prod(a: torch.Tensor, dim: Union[int, NoneType, list[int]] = None, keepdim: bool = False, *, dtype=None, out: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "keepdim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rad2deg",
      "signature": "torch._refs.rad2deg(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rad2deg_",
      "signature": "torch._refs.rad2deg_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.randn",
      "signature": "torch._refs.randn(*shape, dtype: Optional[torch.dtype] = None, device: Union[str, torch.device, int, NoneType] = None, layout: Optional[torch.layout] = None, requires_grad: bool = False, pin_memory: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "shape",
        "dtype",
        "device",
        "layout",
        "requires_grad",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.ravel",
      "signature": "torch._refs.ravel(a: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.real",
      "signature": "torch._refs.real(a: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.reciprocal",
      "signature": "torch._refs.reciprocal(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.reciprocal_",
      "signature": "torch._refs.reciprocal_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.register_decomposition",
      "signature": "torch._refs.register_decomposition(aten_op, registry=None, *, type='post_autograd', unsafe=False) -> Callable[[Callable[~_P, ~_T]], Callable[~_P, ~_T]]",
      "doc": "\n    A decorator to register a function as a decomposition to the Python\n    decomposition table.  Use it like this::\n\n        @register_decomposition(torch.ops.aten.clamp_min)\n        def clamp_min(x):\n            return torch.clamp(self, min=min)\n\n    If you are writing a new decomposition, consider contributing it\n    directly to PyTorch in torch._decomp.decompositions.\n\n    This API is experimental; we are almost certainly going to extend\n    the API when we make decompositions eligible for use in transforms (e.g.,\n    autograd) and not just backend tracing, where we then need to know if a\n    decomposition can be used to simulate a transform.\n\n    By default, we also will register it to the Meta key of dispatcher,\n    and replace the c++ Meta implementation if there is already one.\n\n    unsafe kwarg is for reuse of this function for registering non-function\n    things\n    ",
      "arguments": [
        "aten_op",
        "registry",
        "type",
        "unsafe"
      ],
      "return_type": "typing.Callable[[typing.Callable[~_P, ~_T]], typing.Callable[~_P, ~_T]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    A decorator to register a function as a decomposition to the Python\n    decomposition table.  Use it like this::\n\n        @register_decomposition(torch.ops.aten.clamp_min)\n        def clamp_min(x):\n            return torch.clamp(self, min=min)\n\n    If you are writing a new decomposition, consider contributing it\n    directly to PyTorch in torch._decomp.decompositions.\n\n    This API is experimental; we are almost certainly going to extend\n    the API when we make decompositions eligible for use in transforms (e.g.,\n    autograd) and not just backend tracing, where we then need to know if a\n    decomposition can be used to simulate a transform.\n\n    By default, we also will register it to the Meta key of dispatcher,\n    and replace the c++ Meta implementation if there is already one.\n\n    unsafe kwarg is for reuse of this function for registering non-function\n    things\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.remainder",
      "signature": "torch._refs.remainder(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.remainder_",
      "signature": "torch._refs.remainder_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.renorm",
      "signature": "torch._refs.renorm(input: torch.Tensor, p: Union[bool, int, float], dim: int, maxnorm: Union[bool, int, float], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "p",
        "dim",
        "maxnorm",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.repeat",
      "signature": "torch._refs.repeat(a: torch.Tensor, *repeat_shape, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "repeat_shape",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.reshape",
      "signature": "torch._refs.reshape(a: torch.Tensor, *shape: Union[torch.Size, list[int], tuple[int, ...]]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "shape"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.reshape_as",
      "signature": "torch._refs.reshape_as(self: torch.Tensor, other: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "other"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rfloordiv",
      "signature": "torch._refs.rfloordiv(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.roll",
      "signature": "torch._refs.roll(a: torch.Tensor, shifts: Union[int, list[int], tuple[int, ...]], dims: Union[int, list[int], tuple[int, ...]] = (), *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "Reference implementation of :func:`torch.roll`.",
      "arguments": [
        "a",
        "shifts",
        "dims",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reference implementation of :func:`torch.roll`.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rot90",
      "signature": "torch._refs.rot90(a: torch.Tensor, k: int = 1, dims: Union[list[int], tuple[int, ...]] = (0, 1), *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "Reference implementation of :func:`torch.rot90`.",
      "arguments": [
        "a",
        "k",
        "dims",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reference implementation of :func:`torch.rot90`.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.round",
      "signature": "torch._refs.round(a: torch.Tensor, *, decimals: int = 0, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "decimals",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rpow",
      "signature": "torch._refs.rpow(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rsqrt",
      "signature": "torch._refs.rsqrt(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rsqrt_",
      "signature": "torch._refs.rsqrt_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rsub",
      "signature": "torch._refs.rsub(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], alpha: Union[bool, int, float, complex] = 1, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "alpha",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.rtruediv",
      "signature": "torch._refs.rtruediv(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.scalar_tensor",
      "signature": "torch._refs.scalar_tensor(a: Union[bool, int, float, complex], *, dtype: Optional[torch.dtype] = None, layout: torch.layout = torch.strided, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dtype",
        "layout",
        "device",
        "pin_memory"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.select_scatter",
      "signature": "torch._refs.select_scatter(x: torch.Tensor, src: torch.Tensor, dim: int, index: int, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "x",
        "src",
        "dim",
        "index",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sgn",
      "signature": "torch._refs.sgn(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sgn_",
      "signature": "torch._refs.sgn_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sigmoid",
      "signature": "torch._refs.sigmoid(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sigmoid_",
      "signature": "torch._refs.sigmoid_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sign",
      "signature": "torch._refs.sign(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sign_",
      "signature": "torch._refs.sign_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.signbit",
      "signature": "torch._refs.signbit(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sin",
      "signature": "torch._refs.sin(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sin_",
      "signature": "torch._refs.sin_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sinc",
      "signature": "torch._refs.sinc(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sinc_",
      "signature": "torch._refs.sinc_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.singledispatch",
      "signature": "torch._refs.singledispatch(func)",
      "doc": "Single-dispatch generic function decorator.\n\n    Transforms a function into a generic function, which can have different\n    behaviours depending upon the type of its first argument. The decorated\n    function acts as the default implementation, and additional\n    implementations can be registered using the register() attribute of the\n    generic function.\n    ",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Single-dispatch generic function decorator.\n\n    Transforms a function into a generic function, which can have different\n    behaviours depending upon the type of its first argument. The decorated\n    function acts as the default implementation, and additional\n    implementations can be registered using the register() attribute of the\n    generic function.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sinh",
      "signature": "torch._refs.sinh(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sinh_",
      "signature": "torch._refs.sinh_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.softmax",
      "signature": "torch._refs.softmax(a: torch.Tensor, dim: int, dtype: Optional[torch.dtype] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.split_with_sizes",
      "signature": "torch._refs.split_with_sizes(self: torch.Tensor, split_sizes: list[int], dim: int = 0) -> list[torch.Tensor]",
      "doc": "",
      "arguments": [
        "self",
        "split_sizes",
        "dim"
      ],
      "return_type": "list[torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sqrt",
      "signature": "torch._refs.sqrt(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sqrt_",
      "signature": "torch._refs.sqrt_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.square",
      "signature": "torch._refs.square(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.square_",
      "signature": "torch._refs.square_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.squeeze",
      "signature": "torch._refs.squeeze(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.squeeze_copy",
      "signature": "torch._refs.squeeze_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.stack",
      "signature": "torch._refs.stack(tensors: Union[list[torch.Tensor], tuple[torch.Tensor, ...]], dim: int = 0, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "tensors",
        "dim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.std",
      "signature": "torch._refs.std(a: torch.Tensor, dim: Union[int, NoneType, list[int]] = None, unbiased: Optional[bool] = None, keepdim: bool = False, *, correction: Union[bool, int, float, complex, NoneType] = None, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "unbiased",
        "keepdim",
        "correction",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.std_mean",
      "signature": "torch._refs.std_mean(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, *, unbiased: Optional[bool] = None, keepdim: bool = False, correction: Union[bool, int, float, complex, NoneType] = None, out: tuple[torch.Tensor, torch.Tensor] = None) -> torch._prims_common.wrappers.return_types_std_mean",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "unbiased",
        "keepdim",
        "correction",
        "out"
      ],
      "return_type": "<class 'torch._prims_common.wrappers.return_types_std_mean'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.stft",
      "signature": "torch._refs.stft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None, align_to_window: Optional[bool] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "n_fft",
        "hop_length",
        "win_length",
        "window",
        "center",
        "pad_mode",
        "normalized",
        "onesided",
        "return_complex",
        "align_to_window"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sub",
      "signature": "torch._refs.sub(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, alpha: Union[bool, int, float, complex] = 1, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.sub\n    ",
      "arguments": [
        "a",
        "b",
        "alpha",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.sub\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sub_",
      "signature": "torch._refs.sub_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, alpha: Union[bool, int, float, complex] = 1, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "\n    Reference implementation of torch.sub\n    ",
      "arguments": [
        "a",
        "b",
        "alpha",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reference implementation of torch.sub\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sum",
      "signature": "torch._refs.sum(a: torch.Tensor, dim: Union[int, NoneType, list[int]] = None, keepdim: bool = False, *, dtype: Optional[torch.dtype] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "keepdim",
        "dtype",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sum_to_size",
      "signature": "torch._refs.sum_to_size(a: torch.Tensor, *shape) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "shape"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.swap_axes",
      "signature": "torch._refs.swap_axes(a: torch.Tensor, dim0: int, dim1: int) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim0",
        "dim1"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sym_float",
      "signature": "torch._refs.sym_float(a)",
      "doc": "SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.sym_int",
      "signature": "torch._refs.sym_int(a)",
      "doc": "SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.t",
      "signature": "torch._refs.t(a: torch.Tensor)",
      "doc": "",
      "arguments": [
        "a"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.t_copy",
      "signature": "torch._refs.t_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.take_along_dim",
      "signature": "torch._refs.take_along_dim(a: torch.Tensor, indices: torch.Tensor, dim: Optional[int] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "indices",
        "dim",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tan",
      "signature": "torch._refs.tan(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tan_",
      "signature": "torch._refs.tan_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tanh",
      "signature": "torch._refs.tanh(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tanh_",
      "signature": "torch._refs.tanh_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tensor",
      "signature": "torch._refs.tensor(data, *, dtype=None, device=None, pin_memory=False, requires_grad=False)",
      "doc": "",
      "arguments": [
        "data",
        "dtype",
        "device",
        "pin_memory",
        "requires_grad"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tensor_split",
      "signature": "torch._refs.tensor_split(a: torch.Tensor, indices_or_sections: Union[torch.Tensor, int, list[int], tuple[int, ...]], dim: int = 0) -> tuple[torch.Tensor, ...]",
      "doc": "",
      "arguments": [
        "a",
        "indices_or_sections",
        "dim"
      ],
      "return_type": "tuple[torch.Tensor, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.to",
      "signature": "torch._refs.to(a: torch.Tensor, *args, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "args",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.trace",
      "signature": "torch._refs.trace(self: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.transpose",
      "signature": "torch._refs.transpose(a: torch.Tensor, dim0: int, dim1: int) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim0",
        "dim1"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.transpose_copy",
      "signature": "torch._refs.transpose_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tril",
      "signature": "torch._refs.tril(a: torch.Tensor, diagonal: int = 0, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "diagonal",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tril_",
      "signature": "torch._refs.tril_(a: torch.Tensor, diagonal: int = 0, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "diagonal",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.tril_indices",
      "signature": "torch._refs.tril_indices(row: int, col: int, offset: int = 0, *, dtype: torch.dtype = torch.int64, layout: torch.layout = torch.strided, device: Union[str, torch.device, int] = 'cpu', pin_memory: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "row",
        "col",
        "offset",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.triu",
      "signature": "torch._refs.triu(a: torch.Tensor, diagonal: int = 0, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "diagonal",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.triu_",
      "signature": "torch._refs.triu_(a: torch.Tensor, diagonal: int = 0, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "diagonal",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.triu_indices",
      "signature": "torch._refs.triu_indices(row: int, col: int, offset: int = 0, *, dtype: torch.dtype = torch.int64, layout: torch.layout = torch.strided, device: Union[str, torch.device, int] = 'cpu', pin_memory: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "row",
        "col",
        "offset",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.true_divide",
      "signature": "torch._refs.true_divide(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.true_divide_",
      "signature": "torch._refs.true_divide_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.trunc",
      "signature": "torch._refs.trunc(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.trunc_",
      "signature": "torch._refs.trunc_(a: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.trunc_divide",
      "signature": "torch._refs.trunc_divide(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.unbind",
      "signature": "torch._refs.unbind(t: torch.Tensor, dim: int = 0) -> Union[list[torch.Tensor], tuple[torch.Tensor, ...]]",
      "doc": "",
      "arguments": [
        "t",
        "dim"
      ],
      "return_type": "typing.Union[list[torch.Tensor], tuple[torch.Tensor, ...]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.unbind_copy",
      "signature": "torch._refs.unbind_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.unflatten",
      "signature": "torch._refs.unflatten(a: torch.Tensor, dim: int, sizes: Union[torch.Size, list[int], tuple[int, ...]]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "sizes"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.unfold",
      "signature": "torch._refs.unfold(self: torch.Tensor, dimension: int, size: int, step: int) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "dimension",
        "size",
        "step"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.unfold_copy",
      "signature": "torch._refs.unfold_copy(self: torch.Tensor, dimension: int, size: int, step: int, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "dimension",
        "size",
        "step",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.unsqueeze",
      "signature": "torch._refs.unsqueeze(a: torch.Tensor, dim: int) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.unsqueeze_copy",
      "signature": "torch._refs.unsqueeze_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.var",
      "signature": "torch._refs.var(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, unbiased: Optional[bool] = None, keepdim: bool = False, *, correction: Union[bool, int, float, complex, NoneType] = None, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "unbiased",
        "keepdim",
        "correction",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.var_mean",
      "signature": "torch._refs.var_mean(a: torch.Tensor, dim: Union[int, list[int], tuple[int, ...], NoneType] = None, unbiased: Optional[bool] = None, keepdim: bool = False, *, correction: Union[bool, int, float, complex, NoneType] = None, out: tuple[torch.Tensor, torch.Tensor] = None) -> torch._prims_common.wrappers.return_types_var_mean",
      "doc": "",
      "arguments": [
        "a",
        "dim",
        "unbiased",
        "keepdim",
        "correction",
        "out"
      ],
      "return_type": "<class 'torch._prims_common.wrappers.return_types_var_mean'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.vdot",
      "signature": "torch._refs.vdot(self, other, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "other",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.view",
      "signature": "torch._refs.view(a: torch.Tensor, *shape: Union[torch.Size, list[int], tuple[int, ...]]) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "shape"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.view_as",
      "signature": "torch._refs.view_as(self: torch.Tensor, other: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self",
        "other"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.view_as_complex",
      "signature": "torch._refs.view_as_complex(self: torch.Tensor) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "self"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.view_copy",
      "signature": "torch._refs.view_copy(*args, out: torch.Tensor = None, **kwargs) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "args",
        "out",
        "kwargs"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.vsplit",
      "signature": "torch._refs.vsplit(a: torch.Tensor, indices_or_sections: Union[int, list[int], tuple[int, ...]]) -> tuple[torch.Tensor, ...]",
      "doc": "",
      "arguments": [
        "a",
        "indices_or_sections"
      ],
      "return_type": "tuple[torch.Tensor, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.vstack",
      "signature": "torch._refs.vstack(tensors: Union[list[torch.Tensor], tuple[torch.Tensor, ...]], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "tensors",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.where",
      "signature": "torch._refs.where(pred: torch.Tensor, a: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, b: Union[bool, int, float, complex, torch.Tensor, NoneType] = None, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": " ",
      "arguments": [
        "pred",
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": " ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.wraps",
      "signature": "torch._refs.wraps(wrapped, assigned=('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'), updated=('__dict__',))",
      "doc": "Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    ",
      "arguments": [
        "wrapped",
        "assigned",
        "updated"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.xlogy",
      "signature": "torch._refs.xlogy(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.xlogy_",
      "signature": "torch._refs.xlogy_(a: Union[torch.Tensor, bool, int, float, complex], b: Union[torch.Tensor, bool, int, float, complex], *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "b",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.zero",
      "signature": "torch._refs.zero(input: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.zero_",
      "signature": "torch._refs.zero_(input: torch.Tensor, *, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "input",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.zeros",
      "signature": "torch._refs.zeros(*size, dtype: Optional[torch.dtype] = None, layout: torch.layout = torch.strided, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "size",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch._refs.zeros_like",
      "signature": "torch._refs.zeros_like(a: torch.Tensor, *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = None, device: Union[str, torch.device, int, NoneType] = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = torch.preserve_format, out: torch.Tensor = None) -> torch.Tensor",
      "doc": "",
      "arguments": [
        "a",
        "dtype",
        "layout",
        "device",
        "pin_memory",
        "requires_grad",
        "memory_format",
        "out"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "ACCELERATOR_SUPPORT": [
    {
      "function": "torch.accelerator.current_accelerator",
      "signature": "torch.accelerator.current_accelerator(check_available: bool = False) -> Optional[torch.device]",
      "doc": "Return the device of the accelerator available at compilation time.\n    If no accelerator were available at compilation time, returns None.\n    See :ref:`accelerator<accelerators>` for details.\n\n    Args:\n        check_available (bool, optional): if True, will also do a runtime check to see\n            if the device :func:`torch.accelerator.is_available` on top of the compile-time\n            check.\n            Default: ``False``\n\n    Returns:\n        torch.device: return the current accelerator as :class:`torch.device`.\n\n    .. note:: The index of the returned :class:`torch.device` will be ``None``, please use\n        :func:`torch.accelerator.current_device_index` to know the current index being used.\n        This API does NOT poison fork. For more details, see :ref:`multiprocessing-poison-fork-note`.\n\n    Example::\n\n        >>> # xdoctest:\n        >>> # If an accelerator is available, sent the model to it\n        >>> model = torch.nn.Linear(2, 2)\n        >>> if (current_device := current_accelerator(check_available=True)) is not None:\n        >>>     model.to(current_device)\n    ",
      "arguments": [
        "check_available"
      ],
      "return_type": "typing.Optional[torch.device]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the device of the accelerator available at compilation time.\n    If no accelerator were available at compilation time, returns None.\n    See :ref:`accelerator<accelerators>` for details.\n\n    Args:\n        check_available (bool, optional): if True, will also do a runtime check to see\n            if the device :func:`torch.accelerator.is_available` on top of the compile-time\n            check.\n            Default: ``False``\n\n    Returns:\n        torch.device: return the current accelerator as :class:`torch.device`.\n\n    .. note:: The index of the returned :class:`torch.device` will be ``None``, please use\n        :func:`torch.accelerator.current_device_index` to know the current index being used.\n        This API does NOT poison fork. For more details, see :ref:`multiprocessing-poison-fork-note`.\n\n    Example::\n\n        >>> # xdoctest:\n        >>> # If an accelerator is available, sent the model to it\n        >>> model = torch.nn.Linear(2, 2)\n        >>> if (current_device := current_accelerator(check_available=True)) is not None:\n        >>>     model.to(current_device)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.current_device_idx",
      "signature": "torch.accelerator.current_device_idx() -> int",
      "doc": "Return the index of a currently selected device for the current :ref:`accelerator<accelerators>`.\n\n    Returns:\n        int: the index of a currently selected device.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the index of a currently selected device for the current :ref:`accelerator<accelerators>`.\n\n    Returns:\n        int: the index of a currently selected device.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.current_device_index",
      "signature": "torch.accelerator.current_device_index() -> int",
      "doc": "Return the index of a currently selected device for the current :ref:`accelerator<accelerators>`.\n\n    Returns:\n        int: the index of a currently selected device.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the index of a currently selected device for the current :ref:`accelerator<accelerators>`.\n\n    Returns:\n        int: the index of a currently selected device.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.current_stream",
      "signature": "torch.accelerator.current_stream(device: Union[torch.device, str, int, NoneType] = None, /) -> torch.Stream",
      "doc": "Return the currently selected stream for a given device.\n\n    Args:\n        device (:class:`torch.device`, str, int, optional): a given device that must match the current\n            :ref:`accelerator<accelerators>` device type. If not given,\n            use :func:`torch.accelerator.current_device_index` by default.\n\n    Returns:\n        torch.Stream: the currently selected stream for a given device.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the currently selected stream for a given device.\n\n    Args:\n        device (:class:`torch.device`, str, int, optional): a given device that must match the current\n            :ref:`accelerator<accelerators>` device type. If not given,\n            use :func:`torch.accelerator.current_device_index` by default.\n\n    Returns:\n        torch.Stream: the currently selected stream for a given device.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.device_count",
      "signature": "torch.accelerator.device_count() -> int",
      "doc": "Return the number of current :ref:`accelerator<accelerators>` available.\n\n    Returns:\n        int: the number of the current :ref:`accelerator<accelerators>` available.\n            If there is no available accelerators, return 0.\n\n    .. note:: This API delegates to the device-specific version of `device_count`.\n        On CUDA, this API will NOT posion fork if NVML discovery succeeds.\n        Otherwise, it will. For more details, see :ref:`multiprocessing-poison-fork-note`.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the number of current :ref:`accelerator<accelerators>` available.\n\n    Returns:\n        int: the number of the current :ref:`accelerator<accelerators>` available.\n            If there is no available accelerators, return 0.\n\n    .. note:: This API delegates to the device-specific version of `device_count`.\n        On CUDA, this API will NOT posion fork if NVML discovery succeeds.\n        Otherwise, it will. For more details, see :ref:`multiprocessing-poison-fork-note`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.is_available",
      "signature": "torch.accelerator.is_available() -> bool",
      "doc": "Check if the current accelerator is available at runtime: it was build, all the\n    required drivers are available and at least one device is visible.\n    See :ref:`accelerator<accelerators>` for details.\n\n    Returns:\n        bool: A boolean indicating if there is an available :ref:`accelerator<accelerators>`.\n\n    .. note:: This API delegates to the device-specific version of `is_available`.\n        On CUDA, when the environment variable ``PYTORCH_NVML_BASED_CUDA_CHECK=1`` is set,\n        this function will NOT poison fork. Otherwise, it will. For more details, see\n        :ref:`multiprocessing-poison-fork-note`.\n\n    Example::\n\n        >>> assert torch.accelerator.is_available() \"No available accelerators detected.\"\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if the current accelerator is available at runtime: it was build, all the\n    required drivers are available and at least one device is visible.\n    See :ref:`accelerator<accelerators>` for details.\n\n    Returns:\n        bool: A boolean indicating if there is an available :ref:`accelerator<accelerators>`.\n\n    .. note:: This API delegates to the device-specific version of `is_available`.\n        On CUDA, when the environment variable ``PYTORCH_NVML_BASED_CUDA_CHECK=1`` is set,\n        this function will NOT poison fork. Otherwise, it will. For more details, see\n        :ref:`multiprocessing-poison-fork-note`.\n\n    Example::\n\n        >>> assert torch.accelerator.is_available() \"No available accelerators detected.\"\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.set_device_idx",
      "signature": "torch.accelerator.set_device_idx(device: Union[torch.device, str, int, NoneType], /) -> None",
      "doc": "Set the current device index to a given device.\n\n    Args:\n        device (:class:`torch.device`, str, int): a given device that must match the current\n            :ref:`accelerator<accelerators>` device type.\n\n    .. note:: This function is a no-op if this device index is negative.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current device index to a given device.\n\n    Args:\n        device (:class:`torch.device`, str, int): a given device that must match the current\n            :ref:`accelerator<accelerators>` device type.\n\n    .. note:: This function is a no-op if this device index is negative.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.set_device_index",
      "signature": "torch.accelerator.set_device_index(device: Union[torch.device, str, int, NoneType], /) -> None",
      "doc": "Set the current device index to a given device.\n\n    Args:\n        device (:class:`torch.device`, str, int): a given device that must match the current\n            :ref:`accelerator<accelerators>` device type.\n\n    .. note:: This function is a no-op if this device index is negative.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current device index to a given device.\n\n    Args:\n        device (:class:`torch.device`, str, int): a given device that must match the current\n            :ref:`accelerator<accelerators>` device type.\n\n    .. note:: This function is a no-op if this device index is negative.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.set_stream",
      "signature": "torch.accelerator.set_stream(stream: torch.Stream) -> None",
      "doc": "Set the current stream to a given stream.\n\n    Args:\n        stream (torch.Stream): a given stream that must match the current :ref:`accelerator<accelerators>` device type.\n\n    .. note:: This function will set the current device index to the device index of the given stream.\n    ",
      "arguments": [
        "stream"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current stream to a given stream.\n\n    Args:\n        stream (torch.Stream): a given stream that must match the current :ref:`accelerator<accelerators>` device type.\n\n    .. note:: This function will set the current device index to the device index of the given stream.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.accelerator.synchronize",
      "signature": "torch.accelerator.synchronize(device: Union[torch.device, str, int, NoneType] = None, /) -> None",
      "doc": "Wait for all kernels in all streams on the given device to complete.\n\n    Args:\n        device (:class:`torch.device`, str, int, optional): device for which to synchronize. It must match\n            the current :ref:`accelerator<accelerators>` device type. If not given,\n            use :func:`torch.accelerator.current_device_index` by default.\n\n    .. note:: This function is a no-op if the current :ref:`accelerator<accelerators>` is not initialized.\n\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> assert torch.accelerator.is_available() \"No available accelerators detected.\"\n        >>> start_event = torch.Event(enable_timing=True)\n        >>> end_event = torch.Event(enable_timing=True)\n        >>> start_event.record()\n        >>> tensor = torch.randn(100, device=torch.accelerator.current_accelerator())\n        >>> sum = torch.sum(tensor)\n        >>> end_event.record()\n        >>> torch.accelerator.synchronize()\n        >>> elapsed_time_ms = start_event.elapsed_time(end_event)\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Wait for all kernels in all streams on the given device to complete.\n\n    Args:\n        device (:class:`torch.device`, str, int, optional): device for which to synchronize. It must match\n            the current :ref:`accelerator<accelerators>` device type. If not given,\n            use :func:`torch.accelerator.current_device_index` by default.\n\n    .. note:: This function is a no-op if the current :ref:`accelerator<accelerators>` is not initialized.\n\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> assert torch.accelerator.is_available() \"No available accelerators detected.\"\n        >>> start_event = torch.Event(enable_timing=True)\n        >>> end_event = torch.Event(enable_timing=True)\n        >>> start_event.record()\n        >>> tensor = torch.randn(100, device=torch.accelerator.current_accelerator())\n        >>> sum = torch.sum(tensor)\n        >>> end_event.record()\n        >>> torch.accelerator.synchronize()\n        >>> elapsed_time_ms = start_event.elapsed_time(end_event)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "AUTOMATIC_MIXED_PRECISION": [
    {
      "function": "torch.amp.custom_bwd",
      "signature": "torch.amp.custom_bwd(bwd=None, *, device_type: str)",
      "doc": "Create a helper decorator for backward methods of custom autograd functions.\n\n    Autograd functions are subclasses of :class:`torch.autograd.Function`.\n    Ensures that ``backward`` executes with the same autocast state as ``forward``.\n    See the :ref:`example page<amp-custom-examples>` for more detail.\n\n    Args:\n        device_type(str):  Device type to use. 'cuda', 'cpu', 'mtia', 'maia', 'xpu' and so on.\n            The type is the same as the `type` attribute of a :class:`torch.device`.\n            Thus, you may obtain the device type of a tensor using `Tensor.device.type`.\n    ",
      "arguments": [
        "bwd",
        "device_type"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Create a helper decorator for backward methods of custom autograd functions.\n\n    Autograd functions are subclasses of :class:`torch.autograd.Function`.\n    Ensures that ``backward`` executes with the same autocast state as ``forward``.\n    See the :ref:`example page<amp-custom-examples>` for more detail.\n\n    Args:\n        device_type(str):  Device type to use. 'cuda', 'cpu', 'mtia', 'maia', 'xpu' and so on.\n            The type is the same as the `type` attribute of a :class:`torch.device`.\n            Thus, you may obtain the device type of a tensor using `Tensor.device.type`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.amp.custom_fwd",
      "signature": "torch.amp.custom_fwd(fwd=None, *, device_type: str, cast_inputs: Optional[torch.dtype] = None)",
      "doc": "\n    Create a helper decorator for ``forward`` methods of custom autograd functions.\n\n    Autograd functions are subclasses of :class:`torch.autograd.Function`.\n    See the :ref:`example page<amp-custom-examples>` for more detail.\n\n    Args:\n        device_type(str):  Device type to use. 'cuda', 'cpu', 'mtia', 'maia', 'xpu' and so on.\n            The type is the same as the `type` attribute of a :class:`torch.device`.\n            Thus, you may obtain the device type of a tensor using `Tensor.device.type`.\n        cast_inputs (:class:`torch.dtype` or None, optional, default=None):  If not ``None``,\n            when ``forward`` runs in an autocast-enabled region, casts incoming\n            floating-point Tensors to the target dtype (non-floating-point Tensors are not affected),\n            then executes ``forward`` with autocast disabled.\n            If ``None``, ``forward``'s internal ops execute with the current autocast state.\n\n    .. note::\n        If the decorated ``forward`` is called outside an autocast-enabled region,\n        :func:`custom_fwd<custom_fwd>` is a no-op and ``cast_inputs`` has no effect.\n    ",
      "arguments": [
        "fwd",
        "device_type",
        "cast_inputs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Create a helper decorator for ``forward`` methods of custom autograd functions.\n\n    Autograd functions are subclasses of :class:`torch.autograd.Function`.\n    See the :ref:`example page<amp-custom-examples>` for more detail.\n\n    Args:\n        device_type(str):  Device type to use. 'cuda', 'cpu', 'mtia', 'maia', 'xpu' and so on.\n            The type is the same as the `type` attribute of a :class:`torch.device`.\n            Thus, you may obtain the device type of a tensor using `Tensor.device.type`.\n        cast_inputs (:class:`torch.dtype` or None, optional, default=None):  If not ``None``,\n            when ``forward`` runs in an autocast-enabled region, casts incoming\n            floating-point Tensors to the target dtype (non-floating-point Tensors are not affected),\n            then executes ``forward`` with autocast disabled.\n            If ``None``, ``forward``'s internal ops execute with the current autocast state.\n\n    .. note::\n        If the decorated ``forward`` is called outside an autocast-enabled region,\n        :func:`custom_fwd<custom_fwd>` is a no-op and ``cast_inputs`` has no effect.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.amp.is_autocast_available",
      "signature": "torch.amp.is_autocast_available(device_type: str) -> bool",
      "doc": "\n    Return a bool indicating if autocast is available on :attr:`device_type`.\n\n    Args:\n        device_type(str):  Device type to use. Possible values are: 'cuda', 'cpu', 'mtia', 'maia', 'xpu', and so on.\n            The type is the same as the `type` attribute of a :class:`torch.device`.\n            Thus, you may obtain the device type of a tensor using `Tensor.device.type`.\n    ",
      "arguments": [
        "device_type"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return a bool indicating if autocast is available on :attr:`device_type`.\n\n    Args:\n        device_type(str):  Device type to use. Possible values are: 'cuda', 'cpu', 'mtia', 'maia', 'xpu', and so on.\n            The type is the same as the `type` attribute of a :class:`torch.device`.\n            Thus, you may obtain the device type of a tensor using `Tensor.device.type`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "AUTOGRAD": [
    {
      "function": "torch.autograd.backward",
      "signature": "torch.autograd.backward(tensors: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor], ForwardRef('GradientEdge'), collections.abc.Sequence['GradientEdge']], grad_tensors: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor], NoneType] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, grad_variables: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor], NoneType] = None, inputs: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor], ForwardRef('GradientEdge'), collections.abc.Sequence['GradientEdge'], NoneType] = None) -> None",
      "doc": "Compute the sum of gradients of given tensors with respect to graph leaves.\n\n    The graph is differentiated using the chain rule. If any of ``tensors``\n    are non-scalar (i.e. their data has more than one element) and require\n    gradient, then the Jacobian-vector product would be computed, in this\n    case the function additionally requires specifying ``grad_tensors``.\n    It should be a sequence of matching length, that contains the \"vector\"\n    in the Jacobian-vector product, usually the gradient of the differentiated\n    function w.r.t. corresponding tensors (``None`` is an acceptable value for\n    all tensors that don't need gradient tensors).\n\n    This function accumulates gradients in the leaves - you might need to zero\n    ``.grad`` attributes or set them to ``None`` before calling it.\n    See :ref:`Default gradient layouts<default-grad-layouts>`\n    for details on the memory layout of accumulated gradients.\n\n    .. note::\n        Using this method with ``create_graph=True`` will create a reference cycle\n        between the parameter and its gradient which can cause a memory leak.\n        We recommend using ``autograd.grad`` when creating the graph to avoid this.\n        If you have to use this function, make sure to reset the ``.grad`` fields of your\n        parameters to ``None`` after use to break the cycle and avoid the leak.\n\n    .. note::\n\n        If you run any forward ops, create ``grad_tensors``, and/or call ``backward``\n        in a user-specified CUDA stream context, see\n        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n\n    .. note::\n\n        When ``inputs`` are provided and a given input is not a leaf,\n        the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).\n        It is an implementation detail on which the user should not rely.\n        See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n\n    Args:\n        tensors (Sequence[Tensor] or Tensor or Sequence[GradientEdge] or GradientEdge): Tensors of which\n            the derivative will be computed.\n        grad_tensors (Sequence[Tensor or None] or Tensor, optional): The \"vector\" in\n            the Jacobian-vector product, usually gradients w.r.t. each element of\n            corresponding tensors. None values can be specified for scalar Tensors or\n            ones that don't require grad. If a None value would be acceptable for all\n            grad_tensors, then this argument is optional.\n        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n            will be freed. Note that in nearly all cases setting this option to ``True``\n            is not needed and often can be worked around in a much more efficient\n            way. Defaults to the value of ``create_graph``.\n        create_graph (bool, optional): If ``True``, graph of the derivative will\n            be constructed, allowing to compute higher order derivative products.\n            Defaults to ``False``.\n        inputs (Sequence[Tensor] or Tensor or Sequence[GradientEdge], optional): Inputs w.r.t. which the gradient\n            be will accumulated into ``.grad``. All other Tensors will be ignored. If\n            not provided, the gradient is accumulated into all the leaf Tensors that\n            were used to compute the :attr:`tensors`.\n    ",
      "arguments": [
        "tensors",
        "grad_tensors",
        "retain_graph",
        "create_graph",
        "grad_variables",
        "inputs"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Compute the sum of gradients of given tensors with respect to graph leaves.\n\n    The graph is differentiated using the chain rule. If any of ``tensors``\n    are non-scalar (i.e. their data has more than one element) and require\n    gradient, then the Jacobian-vector product would be computed, in this\n    case the function additionally requires specifying ``grad_tensors``.\n    It should be a sequence of matching length, that contains the \"vector\"\n    in the Jacobian-vector product, usually the gradient of the differentiated\n    function w.r.t. corresponding tensors (``None`` is an acceptable value for\n    all tensors that don't need gradient tensors).\n\n    This function accumulates gradients in the leaves - you might need to zero\n    ``.grad`` attributes or set them to ``None`` before calling it.\n    See :ref:`Default gradient layouts<default-grad-layouts>`\n    for details on the memory layout of accumulated gradients.\n\n    .. note::\n        Using this method with ``create_graph=True`` will create a reference cycle\n        between the parameter and its gradient which can cause a memory leak.\n        We recommend using ``autograd.grad`` when creating the graph to avoid this.\n        If you have to use this function, make sure to reset the ``.grad`` fields of your\n        parameters to ``None`` after use to break the cycle and avoid the leak.\n\n    .. note::\n\n        If you run any forward ops, create ``grad_tensors``, and/or call ``backward``\n        in a user-specified CUDA stream context, see\n        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n\n    .. note::\n\n        When ``inputs`` are provided and a given input is not a leaf,\n        the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).\n        It is an implementation detail on which the user should not rely.\n        See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n\n    Args:\n        tensors (Sequence[Tensor] or Tensor or Sequence[GradientEdge] or GradientEdge): Tensors of which\n            the derivative will be computed.\n        grad_tensors (Sequence[Tensor or None] or Tensor, optional): The \"vector\" in\n            the Jacobian-vector product, usually gradients w.r.t. each element of\n            corresponding tensors. None values can be specified for scalar Tensors or\n            ones that don't require grad. If a None value would be acceptable for all\n            grad_tensors, then this argument is optional.\n        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n            will be freed. Note that in nearly all cases setting this option to ``True``\n            is not needed and often can be worked around in a much more efficient\n            way. Defaults to the value of ``create_graph``.\n        create_graph (bool, optional): If ``True``, graph of the derivative will\n            be constructed, allowing to compute higher order derivative products.\n            Defaults to ``False``.\n        inputs (Sequence[Tensor] or Tensor or Sequence[GradientEdge], optional): Inputs w.r.t. which the gradient\n            be will accumulated into ``.grad``. All other Tensors will be ignored. If\n            not provided, the gradient is accumulated into all the leaf Tensors that\n            were used to compute the :attr:`tensors`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.autograd.cast",
      "signature": "torch.autograd.cast(typ, val)",
      "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
      "arguments": [
        "typ",
        "val"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.autograd.grad",
      "signature": "torch.autograd.grad(outputs: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor], ForwardRef('GradientEdge'), collections.abc.Sequence['GradientEdge']], inputs: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor], ForwardRef('GradientEdge'), collections.abc.Sequence['GradientEdge']], grad_outputs: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor], NoneType] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, only_inputs: bool = True, allow_unused: Optional[bool] = None, is_grads_batched: bool = False, materialize_grads: bool = False) -> tuple[torch.Tensor, ...]",
      "doc": "Compute and return the sum of gradients of outputs with respect to the inputs.\n\n    ``grad_outputs`` should be a sequence of length matching ``output``\n    containing the \"vector\" in vector-Jacobian product, usually the pre-computed\n    gradients w.r.t. each of the outputs. If an output doesn't require_grad,\n    then the gradient can be ``None``).\n\n    .. note::\n\n        If you run any forward ops, create ``grad_outputs``, and/or call ``grad``\n        in a user-specified CUDA stream context, see\n        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n\n    .. note::\n\n        ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).\n        To accumulate gradient for other parts of the graph, please use\n        ``torch.autograd.backward``.\n\n    Args:\n        outputs (sequence of Tensor or GradientEdge): outputs of the differentiated function.\n        inputs (sequence of Tensor or GradientEdge): Inputs w.r.t. which the gradient will be\n            returned (and not accumulated into ``.grad``).\n        grad_outputs (sequence of Tensor): The \"vector\" in the vector-Jacobian product.\n            Usually gradients w.r.t. each output. None values can be specified for scalar\n            Tensors or ones that don't require grad. If a None value would be acceptable\n            for all grad_tensors, then this argument is optional. Default: None.\n        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n            will be freed. Note that in nearly all cases setting this option to ``True``\n            is not needed and often can be worked around in a much more efficient\n            way. Defaults to the value of ``create_graph``.\n        create_graph (bool, optional): If ``True``, graph of the derivative will\n            be constructed, allowing to compute higher order derivative products.\n            Default: ``False``.\n        allow_unused (Optional[bool], optional): If ``False``, specifying inputs\n            that were not used when computing outputs (and therefore their grad is\n            always zero) is an error. Defaults to the value of ``materialize_grads``.\n        is_grads_batched (bool, optional): If ``True``, the first dimension of each\n            tensor in ``grad_outputs`` will be interpreted as the batch dimension.\n            Instead of computing a single vector-Jacobian product, we compute a\n            batch of vector-Jacobian products for each \"vector\" in the batch.\n            We use the vmap prototype feature as the backend to vectorize calls\n            to the autograd engine so that this computation can be performed in a\n            single call. This should lead to performance improvements when compared\n            to manually looping and performing backward multiple times. Note that\n            due to this feature being experimental, there may be performance\n            cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``\n            to show any performance warnings and file an issue on github if warnings exist\n            for your use case. Defaults to ``False``.\n        materialize_grads (bool, optional): If ``True``, set the gradient for unused inputs\n            to zero instead of None. This is useful when computing higher-order derivatives.\n            If ``materialize_grads`` is ``True`` and ``allow_unused`` is ``False``, an error\n            will be raised. Defaults to ``False``.\n\n    ",
      "arguments": [
        "outputs",
        "inputs",
        "grad_outputs",
        "retain_graph",
        "create_graph",
        "only_inputs",
        "allow_unused",
        "is_grads_batched",
        "materialize_grads"
      ],
      "return_type": "tuple[torch.Tensor, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Compute and return the sum of gradients of outputs with respect to the inputs.\n\n    ``grad_outputs`` should be a sequence of length matching ``output``\n    containing the \"vector\" in vector-Jacobian product, usually the pre-computed\n    gradients w.r.t. each of the outputs. If an output doesn't require_grad,\n    then the gradient can be ``None``).\n\n    .. note::\n\n        If you run any forward ops, create ``grad_outputs``, and/or call ``grad``\n        in a user-specified CUDA stream context, see\n        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n\n    .. note::\n\n        ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).\n        To accumulate gradient for other parts of the graph, please use\n        ``torch.autograd.backward``.\n\n    Args:\n        outputs (sequence of Tensor or GradientEdge): outputs of the differentiated function.\n        inputs (sequence of Tensor or GradientEdge): Inputs w.r.t. which the gradient will be\n            returned (and not accumulated into ``.grad``).\n        grad_outputs (sequence of Tensor): The \"vector\" in the vector-Jacobian product.\n            Usually gradients w.r.t. each output. None values can be specified for scalar\n            Tensors or ones that don't require grad. If a None value would be acceptable\n            for all grad_tensors, then this argument is optional. Default: None.\n        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n            will be freed. Note that in nearly all cases setting this option to ``True``\n            is not needed and often can be worked around in a much more efficient\n            way. Defaults to the value of ``create_graph``.\n        create_graph (bool, optional): If ``True``, graph of the derivative will\n            be constructed, allowing to compute higher order derivative products.\n            Default: ``False``.\n        allow_unused (Optional[bool], optional): If ``False``, specifying inputs\n            that were not used when computing outputs (and therefore their grad is\n            always zero) is an error. Defaults to the value of ``materialize_grads``.\n        is_grads_batched (bool, optional): If ``True``, the first dimension of each\n            tensor in ``grad_outputs`` will be interpreted as the batch dimension.\n            Instead of computing a single vector-Jacobian product, we compute a\n            batch of vector-Jacobian products for each \"vector\" in the batch.\n            We use the vmap prototype feature as the backend to vectorize calls\n            to the autograd engine so that this computation can be performed in a\n            single call. This should lead to performance improvements when compared\n            to manually looping and performing backward multiple times. Note that\n            due to this feature being experimental, there may be performance\n            cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``\n            to show any performance warnings and file an issue on github if warnings exist\n            for your use case. Defaults to ``False``.\n        materialize_grads (bool, optional): If ``True``, set the gradient for unused inputs\n            to zero instead of None. This is useful when computing higher-order derivatives.\n            If ``materialize_grads`` is ``True`` and ``allow_unused`` is ``False``, an error\n            will be raised. Defaults to ``False``.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.autograd.gradcheck",
      "signature": "torch.autograd.gradcheck(func: Callable[..., Union[torch.Tensor, collections.abc.Sequence[torch.Tensor]]], inputs: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor]], *, eps: float = 1e-06, atol: float = 1e-05, rtol: float = 0.001, raise_exception: bool = True, nondet_tol: float = 0.0, check_undefined_grad: bool = True, check_grad_dtypes: bool = False, check_batched_grad: bool = False, check_batched_forward_grad: bool = False, check_forward_ad: bool = False, check_backward_ad: bool = True, fast_mode: bool = False, masked: Optional[bool] = None) -> bool",
      "doc": "Check gradients computed via small finite differences against analytical\n    gradients wrt tensors in :attr:`inputs` that are of floating point or complex type\n    and with ``requires_grad=True``.\n\n    The check between numerical and analytical gradients uses :func:`~torch.allclose`.\n\n    For most of the complex functions we consider for optimization purposes, no notion of\n    Jacobian exists. Instead, gradcheck verifies if the numerical and analytical values of\n    the Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient\n    computation is done under the assumption that the overall function has a real-valued\n    output, we treat functions with complex output in a special way. For these functions,\n    gradcheck is applied to two real-valued functions corresponding to taking the real\n    components of the complex outputs for the first, and taking the imaginary components\n    of the complex outputs for the second. For more details, check out\n    :ref:`complex_autograd-doc`.\n\n    .. note::\n        The default values are designed for :attr:`input` of double precision.\n        This check will likely fail if :attr:`input` is of less precision, e.g.,\n        ``FloatTensor``.\n\n    .. note::\n        Gradcheck may fail when evaluated on non-differentiable points\n        because the numerically computed gradients via finite differencing may differ\n        those computed analytically (not necessarily because either is incorrect).\n        For more context, see :ref:`non-differentiable-func-grad`.\n\n    .. warning::\n       If any checked tensor in :attr:`input` has overlapping memory, i.e.,\n       different indices pointing to the same memory address (e.g., from\n       :func:`torch.Tensor.expand`), this check will likely fail because the numerical\n       gradients computed by point perturbation at such indices will change\n       values at all other indices that share the same memory address.\n\n    Args:\n        func (function): a Python function that takes Tensor inputs and returns\n            a Tensor or a tuple of Tensors\n        inputs (tuple of Tensor or Tensor): inputs to the function\n        eps (float, optional): perturbation for finite differences\n        atol (float, optional): absolute tolerance\n        rtol (float, optional): relative tolerance\n        raise_exception (bool, optional): indicating whether to raise an exception if\n            the check fails. The exception gives more information about the\n            exact nature of the failure. This is helpful when debugging gradchecks.\n        nondet_tol (float, optional): tolerance for non-determinism. When running\n            identical inputs through the differentiation, the results must either match\n            exactly (default, 0.0) or be within this tolerance.\n        check_undefined_grad (bool, optional): if ``True``, check if undefined output grads\n            are supported and treated as zeros, for ``Tensor`` outputs.\n        check_batched_grad (bool, optional): if ``True``, check if we can compute\n            batched gradients using prototype vmap support. Defaults to False.\n        check_batched_forward_grad (bool, optional): if ``True``, checks if we can compute\n            batched forward gradients using forward ad and prototype vmap support. Defaults to ``False``.\n        check_forward_ad (bool, optional): if ``True``, check that the gradients computed with forward\n            mode AD match the numerical ones. Defaults to ``False``.\n        check_backward_ad (bool, optional): if ``False``, do not perform any checks that rely on\n            backward mode AD to be implemented. Defaults to ``True``.\n        fast_mode (bool, optional): Fast mode for gradcheck and gradgradcheck is currently only\n            implemented for R to R functions. If none of the inputs and outputs are complex\n            a faster implementation of gradcheck that no longer computes the entire jacobian\n            is run; otherwise, we fall back to the slow implementation.\n        masked (bool, optional): if ``True``, the gradients of unspecified elements of\n            sparse tensors are ignored. Defaults to ``False``.\n    Returns:\n        ``True`` if all differences satisfy allclose condition\n\n    ",
      "arguments": [
        "func",
        "inputs",
        "eps",
        "atol",
        "rtol",
        "raise_exception",
        "nondet_tol",
        "check_undefined_grad",
        "check_grad_dtypes",
        "check_batched_grad",
        "check_batched_forward_grad",
        "check_forward_ad",
        "check_backward_ad",
        "fast_mode",
        "masked"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check gradients computed via small finite differences against analytical\n    gradients wrt tensors in :attr:`inputs` that are of floating point or complex type\n    and with ``requires_grad=True``.\n\n    The check between numerical and analytical gradients uses :func:`~torch.allclose`.\n\n    For most of the complex functions we consider for optimization purposes, no notion of\n    Jacobian exists. Instead, gradcheck verifies if the numerical and analytical values of\n    the Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient\n    computation is done under the assumption that the overall function has a real-valued\n    output, we treat functions with complex output in a special way. For these functions,\n    gradcheck is applied to two real-valued functions corresponding to taking the real\n    components of the complex outputs for the first, and taking the imaginary components\n    of the complex outputs for the second. For more details, check out\n    :ref:`complex_autograd-doc`.\n\n    .. note::\n        The default values are designed for :attr:`input` of double precision.\n        This check will likely fail if :attr:`input` is of less precision, e.g.,\n        ``FloatTensor``.\n\n    .. note::\n        Gradcheck may fail when evaluated on non-differentiable points\n        because the numerically computed gradients via finite differencing may differ\n        those computed analytically (not necessarily because either is incorrect).\n        For more context, see :ref:`non-differentiable-func-grad`.\n\n    .. warning::\n       If any checked tensor in :attr:`input` has overlapping memory, i.e.,\n       different indices pointing to the same memory address (e.g., from\n       :func:`torch.Tensor.expand`), this check will likely fail because the numerical\n       gradients computed by point perturbation at such indices will change\n       values at all other indices that share the same memory address.\n\n    Args:\n        func (function): a Python function that takes Tensor inputs and returns\n            a Tensor or a tuple of Tensors\n        inputs (tuple of Tensor or Tensor): inputs to the function\n        eps (float, optional): perturbation for finite differences\n        atol (float, optional): absolute tolerance\n        rtol (float, optional): relative tolerance\n        raise_exception (bool, optional): indicating whether to raise an exception if\n            the check fails. The exception gives more information about the\n            exact nature of the failure. This is helpful when debugging gradchecks.\n        nondet_tol (float, optional): tolerance for non-determinism. When running\n            identical inputs through the differentiation, the results must either match\n            exactly (default, 0.0) or be within this tolerance.\n        check_undefined_grad (bool, optional): if ``True``, check if undefined output grads\n            are supported and treated as zeros, for ``Tensor`` outputs.\n        check_batched_grad (bool, optional): if ``True``, check if we can compute\n            batched gradients using prototype vmap support. Defaults to False.\n        check_batched_forward_grad (bool, optional): if ``True``, checks if we can compute\n            batched forward gradients using forward ad and prototype vmap support. Defaults to ``False``.\n        check_forward_ad (bool, optional): if ``True``, check that the gradients computed with forward\n            mode AD match the numerical ones. Defaults to ``False``.\n        check_backward_ad (bool, optional): if ``False``, do not perform any checks that rely on\n            backward mode AD to be implemented. Defaults to ``True``.\n        fast_mode (bool, optional): Fast mode for gradcheck and gradgradcheck is currently only\n            implemented for R to R functions. If none of the inputs and outputs are complex\n            a faster implementation of gradcheck that no longer computes the entire jacobian\n            is run; otherwise, we fall back to the slow implementation.\n        masked (bool, optional): if ``True``, the gradients of unspecified elements of\n            sparse tensors are ignored. Defaults to ``False``.\n    Returns:\n        ``True`` if all differences satisfy allclose condition\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.autograd.gradgradcheck",
      "signature": "torch.autograd.gradgradcheck(func: Callable[..., Union[torch.Tensor, collections.abc.Sequence[torch.Tensor]]], inputs: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor]], grad_outputs: Union[torch.Tensor, collections.abc.Sequence[torch.Tensor], NoneType] = None, *, eps: float = 1e-06, atol: float = 1e-05, rtol: float = 0.001, gen_non_contig_grad_outputs: bool = False, raise_exception: bool = True, nondet_tol: float = 0.0, check_undefined_grad: bool = True, check_grad_dtypes: bool = False, check_batched_grad: bool = False, check_fwd_over_rev: bool = False, check_rev_over_rev: bool = True, fast_mode: bool = False, masked: bool = False) -> bool",
      "doc": "Check gradients of gradients computed via small finite differences\n    against analytical gradients wrt tensors in :attr:`inputs` and\n    :attr:`grad_outputs` that are of floating point or complex type and with\n    ``requires_grad=True``.\n\n    This function checks that backpropagating through the gradients computed\n    to the given :attr:`grad_outputs` are correct.\n\n    The check between numerical and analytical gradients uses :func:`~torch.allclose`.\n\n    .. note::\n        The default values are designed for :attr:`input` and\n        :attr:`grad_outputs` of double precision. This check will likely fail if\n        they are of less precision, e.g., ``FloatTensor``.\n\n    .. warning::\n       If any checked tensor in :attr:`input` and :attr:`grad_outputs` has\n       overlapping memory, i.e., different indices pointing to the same memory\n       address (e.g., from :func:`torch.Tensor.expand`), this check will likely fail\n       because the numerical gradients computed by point perturbation at such\n       indices will change values at all other indices that share the same\n       memory address.\n\n    Args:\n        func (function): a Python function that takes Tensor inputs and returns\n            a Tensor or a tuple of Tensors\n        inputs (tuple of Tensor or Tensor): inputs to the function\n        grad_outputs (tuple of Tensor or Tensor, optional): The gradients with\n            respect to the function's outputs.\n        eps (float, optional): perturbation for finite differences\n        atol (float, optional): absolute tolerance\n        rtol (float, optional): relative tolerance\n        gen_non_contig_grad_outputs (bool, optional): if :attr:`grad_outputs` is\n            ``None`` and :attr:`gen_non_contig_grad_outputs` is ``True``, the\n            randomly generated gradient outputs are made to be noncontiguous\n        raise_exception (bool, optional): indicating whether to raise an exception if\n            the check fails. The exception gives more information about the\n            exact nature of the failure. This is helpful when debugging gradchecks.\n        nondet_tol (float, optional): tolerance for non-determinism. When running\n            identical inputs through the differentiation, the results must either match\n            exactly (default, 0.0) or be within this tolerance. Note that a small amount\n            of nondeterminism in the gradient will lead to larger inaccuracies in\n            the second derivative.\n        check_undefined_grad (bool, optional): if True, check if undefined output grads\n            are supported and treated as zeros\n        check_batched_grad (bool, optional): if True, check if we can compute\n            batched gradients using prototype vmap support. Defaults to False.\n        fast_mode (bool, optional): if True, run a faster implementation of gradgradcheck that\n            no longer computes the entire jacobian.\n        masked (bool, optional): if True, the gradients of unspecified elements of\n            sparse tensors are ignored (default, False).\n    Returns:\n        True if all differences satisfy allclose condition\n    ",
      "arguments": [
        "func",
        "inputs",
        "grad_outputs",
        "eps",
        "atol",
        "rtol",
        "gen_non_contig_grad_outputs",
        "raise_exception",
        "nondet_tol",
        "check_undefined_grad",
        "check_grad_dtypes",
        "check_batched_grad",
        "check_fwd_over_rev",
        "check_rev_over_rev",
        "fast_mode",
        "masked"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check gradients of gradients computed via small finite differences\n    against analytical gradients wrt tensors in :attr:`inputs` and\n    :attr:`grad_outputs` that are of floating point or complex type and with\n    ``requires_grad=True``.\n\n    This function checks that backpropagating through the gradients computed\n    to the given :attr:`grad_outputs` are correct.\n\n    The check between numerical and analytical gradients uses :func:`~torch.allclose`.\n\n    .. note::\n        The default values are designed for :attr:`input` and\n        :attr:`grad_outputs` of double precision. This check will likely fail if\n        they are of less precision, e.g., ``FloatTensor``.\n\n    .. warning::\n       If any checked tensor in :attr:`input` and :attr:`grad_outputs` has\n       overlapping memory, i.e., different indices pointing to the same memory\n       address (e.g., from :func:`torch.Tensor.expand`), this check will likely fail\n       because the numerical gradients computed by point perturbation at such\n       indices will change values at all other indices that share the same\n       memory address.\n\n    Args:\n        func (function): a Python function that takes Tensor inputs and returns\n            a Tensor or a tuple of Tensors\n        inputs (tuple of Tensor or Tensor): inputs to the function\n        grad_outputs (tuple of Tensor or Tensor, optional): The gradients with\n            respect to the function's outputs.\n        eps (float, optional): perturbation for finite differences\n        atol (float, optional): absolute tolerance\n        rtol (float, optional): relative tolerance\n        gen_non_contig_grad_outputs (bool, optional): if :attr:`grad_outputs` is\n            ``None`` and :attr:`gen_non_contig_grad_outputs` is ``True``, the\n            randomly generated gradient outputs are made to be noncontiguous\n        raise_exception (bool, optional): indicating whether to raise an exception if\n            the check fails. The exception gives more information about the\n            exact nature of the failure. This is helpful when debugging gradchecks.\n        nondet_tol (float, optional): tolerance for non-determinism. When running\n            identical inputs through the differentiation, the results must either match\n            exactly (default, 0.0) or be within this tolerance. Note that a small amount\n            of nondeterminism in the gradient will lead to larger inaccuracies in\n            the second derivative.\n        check_undefined_grad (bool, optional): if True, check if undefined output grads\n            are supported and treated as zeros\n        check_batched_grad (bool, optional): if True, check if we can compute\n            batched gradients using prototype vmap support. Defaults to False.\n        fast_mode (bool, optional): if True, run a faster implementation of gradgradcheck that\n            no longer computes the entire jacobian.\n        masked (bool, optional): if True, the gradients of unspecified elements of\n            sparse tensors are ignored (default, False).\n    Returns:\n        True if all differences satisfy allclose condition\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.autograd.handle_torch_function",
      "signature": "torch.autograd.handle_torch_function(public_api: Callable, relevant_args: collections.abc.Iterable[typing.Any], *args, **kwargs) -> Any",
      "doc": "Implement a function with checks for ``__torch_function__`` overrides.\n\n    See torch::autograd::handle_torch_function for the equivalent of this\n    function in the C++ implementation.\n\n    Arguments\n    ---------\n    public_api : function\n        Function exposed by the public torch API originally called like\n        ``public_api(*args, **kwargs)`` on which arguments are now being\n        checked.\n    relevant_args : iterable\n        Iterable of arguments to check for __torch_function__ methods.\n    args : tuple\n        Arbitrary positional arguments originally passed into ``public_api``.\n    kwargs : tuple\n        Arbitrary keyword arguments originally passed into ``public_api``.\n\n    Returns\n    -------\n    object\n        Result from calling ``implementation`` or an ``__torch_function__``\n        method, as appropriate.\n\n    Raises\n    ------\n    TypeError : if no implementation is found.\n\n    Example\n    -------\n    >>> def func(a):\n    ...     if has_torch_function_unary(a):\n    ...         return handle_torch_function(func, (a,), a)\n    ...     return a + 0\n    ",
      "arguments": [
        "public_api",
        "relevant_args",
        "args",
        "kwargs"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Implement a function with checks for ``__torch_function__`` overrides.\n\n    See torch::autograd::handle_torch_function for the equivalent of this\n    function in the C++ implementation.\n\n    Arguments\n    ---------\n    public_api : function\n        Function exposed by the public torch API originally called like\n        ``public_api(*args, **kwargs)`` on which arguments are now being\n        checked.\n    relevant_args : iterable\n        Iterable of arguments to check for __torch_function__ methods.\n    args : tuple\n        Arbitrary positional arguments originally passed into ``public_api``.\n    kwargs : tuple\n        Arbitrary keyword arguments originally passed into ``public_api``.\n\n    Returns\n    -------\n    object\n        Result from calling ``implementation`` or an ``__torch_function__``\n        method, as appropriate.\n\n    Raises\n    ------\n    TypeError : if no implementation is found.\n\n    Example\n    -------\n    >>> def func(a):\n    ...     if has_torch_function_unary(a):\n    ...         return handle_torch_function(func, (a,), a)\n    ...     return a + 0\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.autograd.is_tensor_like",
      "signature": "torch.autograd.is_tensor_like(inp)",
      "doc": "\n    Returns ``True`` if the passed-in input is a Tensor-like.\n\n    Currently, this occurs whenever there's a ``__torch_function__``\n    attribute on the type of the input.\n\n    Examples\n    --------\n    A subclass of tensor is generally a Tensor-like.\n\n    >>> class SubTensor(torch.Tensor): ...\n    >>> is_tensor_like(SubTensor([0]))\n    True\n\n    Built-in or user types aren't usually Tensor-like.\n\n    >>> is_tensor_like(6)\n    False\n    >>> is_tensor_like(None)\n    False\n    >>> class NotATensor: ...\n    >>> is_tensor_like(NotATensor())\n    False\n\n    But, they can be made Tensor-like by implementing __torch_function__.\n\n    >>> class TensorLike:\n    ...     @classmethod\n    ...     def __torch_function__(cls, func, types, args, kwargs):\n    ...         return -1\n    >>> is_tensor_like(TensorLike())\n    True\n    ",
      "arguments": [
        "inp"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns ``True`` if the passed-in input is a Tensor-like.\n\n    Currently, this occurs whenever there's a ``__torch_function__``\n    attribute on the type of the input.\n\n    Examples\n    --------\n    A subclass of tensor is generally a Tensor-like.\n\n    >>> class SubTensor(torch.Tensor): ...\n    >>> is_tensor_like(SubTensor([0]))\n    True\n\n    Built-in or user types aren't usually Tensor-like.\n\n    >>> is_tensor_like(6)\n    False\n    >>> is_tensor_like(None)\n    False\n    >>> class NotATensor: ...\n    >>> is_tensor_like(NotATensor())\n    False\n\n    But, they can be made Tensor-like by implementing __torch_function__.\n\n    >>> class TensorLike:\n    ...     @classmethod\n    ...     def __torch_function__(cls, func, types, args, kwargs):\n    ...         return -1\n    >>> is_tensor_like(TensorLike())\n    True\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.autograd.variable",
      "signature": "torch.autograd.variable(*args, **kwargs)",
      "doc": "",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "BACKEND_MANAGEMENT": [
    {
      "function": "torch.backends.contextmanager",
      "signature": "torch.backends.contextmanager(func)",
      "doc": "@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    ",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.backends.disable_global_flags",
      "signature": "torch.backends.disable_global_flags()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.backends.flags_frozen",
      "signature": "torch.backends.flags_frozen()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "COMPILATION": [
    {
      "function": "torch.compiler.allow_in_graph",
      "signature": "torch.compiler.allow_in_graph(fn)",
      "doc": "\n    Tells the compiler frontend (Dynamo) to skip symbolic introspection of the function\n    and instead directly write it to the graph when encountered.\n\n    If you are using :func:`torch.compile` (with backend=\"inductor\" (the default)), or\n    :func:`torch.export.export`, and trying to black-box a Python function throughout\n    all tracing, do not use this API.\n    Instead, please create a custom operator (see `PyTorch Custom Operators Landing Page\n    <https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html>`_)\n\n    .. warning::\n\n        If you're a typical torch.compile user (e.g. you're applying torch.compile to\n        a model to make it run faster), you probably don't want to use this function.\n        :func:`allow_in_graph` is a footgun because it skips the compiler frontend\n        (Dynamo) that is responsible for doing safety checks (graph breaks, handling\n        closures, etc). Incorrect usage will lead to difficult-to-debug silent\n        incorrectness issues.\n\n    Given a Python function with no allow_in_graph decorator, regular execution\n    of torch.compile traces through the function. :func:`allow_in_graph` changes\n    it so that the frontend does not trace inside the function, but the compiler\n    backend still traces through it. Compare this to custom operators, which\n    treats a function as a black box throughout the torch.compile stack. The following\n    table compares these mechanisms.\n\n    +------------------------+-----------------------+--------------------------------+\n    | Mechanism              | Frontend (Dynamo)     | Backend (AOTAutograd+Inductor) |\n    +========================+=======================+================================+\n    | no decorator           | trace inside          | trace inside                   |\n    +------------------------+-----------------------+--------------------------------+\n    | allow_in_graph         | opaque callable       | trace inside                   |\n    +------------------------+-----------------------+--------------------------------+\n    | custom op              | opaque callable       | opaque callable                |\n    +------------------------+-----------------------+--------------------------------+\n\n    One common use case for :func:`allow_in_graph()` is as an escape hatch for the compiler\n    frontend: if you know the function works w.r.t. to the downstream components of the\n    compilation stack (AOTAutograd and Inductor) but there is a Dynamo bug that prevents it from\n    symbolically introspecting the function properly (or if your code is in C/C++ and\n    therefore cannot be introspected with Dynamo), then one can decorate said function\n    with :func:`allow_in_graph` to bypass Dynamo.\n\n    We require that ``fn`` adhere to the following restrictions. Failure to adhere\n    results in undefined behavior:\n\n    - The inputs to ``fn`` must be Proxy-able types in the FX graph. Valid types include:\n      Tensor/int/bool/float/None/List[Tensor?]/List[int?]/List[float?]\n      Tuple[Tensor?, ...]/Tuple[int?, ...]/Tuple[float?, ...]/torch.dtype/torch.device\n    - The outputs to ``fn`` must be Proxy-able types in the FX graph (see previous bullet)\n    - all Tensors used inside of ``fn`` must be passed directly as inputs to ``fn``\n      (as opposed to being captured variables).\n\n    Args:\n        fn: A callable representing the function to be included in the graph.\n            If ``fn`` is a list or tuple of callables it recursively applies\n            :func:`allow_in_graph()` to each function and returns a new list or\n            tuple containing the modified functions.\n\n    Example::\n\n        torch.compiler.allow_in_graph(my_custom_function)\n\n        @torch.compile(...)\n        def fn(x):\n            x = torch.add(x, 1)\n            x = my_custom_function(x)\n            x = torch.add(x, 1)\n            return x\n\n        fn(...)\n\n    Will capture a single graph containing ``my_custom_function()``.\n\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Tells the compiler frontend (Dynamo) to skip symbolic introspection of the function\n    and instead directly write it to the graph when encountered.\n\n    If you are using :func:`torch.compile` (with backend=\"inductor\" (the default)), or\n    :func:`torch.export.export`, and trying to black-box a Python function throughout\n    all tracing, do not use this API.\n    Instead, please create a custom operator (see `PyTorch Custom Operators Landing Page\n    <https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html>`_)\n\n    .. warning::\n\n        If you're a typical torch.compile user (e.g. you're applying torch.compile to\n        a model to make it run faster), you probably don't want to use this function.\n        :func:`allow_in_graph` is a footgun because it skips the compiler frontend\n        (Dynamo) that is responsible for doing safety checks (graph breaks, handling\n        closures, etc). Incorrect usage will lead to difficult-to-debug silent\n        incorrectness issues.\n\n    Given a Python function with no allow_in_graph decorator, regular execution\n    of torch.compile traces through the function. :func:`allow_in_graph` changes\n    it so that the frontend does not trace inside the function, but the compiler\n    backend still traces through it. Compare this to custom operators, which\n    treats a function as a black box throughout the torch.compile stack. The following\n    table compares these mechanisms.\n\n    +------------------------+-----------------------+--------------------------------+\n    | Mechanism              | Frontend (Dynamo)     | Backend (AOTAutograd+Inductor) |\n    +========================+=======================+================================+\n    | no decorator           | trace inside          | trace inside                   |\n    +------------------------+-----------------------+--------------------------------+\n    | allow_in_graph         | opaque callable       | trace inside                   |\n    +------------------------+-----------------------+--------------------------------+\n    | custom op              | opaque callable       | opaque callable                |\n    +------------------------+-----------------------+--------------------------------+\n\n    One common use case for :func:`allow_in_graph()` is as an escape hatch for the compiler\n    frontend: if you know the function works w.r.t. to the downstream components of the\n    compilation stack (AOTAutograd and Inductor) but there is a Dynamo bug that prevents it from\n    symbolically introspecting the function properly (or if your code is in C/C++ and\n    therefore cannot be introspected with Dynamo), then one can decorate said function\n    with :func:`allow_in_graph` to bypass Dynamo.\n\n    We require that ``fn`` adhere to the following restrictions. Failure to adhere\n    results in undefined behavior:\n\n    - The inputs to ``fn`` must be Proxy-able types in the FX graph. Valid types include:\n      Tensor/int/bool/float/None/List[Tensor?]/List[int?]/List[float?]\n      Tuple[Tensor?, ...]/Tuple[int?, ...]/Tuple[float?, ...]/torch.dtype/torch.device\n    - The outputs to ``fn`` must be Proxy-able types in the FX graph (see previous bullet)\n    - all Tensors used inside of ``fn`` must be passed directly as inputs to ``fn``\n      (as opposed to being captured variables).\n\n    Args:\n        fn: A callable representing the function to be included in the graph.\n            If ``fn`` is a list or tuple of callables it recursively applies\n            :func:`allow_in_graph()` to each function and returns a new list or\n            tuple containing the modified functions.\n\n    Example::\n\n        torch.compiler.allow_in_graph(my_custom_function)\n\n        @torch.compile(...)\n        def fn(x):\n            x = torch.add(x, 1)\n            x = my_custom_function(x)\n            x = torch.add(x, 1)\n            return x\n\n        fn(...)\n\n    Will capture a single graph containing ``my_custom_function()``.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.assume_constant_result",
      "signature": "torch.compiler.assume_constant_result(fn)",
      "doc": "\n    This function is used to mark a function `fn` as having a constant result.\n    This allows the compiler to optimize away your function.\n    Returns The same function `fn`\n\n    Args:\n        fn: The function to be marked as having a constant result.\n\n    .. warning::\n        `assume_constant_result` can if invalid cause safety and soundness issues, :func:`torch.compile`\n        will not attempt to validate whether the constant assumption is true or not\n\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This function is used to mark a function `fn` as having a constant result.\n    This allows the compiler to optimize away your function.\n    Returns The same function `fn`\n\n    Args:\n        fn: The function to be marked as having a constant result.\n\n    .. warning::\n        `assume_constant_result` can if invalid cause safety and soundness issues, :func:`torch.compile`\n        will not attempt to validate whether the constant assumption is true or not\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.compile",
      "signature": "torch.compiler.compile(*args, **kwargs)",
      "doc": "\n    See :func:`torch.compile` for details on the arguments for this function.\n    ",
      "arguments": [
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    See :func:`torch.compile` for details on the arguments for this function.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.cudagraph_mark_step_begin",
      "signature": "torch.compiler.cudagraph_mark_step_begin()",
      "doc": "\n    Indicates that a new iteration of inference or training is about to begin.\n\n    CUDA Graphs will free tensors of a prior iteration. A new iteration is started on each invocation of\n    torch.compile, so long as there is not a pending backward that has not been called.\n\n    If that heuristic is wrong, such as in the following example, manually mark it with this api.\n\n    .. code-block:: python\n\n        @torch.compile(mode=\"reduce-overhead\")\n        def rand_foo():\n            return torch.rand([4], device=\"cuda\")\n\n        for _ in range(5):\n            torch.compiler.cudagraph_mark_step_begin()\n            rand_foo() + rand_foo()\n\n    For more details, see `torch.compiler_cudagraph_trees <https://pytorch.org/docs/main/torch.compiler_cudagraph_trees.html>`__\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Indicates that a new iteration of inference or training is about to begin.\n\n    CUDA Graphs will free tensors of a prior iteration. A new iteration is started on each invocation of\n    torch.compile, so long as there is not a pending backward that has not been called.\n\n    If that heuristic is wrong, such as in the following example, manually mark it with this api.\n\n    .. code-block:: python\n\n        @torch.compile(mode=\"reduce-overhead\")\n        def rand_foo():\n            return torch.rand([4], device=\"cuda\")\n\n        for _ in range(5):\n            torch.compiler.cudagraph_mark_step_begin()\n            rand_foo() + rand_foo()\n\n    For more details, see `torch.compiler_cudagraph_trees <https://pytorch.org/docs/main/torch.compiler_cudagraph_trees.html>`__\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.disable",
      "signature": "torch.compiler.disable(fn=None, recursive=True, *, reason=None)",
      "doc": "\n    This function provides a decorator to disable compilation on a function.\n    It also provides the option of recursively disabling called functions.\n\n    Args:\n        fn (optional): The function to disable\n        recursive (optional): A boolean value indicating whether the disabling should be recursive.\n        reason (optional): A string value indicating the reason for disabling the function.\n    ",
      "arguments": [
        "fn",
        "recursive",
        "reason"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This function provides a decorator to disable compilation on a function.\n    It also provides the option of recursively disabling called functions.\n\n    Args:\n        fn (optional): The function to disable\n        recursive (optional): A boolean value indicating whether the disabling should be recursive.\n        reason (optional): A string value indicating the reason for disabling the function.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.is_compiling",
      "signature": "torch.compiler.is_compiling() -> bool",
      "doc": "\n    Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().\n\n    Note that there are 2 other related flags that should deprecated eventually:\n      * torch._dynamo.external_utils.is_compiling()\n      * torch._utils.is_compiling()\n\n    Example::\n\n        >>> def forward(self, x):\n        >>>     if not torch.compiler.is_compiling():\n        >>>        pass # ...logic that is not needed in a compiled/traced graph...\n        >>>\n        >>>     # ...rest of the function...\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().\n\n    Note that there are 2 other related flags that should deprecated eventually:\n      * torch._dynamo.external_utils.is_compiling()\n      * torch._utils.is_compiling()\n\n    Example::\n\n        >>> def forward(self, x):\n        >>>     if not torch.compiler.is_compiling():\n        >>>        pass # ...logic that is not needed in a compiled/traced graph...\n        >>>\n        >>>     # ...rest of the function...\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.is_dynamo_compiling",
      "signature": "torch.compiler.is_dynamo_compiling() -> bool",
      "doc": "\n    Indicates whether a graph is traced via TorchDynamo.\n\n    It's stricter than is_compiling() flag, as it would only be set to True when\n    TorchDynamo is used.\n\n    Example::\n\n        >>> def forward(self, x):\n        >>>     if not torch.compiler.is_dynamo_compiling():\n        >>>        pass # ...logic that is not needed in a TorchDynamo-traced graph...\n        >>>\n        >>>     # ...rest of the function...\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Indicates whether a graph is traced via TorchDynamo.\n\n    It's stricter than is_compiling() flag, as it would only be set to True when\n    TorchDynamo is used.\n\n    Example::\n\n        >>> def forward(self, x):\n        >>>     if not torch.compiler.is_dynamo_compiling():\n        >>>        pass # ...logic that is not needed in a TorchDynamo-traced graph...\n        >>>\n        >>>     # ...rest of the function...\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.is_exporting",
      "signature": "torch.compiler.is_exporting() -> bool",
      "doc": "\n    Indicated whether we're under exporting.\n\n    It's stricter than is_compiling() flag, as it would only be set to True when\n    torch.export is used.\n\n    Example::\n\n        >>> def forward(self, x):\n        >>>     if not torch.compiler.is_exporting():\n        >>>        pass # ...logic that is not needed in export...\n        >>>\n        >>>     # ...rest of the function...\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Indicated whether we're under exporting.\n\n    It's stricter than is_compiling() flag, as it would only be set to True when\n    torch.export is used.\n\n    Example::\n\n        >>> def forward(self, x):\n        >>>     if not torch.compiler.is_exporting():\n        >>>        pass # ...logic that is not needed in export...\n        >>>\n        >>>     # ...rest of the function...\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.list_backends",
      "signature": "torch.compiler.list_backends(exclude_tags=('debug', 'experimental')) -> list[str]",
      "doc": "\n    Return valid strings that can be passed to `torch.compile(..., backend=\"name\")`.\n\n    Args:\n        exclude_tags(optional): A tuple of strings representing tags to exclude.\n    ",
      "arguments": [
        "exclude_tags"
      ],
      "return_type": "list[str]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return valid strings that can be passed to `torch.compile(..., backend=\"name\")`.\n\n    Args:\n        exclude_tags(optional): A tuple of strings representing tags to exclude.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.load_cache_artifacts",
      "signature": "torch.compiler.load_cache_artifacts(serialized_artifacts: bytes) -> Optional[ForwardRef('CacheInfo')]",
      "doc": "\n    Hot loads cache artifacts that were previously serialized via\n    save_cache_artifacts\n\n    Example:\n\n    # From a previous invocation\n    artifacts = torch.compiler.save_cache_artifacts()\n\n    torch.compiler.load_cache_artifacts(artifacts[0])\n    ",
      "arguments": [
        "serialized_artifacts"
      ],
      "return_type": "typing.Optional[ForwardRef('CacheInfo')]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Hot loads cache artifacts that were previously serialized via\n    save_cache_artifacts\n\n    Example:\n\n    # From a previous invocation\n    artifacts = torch.compiler.save_cache_artifacts()\n\n    torch.compiler.load_cache_artifacts(artifacts[0])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.reset",
      "signature": "torch.compiler.reset() -> None",
      "doc": "\n    This function clears all compilation caches and restores the system to its initial state.\n    It is recommended to call this function, especially after using operations like `torch.compile(...)`\n    to ensure a clean state before another unrelated compilation\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This function clears all compilation caches and restores the system to its initial state.\n    It is recommended to call this function, especially after using operations like `torch.compile(...)`\n    to ensure a clean state before another unrelated compilation\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.save_cache_artifacts",
      "signature": "torch.compiler.save_cache_artifacts() -> Optional[tuple[bytes, 'CacheInfo']]",
      "doc": "\n    Serializes all the cache artifacts that were created during the compilation\n\n    Example:\n\n    - Execute torch.compile\n    - Call torch.compiler.save_cache_artifacts()\n    ",
      "arguments": [],
      "return_type": "typing.Optional[tuple[bytes, 'CacheInfo']]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Serializes all the cache artifacts that were created during the compilation\n\n    Example:\n\n    - Execute torch.compile\n    - Call torch.compiler.save_cache_artifacts()\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.set_stance",
      "signature": "torch.compiler.set_stance(stance: str = 'default', *, skip_guard_eval_unsafe=False, force_backend=None)",
      "doc": "\n    Set the current stance of the compiler.\n    Can be used as a function, context manager, or decorator.\n    Do not use this function inside a `torch.compile` region - an error will be raised otherwise.\n\n    .. code-block:: python\n\n        @torch.compile\n        def foo(x):\n            ...\n\n        @torch.compiler.set_stance(\"force_eager\")\n        def bar():\n            # will not be compiled\n            foo(...)\n\n        bar()\n\n        with torch.compiler.set_stance(\"force_eager\"):\n            # will also not be compiled\n            foo(...)\n\n        torch.compiler.set_stance(\"force_eager\")\n        # will also not be compiled\n        foo(...)\n        torch.compiler.set_stance(\"default\")\n\n        # will be compiled\n        foo(...)\n\n    Args:\n        stance: The stance to set the compiler to. Valid values are:\n\n            - \"default\": The default stance, used for normal compilation.\n            - \"force_eager\": Ignore all `torch.compile` directives.\n            - \"eager_on_recompile\": Run code eagerly when a recompile is necessary.\n              If there is cached compiled code valid for the input, it will still be used.\n            - \"fail_on_recompile\": Raise an error when recompiling a function.\n\n        skip_guard_eval_unsafe: A flag to run only differentiating guards.\n            CAUTION - This flag is unsafe and should only be used if your setup\n            meets the following conditions.\n\n            torch.compile uses a guard system to support recompilations and\n            choose which compiled artifact to run at runtime.  These guards,\n            though efficient, add some overhead, which may impact performance in\n            scenarios where you need to optimize for minimal guard processing\n            time.  This API enables you to disable guard evaluation, assuming\n            that you have warmed up the compiled model with a sufficient variety\n            of inputs. This assumption means that, after the warmup phase, no\n            further recompilations will be necessary.  If this assumption fails,\n            there is a risk of silently producing incorrect results (hence the\n            term \"unsafe\" in the API name).\n\n        force_backend: If `stance` is \"default\", this argument can be used to force `torch.compile`\n            to use a specific backend. Otherwise, an error is raised.\n    ",
      "arguments": [
        "stance",
        "skip_guard_eval_unsafe",
        "force_backend"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Set the current stance of the compiler.\n    Can be used as a function, context manager, or decorator.\n    Do not use this function inside a `torch.compile` region - an error will be raised otherwise.\n\n    .. code-block:: python\n\n        @torch.compile\n        def foo(x):\n            ...\n\n        @torch.compiler.set_stance(\"force_eager\")\n        def bar():\n            # will not be compiled\n            foo(...)\n\n        bar()\n\n        with torch.compiler.set_stance(\"force_eager\"):\n            # will also not be compiled\n            foo(...)\n\n        torch.compiler.set_stance(\"force_eager\")\n        # will also not be compiled\n        foo(...)\n        torch.compiler.set_stance(\"default\")\n\n        # will be compiled\n        foo(...)\n\n    Args:\n        stance: The stance to set the compiler to. Valid values are:\n\n            - \"default\": The default stance, used for normal compilation.\n            - \"force_eager\": Ignore all `torch.compile` directives.\n            - \"eager_on_recompile\": Run code eagerly when a recompile is necessary.\n              If there is cached compiled code valid for the input, it will still be used.\n            - \"fail_on_recompile\": Raise an error when recompiling a function.\n\n        skip_guard_eval_unsafe: A flag to run only differentiating guards.\n            CAUTION - This flag is unsafe and should only be used if your setup\n            meets the following conditions.\n\n            torch.compile uses a guard system to support recompilations and\n            choose which compiled artifact to run at runtime.  These guards,\n            though efficient, add some overhead, which may impact performance in\n            scenarios where you need to optimize for minimal guard processing\n            time.  This API enables you to disable guard evaluation, assuming\n            that you have warmed up the compiled model with a sufficient variety\n            of inputs. This assumption means that, after the warmup phase, no\n            further recompilations will be necessary.  If this assumption fails,\n            there is a risk of silently producing incorrect results (hence the\n            term \"unsafe\" in the API name).\n\n        force_backend: If `stance` is \"default\", this argument can be used to force `torch.compile`\n            to use a specific backend. Otherwise, an error is raised.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.substitute_in_graph",
      "signature": "torch.compiler.substitute_in_graph(original_fn: Callable[~_P, ~_R], *, can_constant_fold_through: bool = False, skip_signature_check: bool = False) -> Callable[[Callable[~_P, ~_R]], Callable[~_P, ~_R]]",
      "doc": "\n    Register a polyfill handler for a function, usually a C function from the C extension, to be\n    used in place of the original function when inlining the original function in the graph.\n\n    .. note::\n\n        The polyfill handler is only used when inlining the original function. It is not used when\n        the original function is called directly. In the eager mode, the decorated function calls\n        the performant C function rather than the polyfill handler.\n\n    The polyfill handler is a function that will be called in place of the original function when\n    inlining the original function. The polyfill handler should have the same signature and the same\n    behavior as the original function.\n\n    Args:\n        original_fn (callable): The original function, usually a C function, to register a polyfill\n            handler for.\n        can_constant_fold_through (bool, optional): Whether the polyfill handler can be constant\n            folded through. That is, if the polyfill handler is a pure function and its arguments\n            are constant, the result of the polyfill handler can be constant folded during the\n            compilation. Defaults to ``False``.\n        skip_signature_check (bool, optional): Whether to skip the signature check between the\n            original function and the polyfill handler. Defaults to ``False``.\n\n    Returns:\n        A decorator that registers the polyfill handler for the original function.\n\n    Example::\n\n        >>> import operator\n        >>> operator.indexOf([1, 2, 3, 4, 5], 3)\n        2\n        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)\n        ... # xdoctest: +SKIP(\"Long tracebacks\")\n        Traceback (most recent call last):\n        ...\n        torch._dynamo.exc.Unsupported: ...\n\n        >>> @torch.compiler.substitute_in_graph(operator.indexOf)\n        ... def indexOf(a, b, /):\n        ...     for i, item in enumerate(a):\n        ...         if item is b or item == b:\n        ...             return i\n        ...     raise ValueError(\"sequence.index(x): x not in sequence\")\n        >>>\n        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)\n        2\n    ",
      "arguments": [
        "original_fn",
        "can_constant_fold_through",
        "skip_signature_check"
      ],
      "return_type": "typing.Callable[[typing.Callable[~_P, ~_R]], typing.Callable[~_P, ~_R]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Register a polyfill handler for a function, usually a C function from the C extension, to be\n    used in place of the original function when inlining the original function in the graph.\n\n    .. note::\n\n        The polyfill handler is only used when inlining the original function. It is not used when\n        the original function is called directly. In the eager mode, the decorated function calls\n        the performant C function rather than the polyfill handler.\n\n    The polyfill handler is a function that will be called in place of the original function when\n    inlining the original function. The polyfill handler should have the same signature and the same\n    behavior as the original function.\n\n    Args:\n        original_fn (callable): The original function, usually a C function, to register a polyfill\n            handler for.\n        can_constant_fold_through (bool, optional): Whether the polyfill handler can be constant\n            folded through. That is, if the polyfill handler is a pure function and its arguments\n            are constant, the result of the polyfill handler can be constant folded during the\n            compilation. Defaults to ``False``.\n        skip_signature_check (bool, optional): Whether to skip the signature check between the\n            original function and the polyfill handler. Defaults to ``False``.\n\n    Returns:\n        A decorator that registers the polyfill handler for the original function.\n\n    Example::\n\n        >>> import operator\n        >>> operator.indexOf([1, 2, 3, 4, 5], 3)\n        2\n        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)\n        ... # xdoctest: +SKIP(\"Long tracebacks\")\n        Traceback (most recent call last):\n        ...\n        torch._dynamo.exc.Unsupported: ...\n\n        >>> @torch.compiler.substitute_in_graph(operator.indexOf)\n        ... def indexOf(a, b, /):\n        ...     for i, item in enumerate(a):\n        ...         if item is b or item == b:\n        ...             return i\n        ...     raise ValueError(\"sequence.index(x): x not in sequence\")\n        >>>\n        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)\n        2\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.compiler.wrap_numpy",
      "signature": "torch.compiler.wrap_numpy(fn)",
      "doc": "Decorator that turns a function from ``np.ndarray``s to ``np.ndarray``s into a function\n    from ``torch.Tensor``s to ``torch.Tensor``s.\n\n    It is designed to be used with :func:`torch.compile` with ``fullgraph=True``. It allows to\n    compile a NumPy function as if it were a PyTorch function. This allows you to run NumPy code\n    on CUDA or compute its gradients.\n\n    .. note::\n\n        This decorator does not work without :func:`torch.compile`.\n\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> # Compile a NumPy function as a Tensor -> Tensor function\n        >>> @torch.compile(fullgraph=True)\n        >>> @torch.compiler.wrap_numpy\n        >>> def fn(a: np.ndarray):\n        >>>     return np.sum(a * a)\n        >>> # Execute the NumPy function using Tensors on CUDA and compute the gradients\n        >>> x = torch.arange(6, dtype=torch.float32, device=\"cuda\", requires_grad=True)\n        >>> out = fn(x)\n        >>> out.backward()\n        >>> print(x.grad)\n        tensor([ 0.,  2.,  4.,  6.,  8., 10.], device='cuda:0')\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Decorator that turns a function from ``np.ndarray``s to ``np.ndarray``s into a function\n    from ``torch.Tensor``s to ``torch.Tensor``s.\n\n    It is designed to be used with :func:`torch.compile` with ``fullgraph=True``. It allows to\n    compile a NumPy function as if it were a PyTorch function. This allows you to run NumPy code\n    on CUDA or compute its gradients.\n\n    .. note::\n\n        This decorator does not work without :func:`torch.compile`.\n\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> # Compile a NumPy function as a Tensor -> Tensor function\n        >>> @torch.compile(fullgraph=True)\n        >>> @torch.compiler.wrap_numpy\n        >>> def fn(a: np.ndarray):\n        >>>     return np.sum(a * a)\n        >>> # Execute the NumPy function using Tensors on CUDA and compute the gradients\n        >>> x = torch.arange(6, dtype=torch.float32, device=\"cuda\", requires_grad=True)\n        >>> out = fn(x)\n        >>> out.backward()\n        >>> print(x.grad)\n        tensor([ 0.,  2.,  4.,  6.,  8., 10.], device='cuda:0')\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "CPU_OPERATIONS": [
    {
      "function": "torch.cpu.current_device",
      "signature": "torch.cpu.current_device() -> str",
      "doc": "Returns current device for cpu. Always 'cpu'.\n\n    N.B. This function only exists to facilitate device-agnostic code\n    ",
      "arguments": [],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns current device for cpu. Always 'cpu'.\n\n    N.B. This function only exists to facilitate device-agnostic code\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cpu.current_stream",
      "signature": "torch.cpu.current_stream(device: Union[torch.device, str, int, NoneType] = None) -> torch.cpu.Stream",
      "doc": "Returns the currently selected :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): Ignored.\n\n    N.B. This function only exists to facilitate device-agnostic code\n\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.cpu.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the currently selected :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): Ignored.\n\n    N.B. This function only exists to facilitate device-agnostic code\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cpu.device_count",
      "signature": "torch.cpu.device_count() -> int",
      "doc": "Returns number of CPU devices (not cores). Always 1.\n\n    N.B. This function only exists to facilitate device-agnostic code\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns number of CPU devices (not cores). Always 1.\n\n    N.B. This function only exists to facilitate device-agnostic code\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cpu.is_available",
      "signature": "torch.cpu.is_available() -> bool",
      "doc": "Returns a bool indicating if CPU is currently available.\n\n    N.B. This function only exists to facilitate device-agnostic code\n\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns a bool indicating if CPU is currently available.\n\n    N.B. This function only exists to facilitate device-agnostic code\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cpu.set_device",
      "signature": "torch.cpu.set_device(device: Union[torch.device, str, int, NoneType]) -> None",
      "doc": "Sets the current device, in CPU we do nothing.\n\n    N.B. This function only exists to facilitate device-agnostic code\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the current device, in CPU we do nothing.\n\n    N.B. This function only exists to facilitate device-agnostic code\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cpu.stream",
      "signature": "torch.cpu.stream(stream: torch.cpu.Stream) -> contextlib.AbstractContextManager",
      "doc": "Wrapper around the Context-manager StreamContext that\n    selects a given stream.\n\n    N.B. This function only exists to facilitate device-agnostic code\n    ",
      "arguments": [
        "stream"
      ],
      "return_type": "<class 'contextlib.AbstractContextManager'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Wrapper around the Context-manager StreamContext that\n    selects a given stream.\n\n    N.B. This function only exists to facilitate device-agnostic code\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cpu.synchronize",
      "signature": "torch.cpu.synchronize(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Waits for all kernels in all streams on the CPU device to complete.\n\n    Args:\n        device (torch.device or int, optional): ignored, there's only one CPU device.\n\n    N.B. This function only exists to facilitate device-agnostic code.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Waits for all kernels in all streams on the CPU device to complete.\n\n    Args:\n        device (torch.device or int, optional): ignored, there's only one CPU device.\n\n    N.B. This function only exists to facilitate device-agnostic code.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "CUDA_OPERATIONS": [
    {
      "function": "torch.cuda.caching_allocator_alloc",
      "signature": "torch.cuda.caching_allocator_alloc(size, device: Union[torch.device, str, int, NoneType] = None, stream=None)",
      "doc": "Perform a memory allocation using the CUDA memory allocator.\n\n    Memory is allocated for a given device and a stream, this\n    function is intended to be used for interoperability with other\n    frameworks. Allocated memory is released through\n    :func:`~torch.cuda.caching_allocator_delete`.\n\n    Args:\n        size (int): number of bytes to be allocated.\n        device (torch.device or int, optional): selected device. If it is\n            ``None`` the default CUDA device is used.\n        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then\n            the default stream for the selected device is used.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "size",
        "device",
        "stream"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Perform a memory allocation using the CUDA memory allocator.\n\n    Memory is allocated for a given device and a stream, this\n    function is intended to be used for interoperability with other\n    frameworks. Allocated memory is released through\n    :func:`~torch.cuda.caching_allocator_delete`.\n\n    Args:\n        size (int): number of bytes to be allocated.\n        device (torch.device or int, optional): selected device. If it is\n            ``None`` the default CUDA device is used.\n        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then\n            the default stream for the selected device is used.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.caching_allocator_delete",
      "signature": "torch.cuda.caching_allocator_delete(mem_ptr)",
      "doc": "Delete memory allocated using the CUDA memory allocator.\n\n    Memory allocated with :func:`~torch.cuda.caching_allocator_alloc`.\n    is freed here. The associated device and stream are tracked inside\n    the allocator.\n\n    Args:\n        mem_ptr (int): memory address to be freed by the allocator.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "mem_ptr"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Delete memory allocated using the CUDA memory allocator.\n\n    Memory allocated with :func:`~torch.cuda.caching_allocator_alloc`.\n    is freed here. The associated device and stream are tracked inside\n    the allocator.\n\n    Args:\n        mem_ptr (int): memory address to be freed by the allocator.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.caching_allocator_enable",
      "signature": "torch.cuda.caching_allocator_enable(value: bool = True) -> None",
      "doc": "Enable or disable the CUDA memory allocator. On by default.",
      "arguments": [
        "value"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Enable or disable the CUDA memory allocator. On by default.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.can_device_access_peer",
      "signature": "torch.cuda.can_device_access_peer(device: Union[torch.device, str, int, NoneType], peer_device: Union[torch.device, str, int, NoneType]) -> bool",
      "doc": "Check if peer access between two devices is possible.",
      "arguments": [
        "device",
        "peer_device"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if peer access between two devices is possible.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.cast",
      "signature": "torch.cuda.cast(typ, val)",
      "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
      "arguments": [
        "typ",
        "val"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.change_current_allocator",
      "signature": "torch.cuda.change_current_allocator(allocator: torch.cuda.memory._CUDAAllocator) -> None",
      "doc": "Change the currently used memory allocator to be the one provided.\n\n    If the current allocator has already been used/initialized, this function will error.\n\n\n    Args:\n        allocator (torch.cuda.memory._CUDAAllocator): allocator to be set as the active one.\n    .. note::\n        See :ref:`cuda-memory-management` for details on creating and using a custom allocator\n    ",
      "arguments": [
        "allocator"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Change the currently used memory allocator to be the one provided.\n\n    If the current allocator has already been used/initialized, this function will error.\n\n\n    Args:\n        allocator (torch.cuda.memory._CUDAAllocator): allocator to be set as the active one.\n    .. note::\n        See :ref:`cuda-memory-management` for details on creating and using a custom allocator\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.check_error",
      "signature": "torch.cuda.check_error(res: int) -> None",
      "doc": "",
      "arguments": [
        "res"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.classproperty",
      "signature": "torch.cuda.classproperty(func)",
      "doc": "",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.clock_rate",
      "signature": "torch.cuda.clock_rate(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given by `nvidia-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given by `nvidia-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.cudart",
      "signature": "torch.cuda.cudart()",
      "doc": "Retrieves the CUDA runtime API module.\n\n\n    This function initializes the CUDA runtime environment if it is not already\n    initialized and returns the CUDA runtime API module (_cudart). The CUDA\n    runtime API module provides access to various CUDA runtime functions.\n\n    Args:\n        ``None``\n\n    Returns:\n        module: The CUDA runtime API module (_cudart).\n\n    Raises:\n        RuntimeError: If CUDA cannot be re-initialized in a forked subprocess.\n        AssertionError: If PyTorch is not compiled with CUDA support or if libcudart functions are unavailable.\n\n    Example of CUDA operations with profiling:\n        >>> import torch\n        >>> from torch.cuda import cudart, check_error\n        >>> import os\n        >>>\n        >>> os.environ['CUDA_PROFILE'] = '1'\n        >>>\n        >>> def perform_cuda_operations_with_streams():\n        >>>     stream = torch.cuda.Stream()\n        >>>     with torch.cuda.stream(stream):\n        >>>         x = torch.randn(100, 100, device='cuda')\n        >>>         y = torch.randn(100, 100, device='cuda')\n        >>>         z = torch.mul(x, y)\n        >>>     return z\n        >>>\n        >>> torch.cuda.synchronize()\n        >>> print(\"====== Start nsys profiling ======\")\n        >>> check_error(cudart().cudaProfilerStart())\n        >>> with torch.autograd.profiler.emit_nvtx():\n        >>>     result = perform_cuda_operations_with_streams()\n        >>>     print(\"CUDA operations completed.\")\n        >>> check_error(torch.cuda.cudart().cudaProfilerStop())\n        >>> print(\"====== End nsys profiling ======\")\n\n    To run this example and save the profiling information, execute:\n        >>> $ nvprof --profile-from-start off --csv --print-summary -o trace_name.prof -f -- python cudart_test.py\n\n    This command profiles the CUDA operations in the provided script and saves\n    the profiling information to a file named `trace_name.prof`.\n    The `--profile-from-start off` option ensures that profiling starts only\n    after the `cudaProfilerStart` call in the script.\n    The `--csv` and `--print-summary` options format the profiling output as a\n    CSV file and print a summary, respectively.\n    The `-o` option specifies the output file name, and the `-f` option forces the\n    overwrite of the output file if it already exists.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Retrieves the CUDA runtime API module.\n\n\n    This function initializes the CUDA runtime environment if it is not already\n    initialized and returns the CUDA runtime API module (_cudart). The CUDA\n    runtime API module provides access to various CUDA runtime functions.\n\n    Args:\n        ``None``\n\n    Returns:\n        module: The CUDA runtime API module (_cudart).\n\n    Raises:\n        RuntimeError: If CUDA cannot be re-initialized in a forked subprocess.\n        AssertionError: If PyTorch is not compiled with CUDA support or if libcudart functions are unavailable.\n\n    Example of CUDA operations with profiling:\n        >>> import torch\n        >>> from torch.cuda import cudart, check_error\n        >>> import os\n        >>>\n        >>> os.environ['CUDA_PROFILE'] = '1'\n        >>>\n        >>> def perform_cuda_operations_with_streams():\n        >>>     stream = torch.cuda.Stream()\n        >>>     with torch.cuda.stream(stream):\n        >>>         x = torch.randn(100, 100, device='cuda')\n        >>>         y = torch.randn(100, 100, device='cuda')\n        >>>         z = torch.mul(x, y)\n        >>>     return z\n        >>>\n        >>> torch.cuda.synchronize()\n        >>> print(\"====== Start nsys profiling ======\")\n        >>> check_error(cudart().cudaProfilerStart())\n        >>> with torch.autograd.profiler.emit_nvtx():\n        >>>     result = perform_cuda_operations_with_streams()\n        >>>     print(\"CUDA operations completed.\")\n        >>> check_error(torch.cuda.cudart().cudaProfilerStop())\n        >>> print(\"====== End nsys profiling ======\")\n\n    To run this example and save the profiling information, execute:\n        >>> $ nvprof --profile-from-start off --csv --print-summary -o trace_name.prof -f -- python cudart_test.py\n\n    This command profiles the CUDA operations in the provided script and saves\n    the profiling information to a file named `trace_name.prof`.\n    The `--profile-from-start off` option ensures that profiling starts only\n    after the `cudaProfilerStart` call in the script.\n    The `--csv` and `--print-summary` options format the profiling output as a\n    CSV file and print a summary, respectively.\n    The `-o` option specifies the output file name, and the `-f` option forces the\n    overwrite of the output file if it already exists.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.current_blas_handle",
      "signature": "torch.cuda.current_blas_handle()",
      "doc": "Return cublasHandle_t pointer to current cuBLAS handle",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return cublasHandle_t pointer to current cuBLAS handle",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.current_device",
      "signature": "torch.cuda.current_device() -> int",
      "doc": "Return the index of a currently selected device.",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the index of a currently selected device.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.current_stream",
      "signature": "torch.cuda.current_stream(device: Union[torch.device, str, int, NoneType] = None) -> torch.cuda.streams.Stream",
      "doc": "Return the currently selected :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the currently selected :class:`Stream` for the current device, given\n            by :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.cuda.streams.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the currently selected :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the currently selected :class:`Stream` for the current device, given\n            by :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.default_stream",
      "signature": "torch.cuda.default_stream(device: Union[torch.device, str, int, NoneType] = None) -> torch.cuda.streams.Stream",
      "doc": "Return the default :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the default :class:`Stream` for the current device, given by\n            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.cuda.streams.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the default :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the default :class:`Stream` for the current device, given by\n            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.device_count",
      "signature": "torch.cuda.device_count() -> int",
      "doc": "\n    Return the number of GPUs available.\n\n    .. note:: This API will NOT posion fork if NVML discovery succeeds.\n        See :ref:`multiprocessing-poison-fork-note` for more details.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return the number of GPUs available.\n\n    .. note:: This API will NOT posion fork if NVML discovery succeeds.\n        See :ref:`multiprocessing-poison-fork-note` for more details.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.device_memory_used",
      "signature": "torch.cuda.device_memory_used(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return used global (device) memory in bytes as given by `nvidia-smi` or `amd-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return used global (device) memory in bytes as given by `nvidia-smi` or `amd-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.empty_cache",
      "signature": "torch.cuda.empty_cache() -> None",
      "doc": "Release all unoccupied cached memory currently held by the caching\n    allocator so that those can be used in other GPU application and visible in\n    `nvidia-smi`.\n\n    .. note::\n        :func:`~torch.cuda.empty_cache` doesn't increase the amount of GPU\n        memory available for PyTorch. However, it may help reduce fragmentation\n        of GPU memory in certain cases. See :ref:`cuda-memory-management` for\n        more details about GPU memory management.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Release all unoccupied cached memory currently held by the caching\n    allocator so that those can be used in other GPU application and visible in\n    `nvidia-smi`.\n\n    .. note::\n        :func:`~torch.cuda.empty_cache` doesn't increase the amount of GPU\n        memory available for PyTorch. However, it may help reduce fragmentation\n        of GPU memory in certain cases. See :ref:`cuda-memory-management` for\n        more details about GPU memory management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_allocator_backend",
      "signature": "torch.cuda.get_allocator_backend() -> str",
      "doc": "Return a string describing the active allocator backend as set by\n    ``PYTORCH_CUDA_ALLOC_CONF``. Currently available backends are\n    ``native`` (PyTorch's native caching allocator) and `cudaMallocAsync``\n    (CUDA's built-in asynchronous allocator).\n\n    .. note::\n        See :ref:`cuda-memory-management` for details on choosing the allocator backend.\n    ",
      "arguments": [],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a string describing the active allocator backend as set by\n    ``PYTORCH_CUDA_ALLOC_CONF``. Currently available backends are\n    ``native`` (PyTorch's native caching allocator) and `cudaMallocAsync``\n    (CUDA's built-in asynchronous allocator).\n\n    .. note::\n        See :ref:`cuda-memory-management` for details on choosing the allocator backend.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_arch_list",
      "signature": "torch.cuda.get_arch_list() -> list[str]",
      "doc": "Return list CUDA architectures this library was compiled for.",
      "arguments": [],
      "return_type": "list[str]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return list CUDA architectures this library was compiled for.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_device_capability",
      "signature": "torch.cuda.get_device_capability(device: Union[torch.device, str, int, NoneType] = None) -> tuple[int, int]",
      "doc": "Get the cuda capability of a device.\n\n    Args:\n        device (torch.device or int or str, optional): device for which to return the\n            device capability. This function is a no-op if this argument is\n            a negative integer. It uses the current device, given by\n            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n            (default).\n\n    Returns:\n        tuple(int, int): the major and minor cuda capability of the device\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "tuple[int, int]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get the cuda capability of a device.\n\n    Args:\n        device (torch.device or int or str, optional): device for which to return the\n            device capability. This function is a no-op if this argument is\n            a negative integer. It uses the current device, given by\n            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n            (default).\n\n    Returns:\n        tuple(int, int): the major and minor cuda capability of the device\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_device_name",
      "signature": "torch.cuda.get_device_name(device: Union[torch.device, str, int, NoneType] = None) -> str",
      "doc": "Get the name of a device.\n\n    Args:\n        device (torch.device or int or str, optional): device for which to return the\n            name. This function is a no-op if this argument is a negative\n            integer. It uses the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Returns:\n        str: the name of the device\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get the name of a device.\n\n    Args:\n        device (torch.device or int or str, optional): device for which to return the\n            name. This function is a no-op if this argument is a negative\n            integer. It uses the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Returns:\n        str: the name of the device\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_device_properties",
      "signature": "torch.cuda.get_device_properties(device: Union[torch.device, str, int, NoneType] = None) -> torch._utils._CudaDeviceProperties",
      "doc": "Get the properties of a device.\n\n    Args:\n        device (torch.device or int or str, optional): device for which to return the\n            properties of the device.  It uses the current device, given by\n            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n            (default).\n\n    Returns:\n        _CudaDeviceProperties: the properties of the device\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch._utils._CudaDeviceProperties'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get the properties of a device.\n\n    Args:\n        device (torch.device or int or str, optional): device for which to return the\n            properties of the device.  It uses the current device, given by\n            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n            (default).\n\n    Returns:\n        _CudaDeviceProperties: the properties of the device\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_gencode_flags",
      "signature": "torch.cuda.get_gencode_flags() -> str",
      "doc": "Return NVCC gencode flags this library was compiled with.",
      "arguments": [],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return NVCC gencode flags this library was compiled with.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_per_process_memory_fraction",
      "signature": "torch.cuda.get_per_process_memory_fraction(device: Union[torch.device, str, int, NoneType] = None) -> float",
      "doc": "Get memory fraction for a process.\n\n    Args:\n        device (torch.device or int, optional): selected device. If it is\n            ``None`` the default CUDA device is used.\n    Returns:\n        memory fraction, in range 0~1. Allowed memory equals total_memory * fraction.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'float'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get memory fraction for a process.\n\n    Args:\n        device (torch.device or int, optional): selected device. If it is\n            ``None`` the default CUDA device is used.\n    Returns:\n        memory fraction, in range 0~1. Allowed memory equals total_memory * fraction.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_rng_state",
      "signature": "torch.cuda.get_rng_state(device: Union[int, str, torch.device] = 'cuda') -> torch.Tensor",
      "doc": "Return the random number generator state of the specified GPU as a ByteTensor.\n\n    Args:\n        device (torch.device or int, optional): The device to return the RNG state of.\n            Default: ``'cuda'`` (i.e., ``torch.device('cuda')``, the current CUDA device).\n\n    .. warning::\n        This function eagerly initializes CUDA.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the random number generator state of the specified GPU as a ByteTensor.\n\n    Args:\n        device (torch.device or int, optional): The device to return the RNG state of.\n            Default: ``'cuda'`` (i.e., ``torch.device('cuda')``, the current CUDA device).\n\n    .. warning::\n        This function eagerly initializes CUDA.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_rng_state_all",
      "signature": "torch.cuda.get_rng_state_all() -> list[torch.Tensor]",
      "doc": "Return a list of ByteTensor representing the random number states of all devices.",
      "arguments": [],
      "return_type": "list[torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a list of ByteTensor representing the random number states of all devices.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_stream_from_external",
      "signature": "torch.cuda.get_stream_from_external(data_ptr: int, device: Union[torch.device, str, int, NoneType] = None) -> torch.cuda.streams.Stream",
      "doc": "Return a :class:`Stream` from an externally allocated CUDA stream.\n\n    This function is used to wrap streams allocated in other libraries in order\n    to facilitate data exchange and multi-library interactions.\n\n    .. note:: This function doesn't manage the stream life-cycle, it is the user\n       responsibility to keep the referenced stream alive while this returned\n       stream is being used.\n\n    Args:\n        data_ptr(int): Integer representation of the `cudaStream_t` value that\n            is allocated externally.\n        device(torch.device or int, optional): the device where the stream\n            was originally allocated. If device is specified incorrectly,\n            subsequent launches using this stream may fail.\n    ",
      "arguments": [
        "data_ptr",
        "device"
      ],
      "return_type": "<class 'torch.cuda.streams.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a :class:`Stream` from an externally allocated CUDA stream.\n\n    This function is used to wrap streams allocated in other libraries in order\n    to facilitate data exchange and multi-library interactions.\n\n    .. note:: This function doesn't manage the stream life-cycle, it is the user\n       responsibility to keep the referenced stream alive while this returned\n       stream is being used.\n\n    Args:\n        data_ptr(int): Integer representation of the `cudaStream_t` value that\n            is allocated externally.\n        device(torch.device or int, optional): the device where the stream\n            was originally allocated. If device is specified incorrectly,\n            subsequent launches using this stream may fail.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.get_sync_debug_mode",
      "signature": "torch.cuda.get_sync_debug_mode() -> int",
      "doc": "Return current value of debug mode for cuda synchronizing operations.",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return current value of debug mode for cuda synchronizing operations.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.graph_pool_handle",
      "signature": "torch.cuda.graph_pool_handle()",
      "doc": "Return an opaque token representing the id of a graph memory pool.\n\n    See :ref:`Graph memory management<graph-memory-management>`.\n\n    .. warning::\n        This API is in beta and may change in future releases.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return an opaque token representing the id of a graph memory pool.\n\n    See :ref:`Graph memory management<graph-memory-management>`.\n\n    .. warning::\n        This API is in beta and may change in future releases.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.host_memory_stats",
      "signature": "torch.cuda.host_memory_stats() -> dict[str, typing.Any]",
      "doc": "Return a dictionary of CUDA memory allocator statistics for a given device.\n\n     The return value of this function is a dictionary of statistics, each of\n     which is a non-negative integer.\n\n     Core statistics:\n\n     - ``\"allocated.{current,peak,allocated,freed}\"``:\n       number of allocation requests received by the memory allocator.\n     - ``\"allocated_bytes.{current,peak,allocated,freed}\"``:\n       amount of allocated memory.\n     - ``\"segment.{current,peak,allocated,freed}\"``:\n       number of reserved segments from ``cudaMalloc()``.\n     - ``\"reserved_bytes.{current,peak,allocated,freed}\"``:\n       amount of reserved memory.\n\n     For these core statistics, values are broken down as follows.\n\n     Metric type:\n\n     - ``current``: current value of this metric.\n     - ``peak``: maximum value of this metric.\n     - ``allocated``: historical total increase in this metric.\n     - ``freed``: historical total decrease in this metric.\n\n     In addition to the core statistics, we also provide some simple event\n     counters:\n\n     - ``\"num_host_alloc\"``: number of CUDA allocation calls. This includes both\n       cudaHostAlloc and cudaHostRegister.\n     - ``\"num_host_free\"``: number of CUDA free calls. This includes both cudaHostFree\n       and cudaHostUnregister.\n\n     Finally, we also provide some simple timing counters:\n\n     - ``\"host_alloc_time.{total,max,min,count,avg}\"``:\n       timing of allocation requests going through CUDA calls.\n     - ``\"host_free_time.{total,max,min,count,avg}\"``:\n       timing of free requests going through CUDA calls.\n\n    For these timing statistics, values are broken down as follows.\n\n     Metric type:\n\n     - ``total``: total time spent.\n     - ``max``: maximum value per call.\n     - ``min``: minimum value per call.\n     - ``count``: number of times it was called.\n     - ``avg``: average time per call.\n    ",
      "arguments": [],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a dictionary of CUDA memory allocator statistics for a given device.\n\n     The return value of this function is a dictionary of statistics, each of\n     which is a non-negative integer.\n\n     Core statistics:\n\n     - ``\"allocated.{current,peak,allocated,freed}\"``:\n       number of allocation requests received by the memory allocator.\n     - ``\"allocated_bytes.{current,peak,allocated,freed}\"``:\n       amount of allocated memory.\n     - ``\"segment.{current,peak,allocated,freed}\"``:\n       number of reserved segments from ``cudaMalloc()``.\n     - ``\"reserved_bytes.{current,peak,allocated,freed}\"``:\n       amount of reserved memory.\n\n     For these core statistics, values are broken down as follows.\n\n     Metric type:\n\n     - ``current``: current value of this metric.\n     - ``peak``: maximum value of this metric.\n     - ``allocated``: historical total increase in this metric.\n     - ``freed``: historical total decrease in this metric.\n\n     In addition to the core statistics, we also provide some simple event\n     counters:\n\n     - ``\"num_host_alloc\"``: number of CUDA allocation calls. This includes both\n       cudaHostAlloc and cudaHostRegister.\n     - ``\"num_host_free\"``: number of CUDA free calls. This includes both cudaHostFree\n       and cudaHostUnregister.\n\n     Finally, we also provide some simple timing counters:\n\n     - ``\"host_alloc_time.{total,max,min,count,avg}\"``:\n       timing of allocation requests going through CUDA calls.\n     - ``\"host_free_time.{total,max,min,count,avg}\"``:\n       timing of free requests going through CUDA calls.\n\n    For these timing statistics, values are broken down as follows.\n\n     Metric type:\n\n     - ``total``: total time spent.\n     - ``max``: maximum value per call.\n     - ``min``: minimum value per call.\n     - ``count``: number of times it was called.\n     - ``avg``: average time per call.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.host_memory_stats_as_nested_dict",
      "signature": "torch.cuda.host_memory_stats_as_nested_dict() -> dict[str, typing.Any]",
      "doc": "Return the result of :func:`~torch.cuda.host_memory_stats` as a nested dictionary.",
      "arguments": [],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the result of :func:`~torch.cuda.host_memory_stats` as a nested dictionary.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.init",
      "signature": "torch.cuda.init()",
      "doc": "Initialize PyTorch's CUDA state.\n\n    You may need to call this explicitly if you are interacting with\n    PyTorch via its C API, as Python bindings for CUDA functionality\n    will not be available until this initialization takes place.\n    Ordinary users should not need this, as all of PyTorch's CUDA methods\n    automatically initialize CUDA state on-demand.\n\n    Does nothing if the CUDA state is already initialized.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Initialize PyTorch's CUDA state.\n\n    You may need to call this explicitly if you are interacting with\n    PyTorch via its C API, as Python bindings for CUDA functionality\n    will not be available until this initialization takes place.\n    Ordinary users should not need this, as all of PyTorch's CUDA methods\n    automatically initialize CUDA state on-demand.\n\n    Does nothing if the CUDA state is already initialized.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.initial_seed",
      "signature": "torch.cuda.initial_seed() -> int",
      "doc": "Return the current random seed of the current GPU.\n\n    .. warning::\n        This function eagerly initializes CUDA.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the current random seed of the current GPU.\n\n    .. warning::\n        This function eagerly initializes CUDA.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.ipc_collect",
      "signature": "torch.cuda.ipc_collect()",
      "doc": "Force collects GPU memory after it has been released by CUDA IPC.\n\n    .. note::\n        Checks if any sent CUDA tensors could be cleaned from the memory. Force\n        closes shared memory file used for reference counting if there is no\n        active counters. Useful when the producer process stopped actively sending\n        tensors and want to release unused memory.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Force collects GPU memory after it has been released by CUDA IPC.\n\n    .. note::\n        Checks if any sent CUDA tensors could be cleaned from the memory. Force\n        closes shared memory file used for reference counting if there is no\n        active counters. Useful when the producer process stopped actively sending\n        tensors and want to release unused memory.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.is_available",
      "signature": "torch.cuda.is_available() -> bool",
      "doc": "\n    Return a bool indicating if CUDA is currently available.\n\n    .. note:: This function will NOT poison fork if the environment variable\n        ``PYTORCH_NVML_BASED_CUDA_CHECK=1`` is set. For more details, see\n        :ref:`multiprocessing-poison-fork-note`.\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return a bool indicating if CUDA is currently available.\n\n    .. note:: This function will NOT poison fork if the environment variable\n        ``PYTORCH_NVML_BASED_CUDA_CHECK=1`` is set. For more details, see\n        :ref:`multiprocessing-poison-fork-note`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.is_bf16_supported",
      "signature": "torch.cuda.is_bf16_supported(including_emulation: bool = True)",
      "doc": "Return a bool indicating if the current CUDA/ROCm device supports dtype bfloat16.",
      "arguments": [
        "including_emulation"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a bool indicating if the current CUDA/ROCm device supports dtype bfloat16.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.is_current_stream_capturing",
      "signature": "torch.cuda.is_current_stream_capturing()",
      "doc": "Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\n\n    If a CUDA context does not exist on the current device, returns False without initializing the context.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\n\n    If a CUDA context does not exist on the current device, returns False without initializing the context.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.is_initialized",
      "signature": "torch.cuda.is_initialized()",
      "doc": "Return whether PyTorch's CUDA state has been initialized.",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return whether PyTorch's CUDA state has been initialized.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.is_tf32_supported",
      "signature": "torch.cuda.is_tf32_supported() -> bool",
      "doc": "Return a bool indicating if the current CUDA/ROCm device supports dtype tf32.",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a bool indicating if the current CUDA/ROCm device supports dtype tf32.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.list_gpu_processes",
      "signature": "torch.cuda.list_gpu_processes(device: Union[torch.device, str, int, NoneType] = None) -> str",
      "doc": "Return a human-readable printout of the running processes and their GPU memory use for a given device.\n\n    This can be useful to display periodically during training, or when\n    handling out-of-memory exceptions.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            printout for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a human-readable printout of the running processes and their GPU memory use for a given device.\n\n    This can be useful to display periodically during training, or when\n    handling out-of-memory exceptions.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            printout for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.lru_cache",
      "signature": "torch.cuda.lru_cache(maxsize=128, typed=False)",
      "doc": "Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    ",
      "arguments": [
        "maxsize",
        "typed"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.make_graphed_callables",
      "signature": "torch.cuda.make_graphed_callables(callables, sample_args, num_warmup_iters=3, allow_unused_input=False, pool=None)",
      "doc": "Accept callables (functions or :class:`nn.Module<torch.nn.Module>`\\ s) and returns graphed versions.\n\n    Each graphed callable's forward pass runs its source callable's\n    forward CUDA work as a CUDA graph inside a single autograd node.\n\n    The graphed callable's forward pass also appends\n    a backward node to the autograd graph. During backward, this node runs the\n    callable's backward work as a CUDA graph.\n\n    Therefore, each graphed callable should be a drop-in replacement for its source callable\n    in an autograd-enabled training loop.\n\n    See :ref:`Partial-network capture<partial-network-capture>` for detailed use and constraints.\n\n    If you pass a tuple of several callables, their captures will use the same memory pool.\n    See :ref:`Graph memory management<graph-memory-management>` for when this is appropriate.\n\n    Arguments:\n        callables (torch.nn.Module or Python function, or tuple of these): Callable or callables to graph.\n            See :ref:`Graph memory management<graph-memory-management>` for when passing a tuple of callables\n            is appropriate.  If you pass a tuple of callables, their order in the tuple must be the same order\n            they'll run in the live workload.\n        sample_args (tuple of Tensors, or tuple of tuples of Tensors): Samples args for each callable.\n            If a single callable was passed, ``sample_args`` must be a single tuple of argument Tensors.\n            If a tuple of callables was passed, ``sample_args`` must be tuple of tuples of argument Tensors.\n        num_warmup_iters (int): The number of warmup iterations. Currently, ``DataDistributedParallel`` needs\n            11 iterations for warm up. Default: ``3``.\n        allow_unused_input (bool): If False, specifying inputs that were not used when computing outputs\n            (and therefore their grad is always zero) is an error. Defaults to False.\n        pool (optional): Token (returned by :func:`~torch.cuda.graph_pool_handle` or\n            :meth:`other_Graph_instance.pool()<torch.cuda.CUDAGraph.pool>`) that hints this graph may share memory\n            with the indicated pool.  See :ref:`Graph memory management<graph-memory-management>`.\n    .. note::\n        The ``requires_grad`` state of each Tensor in ``sample_args`` must match the state\n        that's expected for the corresponding real input in the training loop.\n\n    .. warning::\n        This API is in beta and may change in future releases.\n\n    .. warning::\n        ``sample_args`` for each callable must contain only Tensors. Other types are not allowed.\n\n    .. warning::\n        Returned callables do not support higher order differentiation (e.g., double backward).\n\n    .. warning::\n        In any :class:`~torch.nn.Module` passed to :func:`~make_graphed_callables`, only parameters\n        may be trainable. Buffers must have ``requires_grad=False``.\n\n    .. warning::\n        After you pass a :class:`torch.nn.Module` through :func:`~make_graphed_callables`,\n        you may not add or remove any of that Module's parameters or buffers.\n\n    .. warning::\n        :class:`torch.nn.Module`\\s passed to :func:`~torch.cuda.make_graphed_callables` must not have module hooks\n        registered on them at the time they are passed. However, registering hooks on modules *after* passing them\n        through :func:`~torch.cuda.make_graphed_callables` is allowed.\n\n    .. warning::\n        When running a graphed callable, you must pass its arguments in the same order and format\n        they appeared in that callable's ``sample_args``.\n\n    .. warning::\n        The automatic mixed precision is supported in :func:`~torch.cuda.make_graphed_callables` only with disabled\n        caching. The context manager `torch.cuda.amp.autocast()` must have `cache_enabled=False`.\n    ",
      "arguments": [
        "callables",
        "sample_args",
        "num_warmup_iters",
        "allow_unused_input",
        "pool"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Accept callables (functions or :class:`nn.Module<torch.nn.Module>`\\ s) and returns graphed versions.\n\n    Each graphed callable's forward pass runs its source callable's\n    forward CUDA work as a CUDA graph inside a single autograd node.\n\n    The graphed callable's forward pass also appends\n    a backward node to the autograd graph. During backward, this node runs the\n    callable's backward work as a CUDA graph.\n\n    Therefore, each graphed callable should be a drop-in replacement for its source callable\n    in an autograd-enabled training loop.\n\n    See :ref:`Partial-network capture<partial-network-capture>` for detailed use and constraints.\n\n    If you pass a tuple of several callables, their captures will use the same memory pool.\n    See :ref:`Graph memory management<graph-memory-management>` for when this is appropriate.\n\n    Arguments:\n        callables (torch.nn.Module or Python function, or tuple of these): Callable or callables to graph.\n            See :ref:`Graph memory management<graph-memory-management>` for when passing a tuple of callables\n            is appropriate.  If you pass a tuple of callables, their order in the tuple must be the same order\n            they'll run in the live workload.\n        sample_args (tuple of Tensors, or tuple of tuples of Tensors): Samples args for each callable.\n            If a single callable was passed, ``sample_args`` must be a single tuple of argument Tensors.\n            If a tuple of callables was passed, ``sample_args`` must be tuple of tuples of argument Tensors.\n        num_warmup_iters (int): The number of warmup iterations. Currently, ``DataDistributedParallel`` needs\n            11 iterations for warm up. Default: ``3``.\n        allow_unused_input (bool): If False, specifying inputs that were not used when computing outputs\n            (and therefore their grad is always zero) is an error. Defaults to False.\n        pool (optional): Token (returned by :func:`~torch.cuda.graph_pool_handle` or\n            :meth:`other_Graph_instance.pool()<torch.cuda.CUDAGraph.pool>`) that hints this graph may share memory\n            with the indicated pool.  See :ref:`Graph memory management<graph-memory-management>`.\n    .. note::\n        The ``requires_grad`` state of each Tensor in ``sample_args`` must match the state\n        that's expected for the corresponding real input in the training loop.\n\n    .. warning::\n        This API is in beta and may change in future releases.\n\n    .. warning::\n        ``sample_args`` for each callable must contain only Tensors. Other types are not allowed.\n\n    .. warning::\n        Returned callables do not support higher order differentiation (e.g., double backward).\n\n    .. warning::\n        In any :class:`~torch.nn.Module` passed to :func:`~make_graphed_callables`, only parameters\n        may be trainable. Buffers must have ``requires_grad=False``.\n\n    .. warning::\n        After you pass a :class:`torch.nn.Module` through :func:`~make_graphed_callables`,\n        you may not add or remove any of that Module's parameters or buffers.\n\n    .. warning::\n        :class:`torch.nn.Module`\\s passed to :func:`~torch.cuda.make_graphed_callables` must not have module hooks\n        registered on them at the time they are passed. However, registering hooks on modules *after* passing them\n        through :func:`~torch.cuda.make_graphed_callables` is allowed.\n\n    .. warning::\n        When running a graphed callable, you must pass its arguments in the same order and format\n        they appeared in that callable's ``sample_args``.\n\n    .. warning::\n        The automatic mixed precision is supported in :func:`~torch.cuda.make_graphed_callables` only with disabled\n        caching. The context manager `torch.cuda.amp.autocast()` must have `cache_enabled=False`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.manual_seed",
      "signature": "torch.cuda.manual_seed(seed: int) -> None",
      "doc": "Set the seed for generating random numbers for the current GPU.\n\n    It's safe to call this function if CUDA is not available; in that\n    case, it is silently ignored.\n\n    Args:\n        seed (int): The desired seed.\n\n    .. warning::\n        If you are working with a multi-GPU model, this function is insufficient\n        to get determinism.  To seed all GPUs, use :func:`manual_seed_all`.\n    ",
      "arguments": [
        "seed"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the seed for generating random numbers for the current GPU.\n\n    It's safe to call this function if CUDA is not available; in that\n    case, it is silently ignored.\n\n    Args:\n        seed (int): The desired seed.\n\n    .. warning::\n        If you are working with a multi-GPU model, this function is insufficient\n        to get determinism.  To seed all GPUs, use :func:`manual_seed_all`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.manual_seed_all",
      "signature": "torch.cuda.manual_seed_all(seed: int) -> None",
      "doc": "Set the seed for generating random numbers on all GPUs.\n\n    It's safe to call this function if CUDA is not available; in that\n    case, it is silently ignored.\n\n    Args:\n        seed (int): The desired seed.\n    ",
      "arguments": [
        "seed"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the seed for generating random numbers on all GPUs.\n\n    It's safe to call this function if CUDA is not available; in that\n    case, it is silently ignored.\n\n    Args:\n        seed (int): The desired seed.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.max_memory_allocated",
      "signature": "torch.cuda.max_memory_allocated(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the maximum GPU memory occupied by tensors in bytes for a given device.\n\n    By default, this returns the peak allocated memory since the beginning of\n    this program. :func:`~torch.cuda.reset_peak_memory_stats` can be used to\n    reset the starting point in tracking this metric. For example, these two\n    functions can measure the peak allocated memory usage of each iteration in a\n    training loop.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the maximum GPU memory occupied by tensors in bytes for a given device.\n\n    By default, this returns the peak allocated memory since the beginning of\n    this program. :func:`~torch.cuda.reset_peak_memory_stats` can be used to\n    reset the starting point in tracking this metric. For example, these two\n    functions can measure the peak allocated memory usage of each iteration in a\n    training loop.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.max_memory_cached",
      "signature": "torch.cuda.max_memory_cached(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Deprecated; see :func:`~torch.cuda.max_memory_reserved`.",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Deprecated; see :func:`~torch.cuda.max_memory_reserved`.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.max_memory_reserved",
      "signature": "torch.cuda.max_memory_reserved(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the maximum GPU memory managed by the caching allocator in bytes for a given device.\n\n    By default, this returns the peak cached memory since the beginning of this\n    program. :func:`~torch.cuda.reset_peak_memory_stats` can be used to reset\n    the starting point in tracking this metric. For example, these two functions\n    can measure the peak cached memory amount of each iteration in a training\n    loop.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the maximum GPU memory managed by the caching allocator in bytes for a given device.\n\n    By default, this returns the peak cached memory since the beginning of this\n    program. :func:`~torch.cuda.reset_peak_memory_stats` can be used to reset\n    the starting point in tracking this metric. For example, these two functions\n    can measure the peak cached memory amount of each iteration in a training\n    loop.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.mem_get_info",
      "signature": "torch.cuda.mem_get_info(device: Union[torch.device, str, int, NoneType] = None) -> tuple[int, int]",
      "doc": "Return the global free and total GPU memory for a given device using cudaMemGetInfo.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default) or if the device index is not specified.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more\n        details about GPU memory management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "tuple[int, int]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the global free and total GPU memory for a given device using cudaMemGetInfo.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default) or if the device index is not specified.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more\n        details about GPU memory management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.memory_allocated",
      "signature": "torch.cuda.memory_allocated(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the current GPU memory occupied by tensors in bytes for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        This is likely less than the amount shown in `nvidia-smi` since some\n        unused memory can be held by the caching allocator and some context\n        needs to be created on GPU. See :ref:`cuda-memory-management` for more\n        details about GPU memory management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the current GPU memory occupied by tensors in bytes for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        This is likely less than the amount shown in `nvidia-smi` since some\n        unused memory can be held by the caching allocator and some context\n        needs to be created on GPU. See :ref:`cuda-memory-management` for more\n        details about GPU memory management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.memory_cached",
      "signature": "torch.cuda.memory_cached(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Deprecated; see :func:`~torch.cuda.memory_reserved`.",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Deprecated; see :func:`~torch.cuda.memory_reserved`.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.memory_reserved",
      "signature": "torch.cuda.memory_reserved(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the current GPU memory managed by the caching allocator in bytes for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the current GPU memory managed by the caching allocator in bytes for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.memory_snapshot",
      "signature": "torch.cuda.memory_snapshot()",
      "doc": "Return a snapshot of the CUDA memory allocator state across all devices.\n\n    Interpreting the output of this function requires familiarity with the\n    memory allocator internals.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a snapshot of the CUDA memory allocator state across all devices.\n\n    Interpreting the output of this function requires familiarity with the\n    memory allocator internals.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.memory_stats",
      "signature": "torch.cuda.memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> dict[str, typing.Any]",
      "doc": "Return a dictionary of CUDA memory allocator statistics for a given device.\n\n    The return value of this function is a dictionary of statistics, each of\n    which is a non-negative integer.\n\n    Core statistics:\n\n    - ``\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      number of allocation requests received by the memory allocator.\n    - ``\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of allocated memory.\n    - ``\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      number of reserved segments from ``cudaMalloc()``.\n    - ``\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of reserved memory.\n    - ``\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      number of active memory blocks.\n    - ``\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of active memory.\n    - ``\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      number of inactive, non-releasable memory blocks.\n    - ``\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of inactive, non-releasable memory.\n\n    For these core statistics, values are broken down as follows.\n\n    Pool type:\n\n    - ``all``: combined statistics across all memory pools.\n    - ``large_pool``: statistics for the large allocation pool\n      (as of October 2019, for size >= 1MB allocations).\n    - ``small_pool``: statistics for the small allocation pool\n      (as of October 2019, for size < 1MB allocations).\n\n    Metric type:\n\n    - ``current``: current value of this metric.\n    - ``peak``: maximum value of this metric.\n    - ``allocated``: historical total increase in this metric.\n    - ``freed``: historical total decrease in this metric.\n\n    In addition to the core statistics, we also provide some simple event\n    counters:\n\n    - ``\"num_alloc_retries\"``: number of failed ``cudaMalloc`` calls that\n      result in a cache flush and retry.\n    - ``\"num_ooms\"``: number of out-of-memory errors thrown.\n    - ``\"num_sync_all_streams\"``: number of ``synchronize_and_free_events`` calls.\n    - ``\"num_device_alloc\"``: number of CUDA allocation calls. This includes both\n      cuMemMap and cudaMalloc.\n    - ``\"num_device_free\"``: number of CUDA free calls. This includes both cuMemUnmap\n      and cudaFree.\n\n    The caching allocator can be configured via ENV to not split blocks larger than a\n    defined size (see Memory Management section of the Cuda Semantics documentation).\n    This helps avoid memory fragmentation but may have a performance\n    penalty. Additional outputs to assist with tuning and evaluating impact:\n\n    - ``\"max_split_size\"``: blocks above this size will not be split.\n    - ``\"oversize_allocations.{current,peak,allocated,freed}\"``:\n      number of over-size allocation requests received by the memory allocator.\n    - ``\"oversize_segments.{current,peak,allocated,freed}\"``:\n      number of over-size reserved segments from ``cudaMalloc()``.\n\n    The caching allocator can be configured via ENV to round memory allocations in order\n    to reduce fragmentation. Sometimes the overhead from rounding can be higher than\n    the fragmentation it helps reduce. The following stat can be used to check if\n    rounding adds too much overhead:\n\n    - ``\"requested_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      memory requested by client code, compare this with allocated_bytes to check if\n      allocation rounding adds too much overhead.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistics for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n\n    .. note::\n        With :ref:`backend:cudaMallocAsync<cuda-memory-envvars>`, some stats are not\n        meaningful, and are always reported as zero.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a dictionary of CUDA memory allocator statistics for a given device.\n\n    The return value of this function is a dictionary of statistics, each of\n    which is a non-negative integer.\n\n    Core statistics:\n\n    - ``\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      number of allocation requests received by the memory allocator.\n    - ``\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of allocated memory.\n    - ``\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      number of reserved segments from ``cudaMalloc()``.\n    - ``\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of reserved memory.\n    - ``\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      number of active memory blocks.\n    - ``\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of active memory.\n    - ``\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      number of inactive, non-releasable memory blocks.\n    - ``\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of inactive, non-releasable memory.\n\n    For these core statistics, values are broken down as follows.\n\n    Pool type:\n\n    - ``all``: combined statistics across all memory pools.\n    - ``large_pool``: statistics for the large allocation pool\n      (as of October 2019, for size >= 1MB allocations).\n    - ``small_pool``: statistics for the small allocation pool\n      (as of October 2019, for size < 1MB allocations).\n\n    Metric type:\n\n    - ``current``: current value of this metric.\n    - ``peak``: maximum value of this metric.\n    - ``allocated``: historical total increase in this metric.\n    - ``freed``: historical total decrease in this metric.\n\n    In addition to the core statistics, we also provide some simple event\n    counters:\n\n    - ``\"num_alloc_retries\"``: number of failed ``cudaMalloc`` calls that\n      result in a cache flush and retry.\n    - ``\"num_ooms\"``: number of out-of-memory errors thrown.\n    - ``\"num_sync_all_streams\"``: number of ``synchronize_and_free_events`` calls.\n    - ``\"num_device_alloc\"``: number of CUDA allocation calls. This includes both\n      cuMemMap and cudaMalloc.\n    - ``\"num_device_free\"``: number of CUDA free calls. This includes both cuMemUnmap\n      and cudaFree.\n\n    The caching allocator can be configured via ENV to not split blocks larger than a\n    defined size (see Memory Management section of the Cuda Semantics documentation).\n    This helps avoid memory fragmentation but may have a performance\n    penalty. Additional outputs to assist with tuning and evaluating impact:\n\n    - ``\"max_split_size\"``: blocks above this size will not be split.\n    - ``\"oversize_allocations.{current,peak,allocated,freed}\"``:\n      number of over-size allocation requests received by the memory allocator.\n    - ``\"oversize_segments.{current,peak,allocated,freed}\"``:\n      number of over-size reserved segments from ``cudaMalloc()``.\n\n    The caching allocator can be configured via ENV to round memory allocations in order\n    to reduce fragmentation. Sometimes the overhead from rounding can be higher than\n    the fragmentation it helps reduce. The following stat can be used to check if\n    rounding adds too much overhead:\n\n    - ``\"requested_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      memory requested by client code, compare this with allocated_bytes to check if\n      allocation rounding adds too much overhead.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistics for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n\n    .. note::\n        With :ref:`backend:cudaMallocAsync<cuda-memory-envvars>`, some stats are not\n        meaningful, and are always reported as zero.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.memory_stats_as_nested_dict",
      "signature": "torch.cuda.memory_stats_as_nested_dict(device: Union[torch.device, str, int, NoneType] = None) -> dict[str, typing.Any]",
      "doc": "Return the result of :func:`~torch.cuda.memory_stats` as a nested dictionary.",
      "arguments": [
        "device"
      ],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the result of :func:`~torch.cuda.memory_stats` as a nested dictionary.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.memory_summary",
      "signature": "torch.cuda.memory_summary(device: Union[torch.device, str, int, NoneType] = None, abbreviated: bool = False) -> str",
      "doc": "Return a human-readable printout of the current memory allocator statistics for a given device.\n\n    This can be useful to display periodically during training, or when\n    handling out-of-memory exceptions.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            printout for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n        abbreviated (bool, optional): whether to return an abbreviated summary\n            (default: False).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "device",
        "abbreviated"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a human-readable printout of the current memory allocator statistics for a given device.\n\n    This can be useful to display periodically during training, or when\n    handling out-of-memory exceptions.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            printout for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n        abbreviated (bool, optional): whether to return an abbreviated summary\n            (default: False).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.memory_usage",
      "signature": "torch.cuda.memory_usage(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the percent of time over the past sample period during which global (device)\n    memory was being read or written as given by `nvidia-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the percent of time over the past sample period during which global (device)\n    memory was being read or written as given by `nvidia-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.power_draw",
      "signature": "torch.cuda.power_draw(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the average power draw of the GPU sensor in mW (MilliWatts)\n        over the past sample period as given by `nvidia-smi` for Fermi or newer fully supported devices.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the average power draw of the GPU sensor in mW (MilliWatts)\n        over the past sample period as given by `nvidia-smi` for Fermi or newer fully supported devices.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.reset_accumulated_host_memory_stats",
      "signature": "torch.cuda.reset_accumulated_host_memory_stats() -> None",
      "doc": "Reset the \"accumulated\" (historical) stats tracked by the host memory allocator.\n\n    See :func:`~torch.cuda.host_memory_stats` for details. Accumulated stats correspond to\n    the `\"allocated\"` and `\"freed\"` keys in each individual stat dict.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the \"accumulated\" (historical) stats tracked by the host memory allocator.\n\n    See :func:`~torch.cuda.host_memory_stats` for details. Accumulated stats correspond to\n    the `\"allocated\"` and `\"freed\"` keys in each individual stat dict.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.reset_accumulated_memory_stats",
      "signature": "torch.cuda.reset_accumulated_memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Reset the \"accumulated\" (historical) stats tracked by the CUDA memory allocator.\n\n    See :func:`~torch.cuda.memory_stats` for details. Accumulated stats correspond to\n    the `\"allocated\"` and `\"freed\"` keys in each individual stat dict, as well as\n    `\"num_alloc_retries\"` and `\"num_ooms\"`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the \"accumulated\" (historical) stats tracked by the CUDA memory allocator.\n\n    See :func:`~torch.cuda.memory_stats` for details. Accumulated stats correspond to\n    the `\"allocated\"` and `\"freed\"` keys in each individual stat dict, as well as\n    `\"num_alloc_retries\"` and `\"num_ooms\"`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.reset_max_memory_allocated",
      "signature": "torch.cuda.reset_max_memory_allocated(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Reset the starting point in tracking maximum GPU memory occupied by tensors for a given device.\n\n    See :func:`~torch.cuda.max_memory_allocated` for details.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. warning::\n        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets\n        /all/ peak memory stats.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the starting point in tracking maximum GPU memory occupied by tensors for a given device.\n\n    See :func:`~torch.cuda.max_memory_allocated` for details.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. warning::\n        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets\n        /all/ peak memory stats.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.reset_max_memory_cached",
      "signature": "torch.cuda.reset_max_memory_cached(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Reset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.\n\n    See :func:`~torch.cuda.max_memory_cached` for details.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. warning::\n        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets\n        /all/ peak memory stats.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.\n\n    See :func:`~torch.cuda.max_memory_cached` for details.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. warning::\n        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets\n        /all/ peak memory stats.\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.reset_peak_host_memory_stats",
      "signature": "torch.cuda.reset_peak_host_memory_stats() -> None",
      "doc": "Reset the \"peak\" stats tracked by the host memory allocator.\n\n    See :func:`~torch.cuda.host_memory_stats` for details. Peak stats correspond to the\n    `\"peak\"` key in each individual stat dict.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the \"peak\" stats tracked by the host memory allocator.\n\n    See :func:`~torch.cuda.host_memory_stats` for details. Peak stats correspond to the\n    `\"peak\"` key in each individual stat dict.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.reset_peak_memory_stats",
      "signature": "torch.cuda.reset_peak_memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Reset the \"peak\" stats tracked by the CUDA memory allocator.\n\n    See :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the\n    `\"peak\"` key in each individual stat dict.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the \"peak\" stats tracked by the CUDA memory allocator.\n\n    See :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the\n    `\"peak\"` key in each individual stat dict.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        See :ref:`cuda-memory-management` for more details about GPU memory\n        management.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.seed",
      "signature": "torch.cuda.seed() -> None",
      "doc": "Set the seed for generating random numbers to a random number for the current GPU.\n\n    It's safe to call this function if CUDA is not available; in that\n    case, it is silently ignored.\n\n    .. warning::\n        If you are working with a multi-GPU model, this function will only initialize\n        the seed on one GPU.  To initialize all GPUs, use :func:`seed_all`.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the seed for generating random numbers to a random number for the current GPU.\n\n    It's safe to call this function if CUDA is not available; in that\n    case, it is silently ignored.\n\n    .. warning::\n        If you are working with a multi-GPU model, this function will only initialize\n        the seed on one GPU.  To initialize all GPUs, use :func:`seed_all`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.seed_all",
      "signature": "torch.cuda.seed_all() -> None",
      "doc": "Set the seed for generating random numbers to a random number on all GPUs.\n\n    It's safe to call this function if CUDA is not available; in that\n    case, it is silently ignored.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the seed for generating random numbers to a random number on all GPUs.\n\n    It's safe to call this function if CUDA is not available; in that\n    case, it is silently ignored.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.set_device",
      "signature": "torch.cuda.set_device(device: Union[torch.device, str, int, NoneType]) -> None",
      "doc": "Set the current device.\n\n    Usage of this function is discouraged in favor of :any:`device`. In most\n    cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.\n\n    Args:\n        device (torch.device or int): selected device. This function is a no-op\n            if this argument is negative.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current device.\n\n    Usage of this function is discouraged in favor of :any:`device`. In most\n    cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.\n\n    Args:\n        device (torch.device or int): selected device. This function is a no-op\n            if this argument is negative.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.set_per_process_memory_fraction",
      "signature": "torch.cuda.set_per_process_memory_fraction(fraction, device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Set memory fraction for a process.\n\n    The fraction is used to limit an caching allocator to allocated memory on a CUDA device.\n    The allowed value equals the total visible memory multiplied fraction.\n    If trying to allocate more than the allowed value in a process, will raise an out of\n    memory error in allocator.\n\n    Args:\n        fraction(float): Range: 0~1. Allowed memory equals total_memory * fraction.\n        device (torch.device or int, optional): selected device. If it is\n            ``None`` the default CUDA device is used.\n    .. note::\n        In general, the total available free memory is less than the total capacity.\n    ",
      "arguments": [
        "fraction",
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set memory fraction for a process.\n\n    The fraction is used to limit an caching allocator to allocated memory on a CUDA device.\n    The allowed value equals the total visible memory multiplied fraction.\n    If trying to allocate more than the allowed value in a process, will raise an out of\n    memory error in allocator.\n\n    Args:\n        fraction(float): Range: 0~1. Allowed memory equals total_memory * fraction.\n        device (torch.device or int, optional): selected device. If it is\n            ``None`` the default CUDA device is used.\n    .. note::\n        In general, the total available free memory is less than the total capacity.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.set_rng_state",
      "signature": "torch.cuda.set_rng_state(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'cuda') -> None",
      "doc": "Set the random number generator state of the specified GPU.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n        device (torch.device or int, optional): The device to set the RNG state.\n            Default: ``'cuda'`` (i.e., ``torch.device('cuda')``, the current CUDA device).\n    ",
      "arguments": [
        "new_state",
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the random number generator state of the specified GPU.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n        device (torch.device or int, optional): The device to set the RNG state.\n            Default: ``'cuda'`` (i.e., ``torch.device('cuda')``, the current CUDA device).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.set_rng_state_all",
      "signature": "torch.cuda.set_rng_state_all(new_states: collections.abc.Iterable[torch.Tensor]) -> None",
      "doc": "Set the random number generator state of all devices.\n\n    Args:\n        new_states (Iterable of torch.ByteTensor): The desired state for each device.\n    ",
      "arguments": [
        "new_states"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the random number generator state of all devices.\n\n    Args:\n        new_states (Iterable of torch.ByteTensor): The desired state for each device.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.set_stream",
      "signature": "torch.cuda.set_stream(stream: torch.cuda.streams.Stream)",
      "doc": "Set the current stream.This is a wrapper API to set the stream.\n        Usage of this function is discouraged in favor of the ``stream``\n        context manager.\n\n    Args:\n        stream (Stream): selected stream. This function is a no-op\n            if this argument is ``None``.\n    ",
      "arguments": [
        "stream"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current stream.This is a wrapper API to set the stream.\n        Usage of this function is discouraged in favor of the ``stream``\n        context manager.\n\n    Args:\n        stream (Stream): selected stream. This function is a no-op\n            if this argument is ``None``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.set_sync_debug_mode",
      "signature": "torch.cuda.set_sync_debug_mode(debug_mode: Union[int, str]) -> None",
      "doc": "Set the debug mode for cuda synchronizing operations.\n\n    Args:\n        debug_mode(str or int): if \"default\" or 0, don't error or warn on synchronizing operations,\n            if \"warn\" or 1, warn on synchronizing operations, if \"error\" or 2, error out synchronizing operations.\n\n    Warning:\n        This is an experimental feature, and not all synchronizing operations will trigger warning or error. In\n        particular, operations in torch.distributed and torch.sparse namespaces are not covered yet.\n    ",
      "arguments": [
        "debug_mode"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the debug mode for cuda synchronizing operations.\n\n    Args:\n        debug_mode(str or int): if \"default\" or 0, don't error or warn on synchronizing operations,\n            if \"warn\" or 1, warn on synchronizing operations, if \"error\" or 2, error out synchronizing operations.\n\n    Warning:\n        This is an experimental feature, and not all synchronizing operations will trigger warning or error. In\n        particular, operations in torch.distributed and torch.sparse namespaces are not covered yet.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.stream",
      "signature": "torch.cuda.stream(stream: Optional[ForwardRef('torch.cuda.Stream')]) -> torch.cuda.StreamContext",
      "doc": "Wrap around the Context-manager StreamContext that selects a given stream.\n\n    Arguments:\n        stream (Stream): selected stream. This manager is a no-op if it's\n            ``None``.\n    .. note::\n        In eager mode stream is of type Stream class while in JIT it is\n        an object of the custom class ``torch.classes.cuda.Stream``.\n    ",
      "arguments": [
        "stream"
      ],
      "return_type": "<class 'torch.cuda.StreamContext'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Wrap around the Context-manager StreamContext that selects a given stream.\n\n    Arguments:\n        stream (Stream): selected stream. This manager is a no-op if it's\n            ``None``.\n    .. note::\n        In eager mode stream is of type Stream class while in JIT it is\n        an object of the custom class ``torch.classes.cuda.Stream``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.synchronize",
      "signature": "torch.cuda.synchronize(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Wait for all kernels in all streams on a CUDA device to complete.\n\n    Args:\n        device (torch.device or int, optional): device for which to synchronize.\n            It uses the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Wait for all kernels in all streams on a CUDA device to complete.\n\n    Args:\n        device (torch.device or int, optional): device for which to synchronize.\n            It uses the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.temperature",
      "signature": "torch.cuda.temperature(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the average temperature of the GPU sensor in Degrees C (Centigrades).\n\n    The average temperature is computed based on past sample period as given by `nvidia-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the average temperature of the GPU sensor in Degrees C (Centigrades).\n\n    The average temperature is computed based on past sample period as given by `nvidia-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.use_mem_pool",
      "signature": "torch.cuda.use_mem_pool(pool: torch.cuda.memory.MemPool, device: Union[torch.device, str, int, NoneType] = None)",
      "doc": "A context manager that routes allocations to a given pool.\n\n    Args:\n        pool(torch.cuda.MemPool): a MemPool object to be made active so that\n            allocations route to this pool.\n        device (torch.device or int, optional): selected device. Uses MemPool on\n            the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    ",
      "arguments": [
        "pool",
        "device"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "A context manager that routes allocations to a given pool.\n\n    Args:\n        pool(torch.cuda.MemPool): a MemPool object to be made active so that\n            allocations route to this pool.\n        device (torch.device or int, optional): selected device. Uses MemPool on\n            the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.cuda.utilization",
      "signature": "torch.cuda.utilization(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the percent of time over the past sample period during which one or\n    more kernels was executing on the GPU as given by `nvidia-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the percent of time over the past sample period during which one or\n    more kernels was executing on the GPU as given by `nvidia-smi`.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Warning: Each sample period may be between 1 second and 1/6 second,\n    depending on the product being queried.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "DISTRIBUTED_COMPUTING": [
    {
      "function": "torch.distributed.all_gather",
      "signature": "torch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False)",
      "doc": "\n    Gathers tensors from the whole group in a list.\n\n    Complex and uneven sized tensors are supported.\n\n    Args:\n        tensor_list (list[Tensor]): Output list. It should contain\n            correctly-sized tensors to be used for output of the collective.\n            Uneven sized tensors are supported.\n        tensor (Tensor): Tensor to be broadcast from current process.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # All tensors below are of torch.int64 dtype.\n        >>> # We have 2 process groups, 2 ranks.\n        >>> device = torch.device(f\"cuda:{rank}\")\n        >>> tensor_list = [\n        ...     torch.zeros(2, dtype=torch.int64, device=device) for _ in range(2)\n        ... ]\n        >>> tensor_list\n        [tensor([0, 0], device='cuda:0'), tensor([0, 0], device='cuda:0')] # Rank 0\n        [tensor([0, 0], device='cuda:1'), tensor([0, 0], device='cuda:1')] # Rank 1\n        >>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n        >>> tensor\n        tensor([1, 2], device='cuda:0') # Rank 0\n        tensor([3, 4], device='cuda:1') # Rank 1\n        >>> dist.all_gather(tensor_list, tensor)\n        >>> tensor_list\n        [tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')] # Rank 0\n        [tensor([1, 2], device='cuda:1'), tensor([3, 4], device='cuda:1')] # Rank 1\n\n        >>> # All tensors below are of torch.cfloat dtype.\n        >>> # We have 2 process groups, 2 ranks.\n        >>> tensor_list = [\n        ...     torch.zeros(2, dtype=torch.cfloat, device=device) for _ in range(2)\n        ... ]\n        >>> tensor_list\n        [tensor([0.+0.j, 0.+0.j], device='cuda:0'), tensor([0.+0.j, 0.+0.j], device='cuda:0')] # Rank 0\n        [tensor([0.+0.j, 0.+0.j], device='cuda:1'), tensor([0.+0.j, 0.+0.j], device='cuda:1')] # Rank 1\n        >>> tensor = torch.tensor(\n        ...     [1 + 1j, 2 + 2j], dtype=torch.cfloat, device=device\n        ... ) + 2 * rank * (1 + 1j)\n        >>> tensor\n        tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0\n        tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1\n        >>> dist.all_gather(tensor_list, tensor)\n        >>> tensor_list\n        [tensor([1.+1.j, 2.+2.j], device='cuda:0'), tensor([3.+3.j, 4.+4.j], device='cuda:0')] # Rank 0\n        [tensor([1.+1.j, 2.+2.j], device='cuda:1'), tensor([3.+3.j, 4.+4.j], device='cuda:1')] # Rank 1\n\n    ",
      "arguments": [
        "tensor_list",
        "tensor",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Gathers tensors from the whole group in a list.\n\n    Complex and uneven sized tensors are supported.\n\n    Args:\n        tensor_list (list[Tensor]): Output list. It should contain\n            correctly-sized tensors to be used for output of the collective.\n            Uneven sized tensors are supported.\n        tensor (Tensor): Tensor to be broadcast from current process.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # All tensors below are of torch.int64 dtype.\n        >>> # We have 2 process groups, 2 ranks.\n        >>> device = torch.device(f\"cuda:{rank}\")\n        >>> tensor_list = [\n        ...     torch.zeros(2, dtype=torch.int64, device=device) for _ in range(2)\n        ... ]\n        >>> tensor_list\n        [tensor([0, 0], device='cuda:0'), tensor([0, 0], device='cuda:0')] # Rank 0\n        [tensor([0, 0], device='cuda:1'), tensor([0, 0], device='cuda:1')] # Rank 1\n        >>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n        >>> tensor\n        tensor([1, 2], device='cuda:0') # Rank 0\n        tensor([3, 4], device='cuda:1') # Rank 1\n        >>> dist.all_gather(tensor_list, tensor)\n        >>> tensor_list\n        [tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')] # Rank 0\n        [tensor([1, 2], device='cuda:1'), tensor([3, 4], device='cuda:1')] # Rank 1\n\n        >>> # All tensors below are of torch.cfloat dtype.\n        >>> # We have 2 process groups, 2 ranks.\n        >>> tensor_list = [\n        ...     torch.zeros(2, dtype=torch.cfloat, device=device) for _ in range(2)\n        ... ]\n        >>> tensor_list\n        [tensor([0.+0.j, 0.+0.j], device='cuda:0'), tensor([0.+0.j, 0.+0.j], device='cuda:0')] # Rank 0\n        [tensor([0.+0.j, 0.+0.j], device='cuda:1'), tensor([0.+0.j, 0.+0.j], device='cuda:1')] # Rank 1\n        >>> tensor = torch.tensor(\n        ...     [1 + 1j, 2 + 2j], dtype=torch.cfloat, device=device\n        ... ) + 2 * rank * (1 + 1j)\n        >>> tensor\n        tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0\n        tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1\n        >>> dist.all_gather(tensor_list, tensor)\n        >>> tensor_list\n        [tensor([1.+1.j, 2.+2.j], device='cuda:0'), tensor([3.+3.j, 4.+4.j], device='cuda:0')] # Rank 0\n        [tensor([1.+1.j, 2.+2.j], device='cuda:1'), tensor([3.+3.j, 4.+4.j], device='cuda:1')] # Rank 1\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.all_gather_coalesced",
      "signature": "torch.distributed.all_gather_coalesced(output_tensor_lists, input_tensor_list, group=None, async_op=False)",
      "doc": "\n    Gathers input tensors from the whole group in a list in a coalesced manner.\n\n    Complex tensors are supported.\n\n    Args:\n        output_tensor_lists (list[list[Tensor]]): Output list. It should contain\n            correctly-sized tensors to be used for output of the collective.\n        input_tensor_list (list[Tensor]): Tensors to be broadcast from\n            current process. At least one tensor has to be non empty.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    Example:\n        we have 2 process groups, 2 ranks.\n        rank 0 passes:\n            input_tensor_list = [[[1, 1], [1, 1]], [2], [3, 3]]\n            output_tensor_lists =\n               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],\n                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]\n        rank 1 passes:\n            input_tensor_list = [[[3, 3], [3, 3]], [5], [1, 1]]\n            output_tensor_lists =\n               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],\n                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]\n        both rank 0 and 1 get:\n            output_tensor_lists =\n               [[[1, 1], [1, 1]], [2], [3, 3]],\n                [[3, 3], [3, 3]], [5], [1, 1]]].\n\n    WARNING: at this time individual shape checking is not implemented across nodes.\n    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the\n    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the\n    all_gather_coalesced operation will proceed without complaint and return\n    erroneous outputs. This lack of shape checking results in significant\n    performance improvements but users of this function should take extra care\n    to ensure that each node passes in tensors whose shapes match across nodes.\n    ",
      "arguments": [
        "output_tensor_lists",
        "input_tensor_list",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Gathers input tensors from the whole group in a list in a coalesced manner.\n\n    Complex tensors are supported.\n\n    Args:\n        output_tensor_lists (list[list[Tensor]]): Output list. It should contain\n            correctly-sized tensors to be used for output of the collective.\n        input_tensor_list (list[Tensor]): Tensors to be broadcast from\n            current process. At least one tensor has to be non empty.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    Example:\n        we have 2 process groups, 2 ranks.\n        rank 0 passes:\n            input_tensor_list = [[[1, 1], [1, 1]], [2], [3, 3]]\n            output_tensor_lists =\n               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],\n                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]\n        rank 1 passes:\n            input_tensor_list = [[[3, 3], [3, 3]], [5], [1, 1]]\n            output_tensor_lists =\n               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],\n                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]\n        both rank 0 and 1 get:\n            output_tensor_lists =\n               [[[1, 1], [1, 1]], [2], [3, 3]],\n                [[3, 3], [3, 3]], [5], [1, 1]]].\n\n    WARNING: at this time individual shape checking is not implemented across nodes.\n    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the\n    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the\n    all_gather_coalesced operation will proceed without complaint and return\n    erroneous outputs. This lack of shape checking results in significant\n    performance improvements but users of this function should take extra care\n    to ensure that each node passes in tensors whose shapes match across nodes.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.all_gather_into_tensor",
      "signature": "torch.distributed.all_gather_into_tensor(output_tensor, input_tensor, group=None, async_op=False)",
      "doc": "\n    Gather tensors from all ranks and put them in a single output tensor.\n\n    This function requires all tensors to be the same size on each process.\n\n    Args:\n        output_tensor (Tensor): Output tensor to accommodate tensor elements\n            from all ranks. It must be correctly sized to have one of the\n            following forms:\n            (i) a concatenation of all the input tensors along the primary\n            dimension; for definition of \"concatenation\", see ``torch.cat()``;\n            (ii) a stack of all the input tensors along the primary dimension;\n            for definition of \"stack\", see ``torch.stack()``.\n            Examples below may better explain the supported output forms.\n        input_tensor (Tensor): Tensor to be gathered from current rank.\n            Different from the ``all_gather`` API, the input tensors in this\n            API must have the same size across all ranks.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n        >>> # We have two ranks.\n        >>> device = torch.device(f\"cuda:{rank}\")\n        >>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n        >>> tensor_in\n        tensor([1, 2], device='cuda:0') # Rank 0\n        tensor([3, 4], device='cuda:1') # Rank 1\n        >>> # Output in concatenation form\n        >>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)\n        >>> dist.all_gather_into_tensor(tensor_out, tensor_in)\n        >>> tensor_out\n        tensor([1, 2, 3, 4], device='cuda:0') # Rank 0\n        tensor([1, 2, 3, 4], device='cuda:1') # Rank 1\n        >>> # Output in stack form\n        >>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)\n        >>> dist.all_gather_into_tensor(tensor_out2, tensor_in)\n        >>> tensor_out2\n        tensor([[1, 2],\n                [3, 4]], device='cuda:0') # Rank 0\n        tensor([[1, 2],\n                [3, 4]], device='cuda:1') # Rank 1\n    ",
      "arguments": [
        "output_tensor",
        "input_tensor",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Gather tensors from all ranks and put them in a single output tensor.\n\n    This function requires all tensors to be the same size on each process.\n\n    Args:\n        output_tensor (Tensor): Output tensor to accommodate tensor elements\n            from all ranks. It must be correctly sized to have one of the\n            following forms:\n            (i) a concatenation of all the input tensors along the primary\n            dimension; for definition of \"concatenation\", see ``torch.cat()``;\n            (ii) a stack of all the input tensors along the primary dimension;\n            for definition of \"stack\", see ``torch.stack()``.\n            Examples below may better explain the supported output forms.\n        input_tensor (Tensor): Tensor to be gathered from current rank.\n            Different from the ``all_gather`` API, the input tensors in this\n            API must have the same size across all ranks.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n        >>> # We have two ranks.\n        >>> device = torch.device(f\"cuda:{rank}\")\n        >>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n        >>> tensor_in\n        tensor([1, 2], device='cuda:0') # Rank 0\n        tensor([3, 4], device='cuda:1') # Rank 1\n        >>> # Output in concatenation form\n        >>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)\n        >>> dist.all_gather_into_tensor(tensor_out, tensor_in)\n        >>> tensor_out\n        tensor([1, 2, 3, 4], device='cuda:0') # Rank 0\n        tensor([1, 2, 3, 4], device='cuda:1') # Rank 1\n        >>> # Output in stack form\n        >>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)\n        >>> dist.all_gather_into_tensor(tensor_out2, tensor_in)\n        >>> tensor_out2\n        tensor([[1, 2],\n                [3, 4]], device='cuda:0') # Rank 0\n        tensor([[1, 2],\n                [3, 4]], device='cuda:1') # Rank 1\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.all_gather_object",
      "signature": "torch.distributed.all_gather_object(object_list, obj, group=None)",
      "doc": "\n    Gathers picklable objects from the whole group into a list.\n\n    Similar to :func:`all_gather`, but Python objects can be passed in.\n    Note that the object must be picklable in order to be gathered.\n\n    Args:\n        object_list (list[Any]): Output list. It should be correctly sized as the\n            size of the group for this collective and will contain the output.\n        obj (Any): Pickable Python object to be broadcast from current process.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n\n    Returns:\n        None. If the calling rank is part of this group, the output of the\n        collective will be populated into the input ``object_list``. If the\n        calling rank is not part of the group, the passed in ``object_list`` will\n        be unmodified.\n\n    .. note:: Note that this API differs slightly from the :func:`all_gather`\n        collective since it does not provide an ``async_op`` handle and thus\n        will be a blocking call.\n\n    .. note:: For NCCL-based processed groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsiblity to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`all_gather_object` uses ``pickle`` module implicitly, which is\n        known to be insecure. It is possible to construct malicious pickle data\n        which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`all_gather_object` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`all_gather` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> # Assumes world_size of 3.\n        >>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>> output = [None for _ in gather_objects]\n        >>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n        >>> output\n        ['foo', 12, {1: 2}]\n    ",
      "arguments": [
        "object_list",
        "obj",
        "group"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Gathers picklable objects from the whole group into a list.\n\n    Similar to :func:`all_gather`, but Python objects can be passed in.\n    Note that the object must be picklable in order to be gathered.\n\n    Args:\n        object_list (list[Any]): Output list. It should be correctly sized as the\n            size of the group for this collective and will contain the output.\n        obj (Any): Pickable Python object to be broadcast from current process.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n\n    Returns:\n        None. If the calling rank is part of this group, the output of the\n        collective will be populated into the input ``object_list``. If the\n        calling rank is not part of the group, the passed in ``object_list`` will\n        be unmodified.\n\n    .. note:: Note that this API differs slightly from the :func:`all_gather`\n        collective since it does not provide an ``async_op`` handle and thus\n        will be a blocking call.\n\n    .. note:: For NCCL-based processed groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsiblity to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`all_gather_object` uses ``pickle`` module implicitly, which is\n        known to be insecure. It is possible to construct malicious pickle data\n        which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`all_gather_object` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`all_gather` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> # Assumes world_size of 3.\n        >>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>> output = [None for _ in gather_objects]\n        >>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n        >>> output\n        ['foo', 12, {1: 2}]\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.all_reduce",
      "signature": "torch.distributed.all_reduce(tensor, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
      "doc": "\n    Reduces the tensor data across all machines in a way that all get the final result.\n\n    After the call ``tensor`` is going to be bitwise identical in all processes.\n\n    Complex tensors are supported.\n\n    Args:\n        tensor (Tensor): Input and output of the collective. The function\n            operates in-place.\n        op (optional): One of the values from\n            ``torch.distributed.ReduceOp``\n            enum.  Specifies an operation used for element-wise reductions.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"no rank\")\n        >>> # All tensors below are of torch.int64 type.\n        >>> # We have 2 process groups, 2 ranks.\n        >>> device = torch.device(f\"cuda:{rank}\")\n        >>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n        >>> tensor\n        tensor([1, 2], device='cuda:0') # Rank 0\n        tensor([3, 4], device='cuda:1') # Rank 1\n        >>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n        >>> tensor\n        tensor([4, 6], device='cuda:0') # Rank 0\n        tensor([4, 6], device='cuda:1') # Rank 1\n\n        >>> # All tensors below are of torch.cfloat type.\n        >>> # We have 2 process groups, 2 ranks.\n        >>> tensor = torch.tensor(\n        ...     [1 + 1j, 2 + 2j], dtype=torch.cfloat, device=device\n        ... ) + 2 * rank * (1 + 1j)\n        >>> tensor\n        tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0\n        tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1\n        >>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n        >>> tensor\n        tensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0\n        tensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1\n\n    ",
      "arguments": [
        "tensor",
        "op",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reduces the tensor data across all machines in a way that all get the final result.\n\n    After the call ``tensor`` is going to be bitwise identical in all processes.\n\n    Complex tensors are supported.\n\n    Args:\n        tensor (Tensor): Input and output of the collective. The function\n            operates in-place.\n        op (optional): One of the values from\n            ``torch.distributed.ReduceOp``\n            enum.  Specifies an operation used for element-wise reductions.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"no rank\")\n        >>> # All tensors below are of torch.int64 type.\n        >>> # We have 2 process groups, 2 ranks.\n        >>> device = torch.device(f\"cuda:{rank}\")\n        >>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n        >>> tensor\n        tensor([1, 2], device='cuda:0') # Rank 0\n        tensor([3, 4], device='cuda:1') # Rank 1\n        >>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n        >>> tensor\n        tensor([4, 6], device='cuda:0') # Rank 0\n        tensor([4, 6], device='cuda:1') # Rank 1\n\n        >>> # All tensors below are of torch.cfloat type.\n        >>> # We have 2 process groups, 2 ranks.\n        >>> tensor = torch.tensor(\n        ...     [1 + 1j, 2 + 2j], dtype=torch.cfloat, device=device\n        ... ) + 2 * rank * (1 + 1j)\n        >>> tensor\n        tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0\n        tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1\n        >>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n        >>> tensor\n        tensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0\n        tensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.all_reduce_coalesced",
      "signature": "torch.distributed.all_reduce_coalesced(tensors, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
      "doc": "\n    WARNING: at this time individual shape checking is not implemented across nodes.\n\n    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the\n    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the allreduce\n    operation will proceed without complaint and return erroneous outputs. This lack\n    of shape checking results in significant performance improvements but users of this\n    function should take extra care to ensure that each node passes in tensors whose\n    shapes match across nodes.\n\n    Reduces each tensor in tensors (residing on the same device) across all machines\n    in such a way that all get the final result.\n\n    After the call each tensor in tensors is going to bitwise identical\n    in all processes.\n\n    Complex tensors are supported.\n\n    Args:\n        tensors (Union[List[Tensor], Tensor]): Input and output of the collective.\n            The function operates in-place.\n        op (Optional[ReduceOp]): One of the values from\n            ``torch.distributed.ReduceOp`` enum. Specifies an operation used for\n            element-wise reductions.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (Optional[bool]): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    ",
      "arguments": [
        "tensors",
        "op",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    WARNING: at this time individual shape checking is not implemented across nodes.\n\n    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the\n    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the allreduce\n    operation will proceed without complaint and return erroneous outputs. This lack\n    of shape checking results in significant performance improvements but users of this\n    function should take extra care to ensure that each node passes in tensors whose\n    shapes match across nodes.\n\n    Reduces each tensor in tensors (residing on the same device) across all machines\n    in such a way that all get the final result.\n\n    After the call each tensor in tensors is going to bitwise identical\n    in all processes.\n\n    Complex tensors are supported.\n\n    Args:\n        tensors (Union[List[Tensor], Tensor]): Input and output of the collective.\n            The function operates in-place.\n        op (Optional[ReduceOp]): One of the values from\n            ``torch.distributed.ReduceOp`` enum. Specifies an operation used for\n            element-wise reductions.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (Optional[bool]): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.all_to_all",
      "signature": "torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False)",
      "doc": "\n    Scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.\n\n    Complex tensors are supported.\n\n    Args:\n        output_tensor_list (list[Tensor]): List of tensors to be gathered one\n            per rank.\n        input_tensor_list (list[Tensor]): List of tensors to scatter one per rank.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    .. warning::\n        `all_to_all` is experimental and subject to change.\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"Undefined rank\")\n        >>> input = torch.arange(4) + rank * 4\n        >>> input = list(input.chunk(4))\n        >>> input\n        [tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n        [tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n        [tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n        [tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n        >>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n        >>> dist.all_to_all(output, input)\n        >>> output\n        [tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n        [tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n        [tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n        [tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n\n        >>> # Essentially, it is similar to following operation:\n        >>> scatter_list = input\n        >>> gather_list = output\n        >>> for i in range(world_size):\n        >>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)\n\n        >>> input\n        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\n        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\n        tensor([20, 21, 22, 23, 24])                                     # Rank 2\n        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n        >>> input_splits\n        [2, 2, 1, 1]                                                     # Rank 0\n        [3, 2, 2, 2]                                                     # Rank 1\n        [2, 1, 1, 1]                                                     # Rank 2\n        [2, 2, 2, 1]                                                     # Rank 3\n        >>> output_splits\n        [2, 3, 2, 2]                                                     # Rank 0\n        [2, 2, 1, 2]                                                     # Rank 1\n        [1, 2, 1, 2]                                                     # Rank 2\n        [1, 2, 1, 1]                                                     # Rank 3\n        >>> input = list(input.split(input_splits))\n        >>> input\n        [tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n        [tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n        [tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n        [tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n        >>> output = ...\n        >>> dist.all_to_all(output, input)\n        >>> output\n        [tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n        [tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n        [tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n        [tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n\n        >>> # Another example with tensors of torch.cfloat type.\n        >>> input = torch.tensor(\n        ...     [1 + 1j, 2 + 2j, 3 + 3j, 4 + 4j], dtype=torch.cfloat\n        ... ) + 4 * rank * (1 + 1j)\n        >>> input = list(input.chunk(4))\n        >>> input\n        [tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n        [tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n        [tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n        [tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n        >>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n        >>> dist.all_to_all(output, input)\n        >>> output\n        [tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n        [tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n        [tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n        [tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3\n\n    ",
      "arguments": [
        "output_tensor_list",
        "input_tensor_list",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.\n\n    Complex tensors are supported.\n\n    Args:\n        output_tensor_list (list[Tensor]): List of tensors to be gathered one\n            per rank.\n        input_tensor_list (list[Tensor]): List of tensors to scatter one per rank.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    .. warning::\n        `all_to_all` is experimental and subject to change.\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"Undefined rank\")\n        >>> input = torch.arange(4) + rank * 4\n        >>> input = list(input.chunk(4))\n        >>> input\n        [tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n        [tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n        [tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n        [tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n        >>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n        >>> dist.all_to_all(output, input)\n        >>> output\n        [tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n        [tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n        [tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n        [tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n\n        >>> # Essentially, it is similar to following operation:\n        >>> scatter_list = input\n        >>> gather_list = output\n        >>> for i in range(world_size):\n        >>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)\n\n        >>> input\n        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\n        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\n        tensor([20, 21, 22, 23, 24])                                     # Rank 2\n        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n        >>> input_splits\n        [2, 2, 1, 1]                                                     # Rank 0\n        [3, 2, 2, 2]                                                     # Rank 1\n        [2, 1, 1, 1]                                                     # Rank 2\n        [2, 2, 2, 1]                                                     # Rank 3\n        >>> output_splits\n        [2, 3, 2, 2]                                                     # Rank 0\n        [2, 2, 1, 2]                                                     # Rank 1\n        [1, 2, 1, 2]                                                     # Rank 2\n        [1, 2, 1, 1]                                                     # Rank 3\n        >>> input = list(input.split(input_splits))\n        >>> input\n        [tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n        [tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n        [tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n        [tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n        >>> output = ...\n        >>> dist.all_to_all(output, input)\n        >>> output\n        [tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n        [tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n        [tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n        [tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n\n        >>> # Another example with tensors of torch.cfloat type.\n        >>> input = torch.tensor(\n        ...     [1 + 1j, 2 + 2j, 3 + 3j, 4 + 4j], dtype=torch.cfloat\n        ... ) + 4 * rank * (1 + 1j)\n        >>> input = list(input.chunk(4))\n        >>> input\n        [tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n        [tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n        [tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n        [tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n        >>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n        >>> dist.all_to_all(output, input)\n        >>> output\n        [tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n        [tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n        [tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n        [tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.all_to_all_single",
      "signature": "torch.distributed.all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False)",
      "doc": "\n    Split input tensor and then scatter the split list to all processes in a group.\n\n    Later the received tensors are concatenated from all the processes in the group\n    and returned as a single output tensor.\n\n    Complex tensors are supported.\n\n    Args:\n        output (Tensor): Gathered concatenated output tensor.\n        input (Tensor): Input tensor to scatter.\n        output_split_sizes: (list[Int], optional): Output split sizes for dim 0\n            if specified None or empty, dim 0 of ``output`` tensor must divide\n            equally by ``world_size``.\n        input_split_sizes: (list[Int], optional): Input split sizes for dim 0\n            if specified None or empty, dim 0 of ``input`` tensor must divide\n            equally by ``world_size``.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    .. warning::\n        `all_to_all_single` is experimental and subject to change.\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"Undefined rank\")\n        >>> input = torch.arange(4) + rank * 4\n        >>> input\n        tensor([0, 1, 2, 3])     # Rank 0\n        tensor([4, 5, 6, 7])     # Rank 1\n        tensor([8, 9, 10, 11])   # Rank 2\n        tensor([12, 13, 14, 15]) # Rank 3\n        >>> output = torch.empty([4], dtype=torch.int64)\n        >>> dist.all_to_all_single(output, input)\n        >>> output\n        tensor([0, 4, 8, 12])    # Rank 0\n        tensor([1, 5, 9, 13])    # Rank 1\n        tensor([2, 6, 10, 14])   # Rank 2\n        tensor([3, 7, 11, 15])   # Rank 3\n\n        >>> # Essentially, it is similar to following operation:\n        >>> scatter_list = list(input.chunk(world_size))\n        >>> gather_list = list(output.chunk(world_size))\n        >>> for i in range(world_size):\n        >>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n\n        >>> # Another example with uneven split\n        >>> input\n        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\n        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\n        tensor([20, 21, 22, 23, 24])                                     # Rank 2\n        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n        >>> input_splits\n        [2, 2, 1, 1]                                                     # Rank 0\n        [3, 2, 2, 2]                                                     # Rank 1\n        [2, 1, 1, 1]                                                     # Rank 2\n        [2, 2, 2, 1]                                                     # Rank 3\n        >>> output_splits\n        [2, 3, 2, 2]                                                     # Rank 0\n        [2, 2, 1, 2]                                                     # Rank 1\n        [1, 2, 1, 2]                                                     # Rank 2\n        [1, 2, 1, 1]                                                     # Rank 3\n        >>> output = ...\n        >>> dist.all_to_all_single(output, input, output_splits, input_splits)\n        >>> output\n        tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\n        tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\n        tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\n        tensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n\n\n        >>> # Another example with tensors of torch.cfloat type.\n        >>> input = torch.tensor(\n        ...     [1 + 1j, 2 + 2j, 3 + 3j, 4 + 4j], dtype=torch.cfloat\n        ... ) + 4 * rank * (1 + 1j)\n        >>> input\n        tensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\n        tensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\n        tensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\n        tensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n        >>> output = torch.empty([4], dtype=torch.int64)\n        >>> dist.all_to_all_single(output, input)\n        >>> output\n        tensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\n        tensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\n        tensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\n        tensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3\n    ",
      "arguments": [
        "output",
        "input",
        "output_split_sizes",
        "input_split_sizes",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Split input tensor and then scatter the split list to all processes in a group.\n\n    Later the received tensors are concatenated from all the processes in the group\n    and returned as a single output tensor.\n\n    Complex tensors are supported.\n\n    Args:\n        output (Tensor): Gathered concatenated output tensor.\n        input (Tensor): Input tensor to scatter.\n        output_split_sizes: (list[Int], optional): Output split sizes for dim 0\n            if specified None or empty, dim 0 of ``output`` tensor must divide\n            equally by ``world_size``.\n        input_split_sizes: (list[Int], optional): Input split sizes for dim 0\n            if specified None or empty, dim 0 of ``input`` tensor must divide\n            equally by ``world_size``.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    .. warning::\n        `all_to_all_single` is experimental and subject to change.\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"Undefined rank\")\n        >>> input = torch.arange(4) + rank * 4\n        >>> input\n        tensor([0, 1, 2, 3])     # Rank 0\n        tensor([4, 5, 6, 7])     # Rank 1\n        tensor([8, 9, 10, 11])   # Rank 2\n        tensor([12, 13, 14, 15]) # Rank 3\n        >>> output = torch.empty([4], dtype=torch.int64)\n        >>> dist.all_to_all_single(output, input)\n        >>> output\n        tensor([0, 4, 8, 12])    # Rank 0\n        tensor([1, 5, 9, 13])    # Rank 1\n        tensor([2, 6, 10, 14])   # Rank 2\n        tensor([3, 7, 11, 15])   # Rank 3\n\n        >>> # Essentially, it is similar to following operation:\n        >>> scatter_list = list(input.chunk(world_size))\n        >>> gather_list = list(output.chunk(world_size))\n        >>> for i in range(world_size):\n        >>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n\n        >>> # Another example with uneven split\n        >>> input\n        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\n        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\n        tensor([20, 21, 22, 23, 24])                                     # Rank 2\n        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n        >>> input_splits\n        [2, 2, 1, 1]                                                     # Rank 0\n        [3, 2, 2, 2]                                                     # Rank 1\n        [2, 1, 1, 1]                                                     # Rank 2\n        [2, 2, 2, 1]                                                     # Rank 3\n        >>> output_splits\n        [2, 3, 2, 2]                                                     # Rank 0\n        [2, 2, 1, 2]                                                     # Rank 1\n        [1, 2, 1, 2]                                                     # Rank 2\n        [1, 2, 1, 1]                                                     # Rank 3\n        >>> output = ...\n        >>> dist.all_to_all_single(output, input, output_splits, input_splits)\n        >>> output\n        tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\n        tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\n        tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\n        tensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n\n\n        >>> # Another example with tensors of torch.cfloat type.\n        >>> input = torch.tensor(\n        ...     [1 + 1j, 2 + 2j, 3 + 3j, 4 + 4j], dtype=torch.cfloat\n        ... ) + 4 * rank * (1 + 1j)\n        >>> input\n        tensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\n        tensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\n        tensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\n        tensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n        >>> output = torch.empty([4], dtype=torch.int64)\n        >>> dist.all_to_all_single(output, input)\n        >>> output\n        tensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\n        tensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\n        tensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\n        tensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.barrier",
      "signature": "torch.distributed.barrier(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op=False, device_ids=None)",
      "doc": "\n    Synchronize all processes.\n\n    This collective blocks processes until the whole group enters this function,\n    if async_op is False, or if async work handle is called on wait().\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        device_ids ([int], optional): List of device/GPU ids.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    .. note:: `ProcessGroupNCCL` now blocks the cpu thread till the completion of the barrier collective.\n    ",
      "arguments": [
        "group",
        "async_op",
        "device_ids"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Synchronize all processes.\n\n    This collective blocks processes until the whole group enters this function,\n    if async_op is False, or if async work handle is called on wait().\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        device_ids ([int], optional): List of device/GPU ids.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    .. note:: `ProcessGroupNCCL` now blocks the cpu thread till the completion of the barrier collective.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.batch_isend_irecv",
      "signature": "torch.distributed.batch_isend_irecv(p2p_op_list: list[torch.distributed.distributed_c10d.P2POp]) -> list[torch.distributed.distributed_c10d.Work]",
      "doc": "\n    Send or Receive a batch of tensors asynchronously and return a list of requests.\n\n    Process each of the operations in ``p2p_op_list`` and return the corresponding\n    requests. NCCL, Gloo, and UCC backend are currently supported.\n\n    Args:\n        p2p_op_list: A list of point-to-point operations(type of each operator is\n            ``torch.distributed.P2POp``). The order of the isend/irecv in the list\n            matters and it needs to match with corresponding isend/irecv on the\n            remote end.\n\n    Returns:\n        A list of distributed request objects returned by calling the corresponding\n        op in the op_list.\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"no rank\")\n        >>> send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rank\n        >>> recv_tensor = torch.randn(2, dtype=torch.float32)\n        >>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1) % world_size)\n        >>> recv_op = dist.P2POp(\n        ...     dist.irecv, recv_tensor, (rank - 1 + world_size) % world_size\n        ... )\n        >>> reqs = batch_isend_irecv([send_op, recv_op])\n        >>> for req in reqs:\n        >>>     req.wait()\n        >>> recv_tensor\n        tensor([2, 3])     # Rank 0\n        tensor([0, 1])     # Rank 1\n\n    .. note:: Note that when this API is used with the NCCL PG backend, users must set\n        the current GPU device with `torch.cuda.set_device`, otherwise it will\n        lead to unexpected hang issues.\n\n        In addition, if this API is the first collective call in the ``group``\n        passed to ``dist.P2POp``, all ranks of the ``group`` must participate in\n        this API call; otherwise, the behavior is undefined. If this API call is\n        not the first collective call in the ``group``, batched P2P operations\n        involving only a subset of ranks of the ``group`` are allowed.\n    ",
      "arguments": [
        "p2p_op_list"
      ],
      "return_type": "list[torch.distributed.distributed_c10d.Work]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Send or Receive a batch of tensors asynchronously and return a list of requests.\n\n    Process each of the operations in ``p2p_op_list`` and return the corresponding\n    requests. NCCL, Gloo, and UCC backend are currently supported.\n\n    Args:\n        p2p_op_list: A list of point-to-point operations(type of each operator is\n            ``torch.distributed.P2POp``). The order of the isend/irecv in the list\n            matters and it needs to match with corresponding isend/irecv on the\n            remote end.\n\n    Returns:\n        A list of distributed request objects returned by calling the corresponding\n        op in the op_list.\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"no rank\")\n        >>> send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rank\n        >>> recv_tensor = torch.randn(2, dtype=torch.float32)\n        >>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1) % world_size)\n        >>> recv_op = dist.P2POp(\n        ...     dist.irecv, recv_tensor, (rank - 1 + world_size) % world_size\n        ... )\n        >>> reqs = batch_isend_irecv([send_op, recv_op])\n        >>> for req in reqs:\n        >>>     req.wait()\n        >>> recv_tensor\n        tensor([2, 3])     # Rank 0\n        tensor([0, 1])     # Rank 1\n\n    .. note:: Note that when this API is used with the NCCL PG backend, users must set\n        the current GPU device with `torch.cuda.set_device`, otherwise it will\n        lead to unexpected hang issues.\n\n        In addition, if this API is the first collective call in the ``group``\n        passed to ``dist.P2POp``, all ranks of the ``group`` must participate in\n        this API call; otherwise, the behavior is undefined. If this API call is\n        not the first collective call in the ``group``, batched P2P operations\n        involving only a subset of ranks of the ``group`` are allowed.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.breakpoint",
      "signature": "torch.distributed.breakpoint(rank: int = 0, skip: int = 0)",
      "doc": "\n        Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be\n        done with the breakpoint before continuing.\n\n        Args:\n            rank (int): Which rank to break on.  Default: ``0``\n            skip (int): Skip the first ``skip`` calls to this breakpoint. Default: ``0``.\n        ",
      "arguments": [
        "rank",
        "skip"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n        Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be\n        done with the breakpoint before continuing.\n\n        Args:\n            rank (int): Which rank to break on.  Default: ``0``\n            skip (int): Skip the first ``skip`` calls to this breakpoint. Default: ``0``.\n        ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.broadcast",
      "signature": "torch.distributed.broadcast(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op: bool = False, group_src: Optional[int] = None)",
      "doc": "\n    Broadcasts the tensor to the whole group.\n\n    ``tensor`` must have the same number of elements in all processes\n    participating in the collective.\n\n    Args:\n        tensor (Tensor): Data to be sent if ``src`` is the rank of current\n            process, and tensor to be used to save received data otherwise.\n        src (int): Source rank on global process group (regardless of ``group`` argument).\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        group_src (int): Source rank on ``group``.  Must specify one of ``group_src``\n            and ``src`` but not both.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    ",
      "arguments": [
        "tensor",
        "src",
        "group",
        "async_op",
        "group_src"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Broadcasts the tensor to the whole group.\n\n    ``tensor`` must have the same number of elements in all processes\n    participating in the collective.\n\n    Args:\n        tensor (Tensor): Data to be sent if ``src`` is the rank of current\n            process, and tensor to be used to save received data otherwise.\n        src (int): Source rank on global process group (regardless of ``group`` argument).\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        group_src (int): Source rank on ``group``.  Must specify one of ``group_src``\n            and ``src`` but not both.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.broadcast_object_list",
      "signature": "torch.distributed.broadcast_object_list(object_list: list[typing.Any], src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, device: Optional[torch.device] = None, group_src: Optional[int] = None)",
      "doc": "\n    Broadcasts picklable objects in ``object_list`` to the whole group.\n\n    Similar to :func:`broadcast`, but Python objects can be passed in.\n    Note that all objects in ``object_list`` must be picklable in order to be\n    broadcasted.\n\n    Args:\n        object_list (List[Any]): List of input objects to broadcast.\n            Each object must be picklable. Only objects on the ``src`` rank will\n            be broadcast, but each rank must provide lists of equal sizes.\n        src (int): Source rank from which to broadcast ``object_list``.\n            Source rank is based on global process group (regardless of ``group`` argument)\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        device (``torch.device``, optional): If not None, the objects are\n            serialized and converted to tensors which are moved to the\n            ``device`` before broadcasting. Default is ``None``.\n        group_src (int): Source rank on ``group``.  Must not specify one of ``group_src``\n            and ``src`` but not both.\n\n    Returns:\n        ``None``. If rank is part of the group, ``object_list`` will contain the\n        broadcasted objects from ``src`` rank.\n\n    .. note:: For NCCL-based process groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsibility to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. note:: Note that this API differs slightly from the :func:`broadcast`\n        collective since it does not provide an ``async_op`` handle and thus\n        will be a blocking call.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`broadcast_object_list` uses ``pickle`` module implicitly, which\n        is known to be insecure. It is possible to construct malicious pickle\n        data which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`broadcast_object_list` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`broadcast` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 3.\n        >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>> else:\n        >>>     objects = [None, None, None]\n        >>> # Assumes backend is not NCCL\n        >>> device = torch.device(\"cpu\")\n        >>> dist.broadcast_object_list(objects, src=0, device=device)\n        >>> objects\n        ['foo', 12, {1: 2}]\n    ",
      "arguments": [
        "object_list",
        "src",
        "group",
        "device",
        "group_src"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Broadcasts picklable objects in ``object_list`` to the whole group.\n\n    Similar to :func:`broadcast`, but Python objects can be passed in.\n    Note that all objects in ``object_list`` must be picklable in order to be\n    broadcasted.\n\n    Args:\n        object_list (List[Any]): List of input objects to broadcast.\n            Each object must be picklable. Only objects on the ``src`` rank will\n            be broadcast, but each rank must provide lists of equal sizes.\n        src (int): Source rank from which to broadcast ``object_list``.\n            Source rank is based on global process group (regardless of ``group`` argument)\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        device (``torch.device``, optional): If not None, the objects are\n            serialized and converted to tensors which are moved to the\n            ``device`` before broadcasting. Default is ``None``.\n        group_src (int): Source rank on ``group``.  Must not specify one of ``group_src``\n            and ``src`` but not both.\n\n    Returns:\n        ``None``. If rank is part of the group, ``object_list`` will contain the\n        broadcasted objects from ``src`` rank.\n\n    .. note:: For NCCL-based process groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsibility to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. note:: Note that this API differs slightly from the :func:`broadcast`\n        collective since it does not provide an ``async_op`` handle and thus\n        will be a blocking call.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`broadcast_object_list` uses ``pickle`` module implicitly, which\n        is known to be insecure. It is possible to construct malicious pickle\n        data which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`broadcast_object_list` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`broadcast` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 3.\n        >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>> else:\n        >>>     objects = [None, None, None]\n        >>> # Assumes backend is not NCCL\n        >>> device = torch.device(\"cpu\")\n        >>> dist.broadcast_object_list(objects, src=0, device=device)\n        >>> objects\n        ['foo', 12, {1: 2}]\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.destroy_process_group",
      "signature": "torch.distributed.destroy_process_group(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None)",
      "doc": "\n    Destroy a given process group, and deinitialize the distributed package.\n\n    Args:\n        group (ProcessGroup, optional): The process group to be destroyed, if\n                                        group.WORLD is given, all process\n                                        groups including the default one will\n                                        be destroyed.\n    ",
      "arguments": [
        "group"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Destroy a given process group, and deinitialize the distributed package.\n\n    Args:\n        group (ProcessGroup, optional): The process group to be destroyed, if\n                                        group.WORLD is given, all process\n                                        groups including the default one will\n                                        be destroyed.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.gather",
      "signature": "torch.distributed.gather(tensor: torch.Tensor, gather_list: Optional[list[torch.Tensor]] = None, dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op: bool = False, group_dst: Optional[int] = None)",
      "doc": "\n    Gathers a list of tensors in a single process.\n\n    This function requires all tensors to be the same size on each process.\n\n    Args:\n        tensor (Tensor): Input tensor.\n        gather_list (list[Tensor], optional): List of appropriately,\n            same-sized tensors to use for gathered data\n            (default is None, must be specified on the destination rank)\n        dst (int, optional): Destination rank on global process group (regardless of ``group`` argument).\n            (If both ``dst`` and ``group_dst`` are None, default is global rank 0)\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        group_dst (int, optional): Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    .. note:: Note that all Tensors in gather_list must have the same size.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"no rank\")\n        >>> # We have 2 process groups, 2 ranks.\n        >>> tensor_size = 2\n        >>> device = torch.device(f'cuda:{rank}')\n        >>> tensor = torch.ones(tensor_size, device=device) + rank\n        >>> if dist.get_rank() == 0:\n        >>>     gather_list = [torch.zeros_like(tensor, device=device) for i in range(2)]\n        >>> else:\n        >>>     gather_list = None\n        >>> dist.gather(tensor, gather_list, dst=0)\n        >>> # Rank 0 gets gathered data.\n        >>> gather_list\n        [tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0\n        None                                                                   # Rank 1\n\n    ",
      "arguments": [
        "tensor",
        "gather_list",
        "dst",
        "group",
        "async_op",
        "group_dst"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Gathers a list of tensors in a single process.\n\n    This function requires all tensors to be the same size on each process.\n\n    Args:\n        tensor (Tensor): Input tensor.\n        gather_list (list[Tensor], optional): List of appropriately,\n            same-sized tensors to use for gathered data\n            (default is None, must be specified on the destination rank)\n        dst (int, optional): Destination rank on global process group (regardless of ``group`` argument).\n            (If both ``dst`` and ``group_dst`` are None, default is global rank 0)\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        group_dst (int, optional): Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    .. note:: Note that all Tensors in gather_list must have the same size.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"no rank\")\n        >>> # We have 2 process groups, 2 ranks.\n        >>> tensor_size = 2\n        >>> device = torch.device(f'cuda:{rank}')\n        >>> tensor = torch.ones(tensor_size, device=device) + rank\n        >>> if dist.get_rank() == 0:\n        >>>     gather_list = [torch.zeros_like(tensor, device=device) for i in range(2)]\n        >>> else:\n        >>>     gather_list = None\n        >>> dist.gather(tensor, gather_list, dst=0)\n        >>> # Rank 0 gets gathered data.\n        >>> gather_list\n        [tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0\n        None                                                                   # Rank 1\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.gather_object",
      "signature": "torch.distributed.gather_object(obj: Any, object_gather_list: Optional[list[Any]] = None, dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, group_dst: Optional[int] = None)",
      "doc": "\n    Gathers picklable objects from the whole group in a single process.\n\n    Similar to :func:`gather`, but Python objects can be passed in. Note that the\n    object must be picklable in order to be gathered.\n\n    Args:\n        obj (Any): Input object. Must be picklable.\n        object_gather_list (list[Any]): Output list. On the ``dst`` rank, it\n            should be correctly sized as the size of the group for this\n            collective and will contain the output. Must be ``None`` on non-dst\n            ranks. (default is ``None``)\n        dst (int, optional): Destination rank on global process group (regardless of ``group`` argument).\n            (If both ``dst`` and ``group_dst`` are None, default is global rank 0)\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        group_dst (int, optional): Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``\n\n    Returns:\n        None. On the ``dst`` rank, ``object_gather_list`` will contain the\n        output of the collective.\n\n    .. note:: Note that this API differs slightly from the gather collective\n        since it does not provide an async_op handle and thus will be a blocking\n        call.\n\n    .. note:: For NCCL-based processed groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsiblity to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`gather_object` uses ``pickle`` module implicitly, which is\n        known to be insecure. It is possible to construct malicious pickle data\n        which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`gather_object` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`gather` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> # Assumes world_size of 3.\n        >>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>> output = [None for _ in gather_objects]\n        >>> dist.gather_object(\n        ...     gather_objects[dist.get_rank()],\n        ...     output if dist.get_rank() == 0 else None,\n        ...     dst=0\n        ... )\n        >>> # On rank 0\n        >>> output\n        ['foo', 12, {1: 2}]\n    ",
      "arguments": [
        "obj",
        "object_gather_list",
        "dst",
        "group",
        "group_dst"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Gathers picklable objects from the whole group in a single process.\n\n    Similar to :func:`gather`, but Python objects can be passed in. Note that the\n    object must be picklable in order to be gathered.\n\n    Args:\n        obj (Any): Input object. Must be picklable.\n        object_gather_list (list[Any]): Output list. On the ``dst`` rank, it\n            should be correctly sized as the size of the group for this\n            collective and will contain the output. Must be ``None`` on non-dst\n            ranks. (default is ``None``)\n        dst (int, optional): Destination rank on global process group (regardless of ``group`` argument).\n            (If both ``dst`` and ``group_dst`` are None, default is global rank 0)\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        group_dst (int, optional): Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``\n\n    Returns:\n        None. On the ``dst`` rank, ``object_gather_list`` will contain the\n        output of the collective.\n\n    .. note:: Note that this API differs slightly from the gather collective\n        since it does not provide an async_op handle and thus will be a blocking\n        call.\n\n    .. note:: For NCCL-based processed groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsiblity to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`gather_object` uses ``pickle`` module implicitly, which is\n        known to be insecure. It is possible to construct malicious pickle data\n        which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`gather_object` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`gather` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> # Assumes world_size of 3.\n        >>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>> output = [None for _ in gather_objects]\n        >>> dist.gather_object(\n        ...     gather_objects[dist.get_rank()],\n        ...     output if dist.get_rank() == 0 else None,\n        ...     dst=0\n        ... )\n        >>> # On rank 0\n        >>> output\n        ['foo', 12, {1: 2}]\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_backend",
      "signature": "torch.distributed.get_backend(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None) -> torch.distributed.distributed_c10d.Backend",
      "doc": "\n    Return the backend of the given process group.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. The\n            default is the general main process group. If another specific group\n            is specified, the calling process must be part of :attr:`group`.\n\n    Returns:\n        The backend of the given process group as a lower case string.\n\n    ",
      "arguments": [
        "group"
      ],
      "return_type": "<class 'torch.distributed.distributed_c10d.Backend'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return the backend of the given process group.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. The\n            default is the general main process group. If another specific group\n            is specified, the calling process must be part of :attr:`group`.\n\n    Returns:\n        The backend of the given process group as a lower case string.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_backend_config",
      "signature": "torch.distributed.get_backend_config(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None) -> str",
      "doc": "\n    Return the backend configuration of the given process group.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. The\n            default is the general main process group. If another specific group\n            is specified, the calling process must be part of :attr:`group`.\n\n    Returns:\n        The backend configuration of the given process group as a lower case string.\n\n    ",
      "arguments": [
        "group"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return the backend configuration of the given process group.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. The\n            default is the general main process group. If another specific group\n            is specified, the calling process must be part of :attr:`group`.\n\n    Returns:\n        The backend configuration of the given process group as a lower case string.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_default_backend_for_device",
      "signature": "torch.distributed.get_default_backend_for_device(device: Union[str, torch.device]) -> str",
      "doc": "\n    Return the default backend for the given device.\n\n    Args:\n        Union[str, torch.device]: The device to get the default backend for.\n\n    Returns:\n        The default backend for the given device as a lower case string.\n\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return the default backend for the given device.\n\n    Args:\n        Union[str, torch.device]: The device to get the default backend for.\n\n    Returns:\n        The default backend for the given device as a lower case string.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_global_rank",
      "signature": "torch.distributed.get_global_rank(group: torch.distributed.distributed_c10d.ProcessGroup, group_rank: int) -> int",
      "doc": "\n    Translate a group rank into a global rank.\n\n    ``group_rank`` must be part of `group` otherwise this raises RuntimeError.\n\n    Args:\n        group (ProcessGroup): ProcessGroup to find the global rank from.\n        group_rank (int): Group rank to query.\n\n    Returns:\n        Global rank of ``group_rank`` relative to ``group``\n\n    N.B. calling this function on the default process group returns identity\n    ",
      "arguments": [
        "group",
        "group_rank"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Translate a group rank into a global rank.\n\n    ``group_rank`` must be part of `group` otherwise this raises RuntimeError.\n\n    Args:\n        group (ProcessGroup): ProcessGroup to find the global rank from.\n        group_rank (int): Group rank to query.\n\n    Returns:\n        Global rank of ``group_rank`` relative to ``group``\n\n    N.B. calling this function on the default process group returns identity\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_group_rank",
      "signature": "torch.distributed.get_group_rank(group: torch.distributed.distributed_c10d.ProcessGroup, global_rank: int) -> int",
      "doc": "\n    Translate a global rank into a group rank.\n\n    ``global_rank`` must be part of ``group`` otherwise this raises RuntimeError.\n\n    Args:\n        group (ProcessGroup): ProcessGroup to find the relative rank.\n        global_rank (int): Global rank to query.\n\n    Returns:\n        Group rank of ``global_rank`` relative to ``group``\n\n    N.B. calling this function on the default process group returns identity\n    ",
      "arguments": [
        "group",
        "global_rank"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Translate a global rank into a group rank.\n\n    ``global_rank`` must be part of ``group`` otherwise this raises RuntimeError.\n\n    Args:\n        group (ProcessGroup): ProcessGroup to find the relative rank.\n        global_rank (int): Global rank to query.\n\n    Returns:\n        Group rank of ``global_rank`` relative to ``group``\n\n    N.B. calling this function on the default process group returns identity\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_node_local_rank",
      "signature": "torch.distributed.get_node_local_rank(fallback_rank: Optional[int] = None) -> int",
      "doc": "\n    Return the local rank of the current process relative to the node.\n\n    Semantically, this is a useful concept for mapping processes to devices.\n    For example, on a node with 8 accelerator you could use the node local rank to decide\n    which accelerator device to bind the process to.\n\n    In practice, the actual assignment of node local ranks is handled by the process launcher outside of pytorch,\n    and communicated via the `LOCAL_RANK` environment variable.\n\n    Torchrun will automatically populate `LOCAL_RANK`, but other launchers may not.  If `LOCAL_RANK` is unspecified,\n    this API will fall back to the provided kwarg 'fallback_rank' if specified, otherwise it will raise an error. The\n    intent is to allow writing an application that runs either in single or multi device contexts without error.\n\n    ",
      "arguments": [
        "fallback_rank"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return the local rank of the current process relative to the node.\n\n    Semantically, this is a useful concept for mapping processes to devices.\n    For example, on a node with 8 accelerator you could use the node local rank to decide\n    which accelerator device to bind the process to.\n\n    In practice, the actual assignment of node local ranks is handled by the process launcher outside of pytorch,\n    and communicated via the `LOCAL_RANK` environment variable.\n\n    Torchrun will automatically populate `LOCAL_RANK`, but other launchers may not.  If `LOCAL_RANK` is unspecified,\n    this API will fall back to the provided kwarg 'fallback_rank' if specified, otherwise it will raise an error. The\n    intent is to allow writing an application that runs either in single or multi device contexts without error.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_pg_count",
      "signature": "torch.distributed.get_pg_count() -> int",
      "doc": "\n    Return the number of process groups.\n\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return the number of process groups.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_process_group_ranks",
      "signature": "torch.distributed.get_process_group_ranks(group: torch.distributed.distributed_c10d.ProcessGroup) -> list[int]",
      "doc": "\n    Get all ranks associated with ``group``.\n\n    Args:\n        group (ProcessGroup): ProcessGroup to get all ranks from.\n\n    Returns:\n        List of global ranks ordered by group rank.\n    ",
      "arguments": [
        "group"
      ],
      "return_type": "list[int]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Get all ranks associated with ``group``.\n\n    Args:\n        group (ProcessGroup): ProcessGroup to get all ranks from.\n\n    Returns:\n        List of global ranks ordered by group rank.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_rank",
      "signature": "torch.distributed.get_rank(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None) -> int",
      "doc": "\n    Return the rank of the current process in the provided ``group``, default otherwise.\n\n    Rank is a unique identifier assigned to each process within a distributed\n    process group. They are always consecutive integers ranging from 0 to\n    ``world_size``.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n\n    Returns:\n        The rank of the process group\n        -1, if not part of the group\n\n    ",
      "arguments": [
        "group"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return the rank of the current process in the provided ``group``, default otherwise.\n\n    Rank is a unique identifier assigned to each process within a distributed\n    process group. They are always consecutive integers ranging from 0 to\n    ``world_size``.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n\n    Returns:\n        The rank of the process group\n        -1, if not part of the group\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.get_world_size",
      "signature": "torch.distributed.get_world_size(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None) -> int",
      "doc": "\n    Return the number of processes in the current process group.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n\n    Returns:\n        The world size of the process group\n        -1, if not part of the group\n\n    ",
      "arguments": [
        "group"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return the number of processes in the current process group.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n\n    Returns:\n        The world size of the process group\n        -1, if not part of the group\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.init_device_mesh",
      "signature": "torch.distributed.init_device_mesh(device_type: str, mesh_shape: tuple[int, ...], *, mesh_dim_names: Optional[tuple[str, ...]] = None) -> torch.distributed.device_mesh.DeviceMesh",
      "doc": "\n        Initializes a `DeviceMesh` based on `device_type`, `mesh_shape`, and `mesh_dim_names` parameters.\n\n        This creates a DeviceMesh with an n-dimensional array layout, where `n` is the length of `mesh_shape`.\n        If `mesh_dim_names` is provided, each dimension is labeled as `mesh_dim_names[i]`.\n\n        .. note::\n            `init_device_mesh` follows SPMD programming model, meaning the same PyTorch Python program\n            runs on all processes/ranks in the cluster. Ensure `mesh_shape` (the dimensions of the nD array\n            describing device layout) is identical across all ranks. Inconsistent `mesh_shape` may lead to hanging.\n\n        .. note::\n            If no process group is found, init_device_mesh will initialize distributed process group/groups\n            required for distributed communications behind the scene.\n\n        Args:\n            device_type (str): The device type of the mesh. Currently supports: \"cpu\", \"cuda/cuda-like\".\n                Passing in a device type with a GPU index, such as \"cuda:0\", is not allowed.\n            mesh_shape (Tuple[int]): A tuple defining the dimensions of the multi-dimensional array\n                describing the layout of devices.\n            mesh_dim_names (Tuple[str], optional): A tuple of mesh dimension names to assign to each dimension\n                of the multi-dimensional array describing the layout of devices. Its length must match the length\n                of `mesh_shape`. Each string in `mesh_dim_names` must be unique.\n\n        Returns:\n            DeviceMesh: A :class:`DeviceMesh` object representing the device layout.\n\n        Example::\n            >>> # xdoctest: +SKIP(\"no rank\")\n            >>> from torch.distributed.device_mesh import init_device_mesh\n            >>>\n            >>> mesh_1d = init_device_mesh(\"cuda\", mesh_shape=(8,))\n            >>> mesh_2d = init_device_mesh(\"cuda\", mesh_shape=(2, 8), mesh_dim_names=(\"dp\", \"tp\"))\n\n        ",
      "arguments": [
        "device_type",
        "mesh_shape",
        "mesh_dim_names"
      ],
      "return_type": "<class 'torch.distributed.device_mesh.DeviceMesh'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n        Initializes a `DeviceMesh` based on `device_type`, `mesh_shape`, and `mesh_dim_names` parameters.\n\n        This creates a DeviceMesh with an n-dimensional array layout, where `n` is the length of `mesh_shape`.\n        If `mesh_dim_names` is provided, each dimension is labeled as `mesh_dim_names[i]`.\n\n        .. note::\n            `init_device_mesh` follows SPMD programming model, meaning the same PyTorch Python program\n            runs on all processes/ranks in the cluster. Ensure `mesh_shape` (the dimensions of the nD array\n            describing device layout) is identical across all ranks. Inconsistent `mesh_shape` may lead to hanging.\n\n        .. note::\n            If no process group is found, init_device_mesh will initialize distributed process group/groups\n            required for distributed communications behind the scene.\n\n        Args:\n            device_type (str): The device type of the mesh. Currently supports: \"cpu\", \"cuda/cuda-like\".\n                Passing in a device type with a GPU index, such as \"cuda:0\", is not allowed.\n            mesh_shape (Tuple[int]): A tuple defining the dimensions of the multi-dimensional array\n                describing the layout of devices.\n            mesh_dim_names (Tuple[str], optional): A tuple of mesh dimension names to assign to each dimension\n                of the multi-dimensional array describing the layout of devices. Its length must match the length\n                of `mesh_shape`. Each string in `mesh_dim_names` must be unique.\n\n        Returns:\n            DeviceMesh: A :class:`DeviceMesh` object representing the device layout.\n\n        Example::\n            >>> # xdoctest: +SKIP(\"no rank\")\n            >>> from torch.distributed.device_mesh import init_device_mesh\n            >>>\n            >>> mesh_1d = init_device_mesh(\"cuda\", mesh_shape=(8,))\n            >>> mesh_2d = init_device_mesh(\"cuda\", mesh_shape=(2, 8), mesh_dim_names=(\"dp\", \"tp\"))\n\n        ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.init_process_group",
      "signature": "torch.distributed.init_process_group(backend: Optional[str] = None, init_method: Optional[str] = None, timeout: Optional[datetime.timedelta] = None, world_size: int = -1, rank: int = -1, store: Optional[torch.distributed.distributed_c10d.Store] = None, group_name: str = '', pg_options: Optional[Any] = None, device_id: Optional[torch.device] = None) -> None",
      "doc": "\n    Initialize the default distributed process group.\n\n    This will also initialize the distributed package.\n\n    There are 2 main ways to initialize a process group:\n        1. Specify ``store``, ``rank``, and ``world_size`` explicitly.\n        2. Specify ``init_method`` (a URL string) which indicates where/how\n           to discover peers. Optionally specify ``rank`` and ``world_size``,\n           or encode all required parameters in the URL and omit them.\n\n    If neither is specified, ``init_method`` is assumed to be \"env://\".\n\n\n    Args:\n        backend (str or Backend, optional): The backend to use. Depending on\n            build-time configurations, valid values include ``mpi``, ``gloo``,\n            ``nccl``, ``ucc``, or one that is registered by a third-party\n            plugin.\n            Since 2.6, if ``backend`` is not provided, c10d will use a backend\n            registered for the device type indicated by the `device_id` kwarg\n            (if provided). The known default registrations today are: ``nccl``\n            for ``cuda``, ``gloo`` for ``cpu``.\n            If neither ``backend`` nor ``device_id`` is provided, c10d will\n            detect the accelerator on the run-time machine and use a backend\n            registered for that detected accelerator (or ``cpu``).\n            This field can be given as a lowercase string (e.g., ``\"gloo\"``),\n            which can also be accessed via :class:`Backend` attributes (e.g.,\n            ``Backend.GLOO``).\n            If using multiple processes per machine with ``nccl`` backend, each\n            process must have exclusive access to every GPU it uses, as sharing\n            GPUs between processes can result in deadlock or NCCL invalid usage.\n            ``ucc`` backend is experimental.\n        init_method (str, optional): URL specifying how to initialize the\n                                     process group. Default is \"env://\" if no\n                                     ``init_method`` or ``store`` is specified.\n                                     Mutually exclusive with ``store``.\n        world_size (int, optional): Number of processes participating in\n                                    the job. Required if ``store`` is specified.\n        rank (int, optional): Rank of the current process (it should be a\n                              number between 0 and ``world_size``-1).\n                              Required if ``store`` is specified.\n        store(Store, optional): Key/value store accessible to all workers, used\n                                to exchange connection/address information.\n                                Mutually exclusive with ``init_method``.\n        timeout (timedelta, optional): Timeout for operations executed against\n            the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends.\n            This is the duration after which collectives will be aborted asynchronously and the process will crash.\n            This is done since CUDA execution is async and it is no longer safe to continue executing user code since\n            failed async NCCL operations might result in subsequent CUDA operations running on corrupted data.\n            When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.\n\n        group_name (str, optional, deprecated): Group name. This argument is ignored\n        pg_options (ProcessGroupOptions, optional): process group options\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. As of now, the only\n            options we support is ``ProcessGroupNCCL.Options`` for the ``nccl``\n            backend, ``is_high_priority_stream`` can be specified so that\n            the nccl backend can pick up high priority cuda streams when\n            there're compute kernels waiting. For other availble options to config nccl,\n            See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t\n        device_id (torch.device, optional): a single, specific device\n            to \"bind\" this process to, allowing for backend-specific\n            optimizations.  Currently this has two effects, only under\n            NCCL: the communicator is immediately formed (calling\n            ``ncclCommInit*`` immediately rather than the normal lazy\n            call) and sub-groups will use ``ncclCommSplit`` when\n            possible to avoid unnecessary overhead of group creation. If you\n            want to know NCCL initialization error early, you can also use this\n            field.\n\n    .. note:: To enable ``backend == Backend.MPI``, PyTorch needs to be built from source\n        on a system that supports MPI.\n\n    .. note:: Support for multiple backends is experimental. Currently when no backend is\n        specified, both ``gloo`` and ``nccl`` backends will be created. The ``gloo`` backend\n        will be used for collectives with CPU tensors and the ``nccl`` backend will be used\n        for collectives with CUDA tensors. A custom backend can be specified by passing in\n        a string with format \"<device_type>:<backend_name>,<device_type>:<backend_name>\", e.g.\n        \"cpu:gloo,cuda:custom_backend\".\n\n    ",
      "arguments": [
        "backend",
        "init_method",
        "timeout",
        "world_size",
        "rank",
        "store",
        "group_name",
        "pg_options",
        "device_id"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Initialize the default distributed process group.\n\n    This will also initialize the distributed package.\n\n    There are 2 main ways to initialize a process group:\n        1. Specify ``store``, ``rank``, and ``world_size`` explicitly.\n        2. Specify ``init_method`` (a URL string) which indicates where/how\n           to discover peers. Optionally specify ``rank`` and ``world_size``,\n           or encode all required parameters in the URL and omit them.\n\n    If neither is specified, ``init_method`` is assumed to be \"env://\".\n\n\n    Args:\n        backend (str or Backend, optional): The backend to use. Depending on\n            build-time configurations, valid values include ``mpi``, ``gloo``,\n            ``nccl``, ``ucc``, or one that is registered by a third-party\n            plugin.\n            Since 2.6, if ``backend`` is not provided, c10d will use a backend\n            registered for the device type indicated by the `device_id` kwarg\n            (if provided). The known default registrations today are: ``nccl``\n            for ``cuda``, ``gloo`` for ``cpu``.\n            If neither ``backend`` nor ``device_id`` is provided, c10d will\n            detect the accelerator on the run-time machine and use a backend\n            registered for that detected accelerator (or ``cpu``).\n            This field can be given as a lowercase string (e.g., ``\"gloo\"``),\n            which can also be accessed via :class:`Backend` attributes (e.g.,\n            ``Backend.GLOO``).\n            If using multiple processes per machine with ``nccl`` backend, each\n            process must have exclusive access to every GPU it uses, as sharing\n            GPUs between processes can result in deadlock or NCCL invalid usage.\n            ``ucc`` backend is experimental.\n        init_method (str, optional): URL specifying how to initialize the\n                                     process group. Default is \"env://\" if no\n                                     ``init_method`` or ``store`` is specified.\n                                     Mutually exclusive with ``store``.\n        world_size (int, optional): Number of processes participating in\n                                    the job. Required if ``store`` is specified.\n        rank (int, optional): Rank of the current process (it should be a\n                              number between 0 and ``world_size``-1).\n                              Required if ``store`` is specified.\n        store(Store, optional): Key/value store accessible to all workers, used\n                                to exchange connection/address information.\n                                Mutually exclusive with ``init_method``.\n        timeout (timedelta, optional): Timeout for operations executed against\n            the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends.\n            This is the duration after which collectives will be aborted asynchronously and the process will crash.\n            This is done since CUDA execution is async and it is no longer safe to continue executing user code since\n            failed async NCCL operations might result in subsequent CUDA operations running on corrupted data.\n            When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.\n\n        group_name (str, optional, deprecated): Group name. This argument is ignored\n        pg_options (ProcessGroupOptions, optional): process group options\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. As of now, the only\n            options we support is ``ProcessGroupNCCL.Options`` for the ``nccl``\n            backend, ``is_high_priority_stream`` can be specified so that\n            the nccl backend can pick up high priority cuda streams when\n            there're compute kernels waiting. For other availble options to config nccl,\n            See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t\n        device_id (torch.device, optional): a single, specific device\n            to \"bind\" this process to, allowing for backend-specific\n            optimizations.  Currently this has two effects, only under\n            NCCL: the communicator is immediately formed (calling\n            ``ncclCommInit*`` immediately rather than the normal lazy\n            call) and sub-groups will use ``ncclCommSplit`` when\n            possible to avoid unnecessary overhead of group creation. If you\n            want to know NCCL initialization error early, you can also use this\n            field.\n\n    .. note:: To enable ``backend == Backend.MPI``, PyTorch needs to be built from source\n        on a system that supports MPI.\n\n    .. note:: Support for multiple backends is experimental. Currently when no backend is\n        specified, both ``gloo`` and ``nccl`` backends will be created. The ``gloo`` backend\n        will be used for collectives with CPU tensors and the ``nccl`` backend will be used\n        for collectives with CUDA tensors. A custom backend can be specified by passing in\n        a string with format \"<device_type>:<backend_name>,<device_type>:<backend_name>\", e.g.\n        \"cpu:gloo,cuda:custom_backend\".\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.irecv",
      "signature": "torch.distributed.irecv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, tag: int = 0, group_src: Optional[int] = None) -> Optional[torch.distributed.distributed_c10d.Work]",
      "doc": "\n    Receives a tensor asynchronously.\n\n    .. warning::\n        ``tag`` is not supported with the NCCL backend.\n\n    Unlike recv, which is blocking, irecv allows src == dst rank, i.e. recv from self.\n\n    Args:\n        tensor (Tensor): Tensor to fill with received data.\n        src (int, optional): Source rank on global process group (regardless of ``group`` argument).\n            Will receive from any process if unspecified.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        tag (int, optional): Tag to match recv with remote send\n        group_src (int, optional): Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``.\n\n    Returns:\n        A distributed request object.\n        None, if not part of the group\n\n    ",
      "arguments": [
        "tensor",
        "src",
        "group",
        "tag",
        "group_src"
      ],
      "return_type": "typing.Optional[torch.distributed.distributed_c10d.Work]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Receives a tensor asynchronously.\n\n    .. warning::\n        ``tag`` is not supported with the NCCL backend.\n\n    Unlike recv, which is blocking, irecv allows src == dst rank, i.e. recv from self.\n\n    Args:\n        tensor (Tensor): Tensor to fill with received data.\n        src (int, optional): Source rank on global process group (regardless of ``group`` argument).\n            Will receive from any process if unspecified.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        tag (int, optional): Tag to match recv with remote send\n        group_src (int, optional): Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``.\n\n    Returns:\n        A distributed request object.\n        None, if not part of the group\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_available",
      "signature": "torch.distributed.is_available() -> bool",
      "doc": "\n    Return ``True`` if the distributed package is available.\n\n    Otherwise,\n    ``torch.distributed`` does not expose any other APIs. Currently,\n    ``torch.distributed`` is available on Linux, MacOS and Windows. Set\n    ``USE_DISTRIBUTED=1`` to enable it when building PyTorch from source.\n    Currently, the default value is ``USE_DISTRIBUTED=1`` for Linux and Windows,\n    ``USE_DISTRIBUTED=0`` for MacOS.\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return ``True`` if the distributed package is available.\n\n    Otherwise,\n    ``torch.distributed`` does not expose any other APIs. Currently,\n    ``torch.distributed`` is available on Linux, MacOS and Windows. Set\n    ``USE_DISTRIBUTED=1`` to enable it when building PyTorch from source.\n    Currently, the default value is ``USE_DISTRIBUTED=1`` for Linux and Windows,\n    ``USE_DISTRIBUTED=0`` for MacOS.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_backend_available",
      "signature": "torch.distributed.is_backend_available(backend: str) -> bool",
      "doc": "\n    Check backend availability.\n\n    Checks if the given backend is available and supports the built-in backends or\n    third-party backends through function ``Backend.register_backend``.\n\n    Args:\n        backend (str): Backend name.\n    Returns:\n        bool: Returns true if the backend is available otherwise false.\n    ",
      "arguments": [
        "backend"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Check backend availability.\n\n    Checks if the given backend is available and supports the built-in backends or\n    third-party backends through function ``Backend.register_backend``.\n\n    Args:\n        backend (str): Backend name.\n    Returns:\n        bool: Returns true if the backend is available otherwise false.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_gloo_available",
      "signature": "torch.distributed.is_gloo_available() -> bool",
      "doc": "Check if the Gloo backend is available.",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if the Gloo backend is available.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_initialized",
      "signature": "torch.distributed.is_initialized() -> bool",
      "doc": "Check if the default process group has been initialized.",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if the default process group has been initialized.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_mpi_available",
      "signature": "torch.distributed.is_mpi_available() -> bool",
      "doc": "Check if the MPI backend is available.",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if the MPI backend is available.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_nccl_available",
      "signature": "torch.distributed.is_nccl_available() -> bool",
      "doc": "Check if the NCCL backend is available.",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if the NCCL backend is available.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_torchelastic_launched",
      "signature": "torch.distributed.is_torchelastic_launched() -> bool",
      "doc": "\n    Check whether this process was launched with ``torch.distributed.elastic`` (aka torchelastic).\n\n    The existence of ``TORCHELASTIC_RUN_ID`` environment\n    variable is used as a proxy to determine whether the current process\n    was launched with torchelastic. This is a reasonable proxy since\n    ``TORCHELASTIC_RUN_ID`` maps to the rendezvous id which is always a\n    non-null value indicating the job id for peer discovery purposes..\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Check whether this process was launched with ``torch.distributed.elastic`` (aka torchelastic).\n\n    The existence of ``TORCHELASTIC_RUN_ID`` environment\n    variable is used as a proxy to determine whether the current process\n    was launched with torchelastic. This is a reasonable proxy since\n    ``TORCHELASTIC_RUN_ID`` maps to the rendezvous id which is always a\n    non-null value indicating the job id for peer discovery purposes..\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_ucc_available",
      "signature": "torch.distributed.is_ucc_available() -> bool",
      "doc": "Check if the UCC backend is available.",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if the UCC backend is available.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.is_xccl_available",
      "signature": "torch.distributed.is_xccl_available() -> bool",
      "doc": "Check if the XCCL backend is available.",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Check if the XCCL backend is available.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.isend",
      "signature": "torch.distributed.isend(tensor: torch.Tensor, dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, tag: int = 0, group_dst: Optional[int] = None) -> Optional[torch.distributed.distributed_c10d.Work]",
      "doc": "\n    Send a tensor asynchronously.\n\n    .. warning::\n        Modifying ``tensor`` before the request completes causes undefined\n        behavior.\n\n    .. warning::\n        ``tag`` is not supported with the NCCL backend.\n\n    Unlike send, which is blocking, isend allows src == dst rank, i.e. send to self.\n\n    Args:\n        tensor (Tensor): Tensor to send.\n        dst (int): Destination rank on global process group (regardless of ``group`` argument)\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        tag (int, optional): Tag to match send with remote recv\n        group_dst (int, optional): Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``\n\n    Returns:\n        A distributed request object.\n        None, if not part of the group\n\n    ",
      "arguments": [
        "tensor",
        "dst",
        "group",
        "tag",
        "group_dst"
      ],
      "return_type": "typing.Optional[torch.distributed.distributed_c10d.Work]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Send a tensor asynchronously.\n\n    .. warning::\n        Modifying ``tensor`` before the request completes causes undefined\n        behavior.\n\n    .. warning::\n        ``tag`` is not supported with the NCCL backend.\n\n    Unlike send, which is blocking, isend allows src == dst rank, i.e. send to self.\n\n    Args:\n        tensor (Tensor): Tensor to send.\n        dst (int): Destination rank on global process group (regardless of ``group`` argument)\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        tag (int, optional): Tag to match send with remote recv\n        group_dst (int, optional): Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``\n\n    Returns:\n        A distributed request object.\n        None, if not part of the group\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.monitored_barrier",
      "signature": "torch.distributed.monitored_barrier(group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, timeout=None, wait_all_ranks=False)",
      "doc": "\n    Synchronize processes similar to ``torch.distributed.barrier``, but consider a configurable timeout.\n\n    It is able to report ranks that did not pass this barrier within the provided timeout.\n    Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0.\n    Rank 0 will block until all send /recv from other ranks are processed, and will report\n    failures for ranks that failed to respond in time. Note that if one rank does not reach the\n    monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.\n\n    This collective will block all processes/ranks in the group, until the\n    whole group exits the function successfully, making it useful for debugging\n    and synchronizing. However, it can have a performance impact and should only\n    be used for debugging or scenarios that require full synchronization points\n    on the host-side. For debugging purposes, this barrier can be inserted\n    before the application's collective calls to check if any ranks are\n    desynchronized.\n\n    .. note:: Note that this collective is only supported with the GLOO backend.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If\n            ``None``, the default process group will be used.\n        timeout (datetime.timedelta, optional): Timeout for monitored_barrier.\n            If ``None``, the default process group timeout will be used.\n        wait_all_ranks (bool, optional): Whether to collect all failed ranks or\n            not. By default, this is ``False`` and ``monitored_barrier`` on rank 0\n            will throw on the first failed rank it encounters in order to fail\n            fast. By setting ``wait_all_ranks=True`` ``monitored_barrier`` will\n            collect all failed ranks and throw an error containing information\n            about all failed ranks.\n\n    Returns:\n        ``None``.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> if dist.get_rank() != 1:\n        >>>     dist.monitored_barrier() # Raises exception indicating that\n        >>> # rank 1 did not call into monitored_barrier.\n        >>> # Example with wait_all_ranks=True\n        >>> if dist.get_rank() == 0:\n        >>>     dist.monitored_barrier(wait_all_ranks=True) # Raises exception\n        >>> # indicating that ranks 1, 2, ... world_size - 1 did not call into\n        >>> # monitored_barrier.\n    ",
      "arguments": [
        "group",
        "timeout",
        "wait_all_ranks"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Synchronize processes similar to ``torch.distributed.barrier``, but consider a configurable timeout.\n\n    It is able to report ranks that did not pass this barrier within the provided timeout.\n    Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0.\n    Rank 0 will block until all send /recv from other ranks are processed, and will report\n    failures for ranks that failed to respond in time. Note that if one rank does not reach the\n    monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.\n\n    This collective will block all processes/ranks in the group, until the\n    whole group exits the function successfully, making it useful for debugging\n    and synchronizing. However, it can have a performance impact and should only\n    be used for debugging or scenarios that require full synchronization points\n    on the host-side. For debugging purposes, this barrier can be inserted\n    before the application's collective calls to check if any ranks are\n    desynchronized.\n\n    .. note:: Note that this collective is only supported with the GLOO backend.\n\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If\n            ``None``, the default process group will be used.\n        timeout (datetime.timedelta, optional): Timeout for monitored_barrier.\n            If ``None``, the default process group timeout will be used.\n        wait_all_ranks (bool, optional): Whether to collect all failed ranks or\n            not. By default, this is ``False`` and ``monitored_barrier`` on rank 0\n            will throw on the first failed rank it encounters in order to fail\n            fast. By setting ``wait_all_ranks=True`` ``monitored_barrier`` will\n            collect all failed ranks and throw an error containing information\n            about all failed ranks.\n\n    Returns:\n        ``None``.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> if dist.get_rank() != 1:\n        >>>     dist.monitored_barrier() # Raises exception indicating that\n        >>> # rank 1 did not call into monitored_barrier.\n        >>> # Example with wait_all_ranks=True\n        >>> if dist.get_rank() == 0:\n        >>>     dist.monitored_barrier(wait_all_ranks=True) # Raises exception\n        >>> # indicating that ranks 1, 2, ... world_size - 1 did not call into\n        >>> # monitored_barrier.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.new_group",
      "signature": "torch.distributed.new_group(ranks=None, timeout=None, backend=None, pg_options=None, use_local_synchronization=False, group_desc=None, device_id: Optional[torch.device] = None)",
      "doc": "\n    Create a new distributed group.\n\n    This function requires that all processes in the main group (i.e. all\n    processes that are part of the distributed job) enter this function, even\n    if they are not going to be members of the group. Additionally, groups\n    should be created in the same order in all processes.\n\n    .. warning::\n        Safe concurrent usage:\n        When using multiple process groups with the ``NCCL`` backend, the user\n        must ensure a globally consistent execution order of collectives across\n        ranks.\n\n        If multiple threads within a process issue collectives, explicit\n        synchronization is necessary to ensure consistent ordering.\n\n        When using async variants of torch.distributed communication APIs,\n        a work object is returned and the communication kernel is\n        enqueued on a separate CUDA stream, allowing overlap of communication\n        and computation. Once one or more async ops have been issued on one process\n        group, they must be synchronized with other cuda streams by calling `work.wait()`\n        before using another process group.\n\n        See `Using multiple NCCL communicators concurrently <https://docs.nvid\n        ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using\n        -multiple-nccl-communicators-concurrently>`_ for more details.\n\n    Args:\n        ranks (list[int]): List of ranks of group members. If ``None``, will be\n            set to all ranks. Default is ``None``.\n        timeout (timedelta, optional): see `init_process_group` for details and default value.\n        backend (str or Backend, optional): The backend to use. Depending on\n            build-time configurations, valid values are ``gloo`` and ``nccl``.\n            By default uses the same backend as the global group. This field\n            should be given as a lowercase string (e.g., ``\"gloo\"``), which can\n            also be accessed via :class:`Backend` attributes (e.g.,\n            ``Backend.GLOO``). If ``None`` is passed in, the backend\n            corresponding to the default process group will be used. Default is\n            ``None``.\n        pg_options (ProcessGroupOptions, optional): process group options\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. i.e. for the ``nccl``\n            backend, ``is_high_priority_stream`` can be specified so that\n            process group can pick up high priority cuda streams. For other availble options to config nccl,\n            See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t\n        use_local_synchronization (bool, optional): perform a group-local\n            barrier at the end of the process group creation. This is different\n            in that non-member ranks don't need to call into API and don't\n            join the barrier.\n        group_desc (str, optional): a string to describe the process group.\n        device_id (torch.device, optional): a single, specific device\n            to \"bind\" this process to,  The `new_group` call will try to initialize\n            a communication backend immediately for the device if this field is given.\n\n    Returns:\n        A handle of distributed group that can be given to collective calls or\n        GroupMember.NON_GROUP_MEMBER if the rank is not part of ``ranks``.\n\n    N.B. use_local_synchronization doesn't work with MPI.\n\n    N.B. While use_local_synchronization=True can be significantly faster with larger\n    clusters and small process groups, care must be taken since it changes cluster behavior\n    as non-member ranks don't join the group barrier().\n\n    N.B. use_local_synchronization=True can lead to deadlocks when each rank creates\n    multiple overlaping process groups. To avoid that, make sure all ranks follow the\n    same global creation order.\n    ",
      "arguments": [
        "ranks",
        "timeout",
        "backend",
        "pg_options",
        "use_local_synchronization",
        "group_desc",
        "device_id"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Create a new distributed group.\n\n    This function requires that all processes in the main group (i.e. all\n    processes that are part of the distributed job) enter this function, even\n    if they are not going to be members of the group. Additionally, groups\n    should be created in the same order in all processes.\n\n    .. warning::\n        Safe concurrent usage:\n        When using multiple process groups with the ``NCCL`` backend, the user\n        must ensure a globally consistent execution order of collectives across\n        ranks.\n\n        If multiple threads within a process issue collectives, explicit\n        synchronization is necessary to ensure consistent ordering.\n\n        When using async variants of torch.distributed communication APIs,\n        a work object is returned and the communication kernel is\n        enqueued on a separate CUDA stream, allowing overlap of communication\n        and computation. Once one or more async ops have been issued on one process\n        group, they must be synchronized with other cuda streams by calling `work.wait()`\n        before using another process group.\n\n        See `Using multiple NCCL communicators concurrently <https://docs.nvid\n        ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using\n        -multiple-nccl-communicators-concurrently>`_ for more details.\n\n    Args:\n        ranks (list[int]): List of ranks of group members. If ``None``, will be\n            set to all ranks. Default is ``None``.\n        timeout (timedelta, optional): see `init_process_group` for details and default value.\n        backend (str or Backend, optional): The backend to use. Depending on\n            build-time configurations, valid values are ``gloo`` and ``nccl``.\n            By default uses the same backend as the global group. This field\n            should be given as a lowercase string (e.g., ``\"gloo\"``), which can\n            also be accessed via :class:`Backend` attributes (e.g.,\n            ``Backend.GLOO``). If ``None`` is passed in, the backend\n            corresponding to the default process group will be used. Default is\n            ``None``.\n        pg_options (ProcessGroupOptions, optional): process group options\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. i.e. for the ``nccl``\n            backend, ``is_high_priority_stream`` can be specified so that\n            process group can pick up high priority cuda streams. For other availble options to config nccl,\n            See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t\n        use_local_synchronization (bool, optional): perform a group-local\n            barrier at the end of the process group creation. This is different\n            in that non-member ranks don't need to call into API and don't\n            join the barrier.\n        group_desc (str, optional): a string to describe the process group.\n        device_id (torch.device, optional): a single, specific device\n            to \"bind\" this process to,  The `new_group` call will try to initialize\n            a communication backend immediately for the device if this field is given.\n\n    Returns:\n        A handle of distributed group that can be given to collective calls or\n        GroupMember.NON_GROUP_MEMBER if the rank is not part of ``ranks``.\n\n    N.B. use_local_synchronization doesn't work with MPI.\n\n    N.B. While use_local_synchronization=True can be significantly faster with larger\n    clusters and small process groups, care must be taken since it changes cluster behavior\n    as non-member ranks don't join the group barrier().\n\n    N.B. use_local_synchronization=True can lead to deadlocks when each rank creates\n    multiple overlaping process groups. To avoid that, make sure all ranks follow the\n    same global creation order.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.new_subgroups",
      "signature": "torch.distributed.new_subgroups(group_size=None, group=None, timeout=None, backend=None, pg_options=None, group_desc=None)",
      "doc": "\n    Create subgroups of equal size.\n\n    By default, it creates intra-machine subgroups,\n    where each of which contains all the ranks of a machine, based on the assumption\n    that each machine has the same number of devices.\n\n    This is a convenience API that calls ``new_group`` to generate multiple subgroups.\n    It requires that all processes in the main group (i.e. all\n    processes that are part of the distributed job) enter this function, even\n    if they are not going to be members of the group.\n\n    .. warning::\n        If ``group_size`` is passed in, the world size must be divisible by ``group_size``.\n        If no ``group_size`` is passed in, it believe that you are creating a group based\n        on CUDA and determining the group size by number of CUDA devices, and if not all\n        the machines have the same number of devices, the subgroup division will be\n        different across nodes and can cause unexpected behaviors. Therefore, if you are\n        creating a subgroup that does not depend on CUDA (such as Gloo on CPU), please\n        pass in ``group_size`` correctly.\n\n    .. warning::\n        See warning `Safe concurrent usage` for `new_group` API for important details about\n        using multiple process groups concurrently in a safe manner.\n\n    Args:\n        group_size (int, optional): The size of each subgroup. If ``None``,\n            the default subgroup size is equal to the number of devices on each machine,\n            based on the assumption that each machine has exactly the same\n            number of devices. Default is ``None``.\n        timeout (timedelta, optional): see `init_process_group` for details and default value.\n        backend (str or Backend, optional): The backend to use. Depending on\n            build-time configurations, valid values are ``gloo`` and ``nccl``.\n            By default uses the same backend as the global group. This field\n            should be given as a lowercase string (e.g., ``\"gloo\"``), which can\n            also be accessed via :class:`Backend` attributes (e.g.,\n            ``Backend.GLOO``). If ``None`` is passed in, the backend\n            corresponding to the default process group will be used. Default is\n            ``None``.\n        pg_options (ProcessGroupOptions, optional): process group options\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. i.e. for the ``nccl``\n            backend, ``is_high_priority_stream`` can be specified so that\n            process group can pick up high priority cuda streams.\n        group_desc (str, optional): A string describing the group. Each subgroup will\n            inherit its group_desc\n\n    Returns:\n        The subgroup containing the current rank, and all the subgroups used for cleanup.\n\n    Examples:\n        >>> # Create intra-machine subgroups.\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> cur_subgroup, subgroups = dist.new_subgroups()\n        >>> # Allreduce within the machine.\n        >>> rank = dist.get_rank()\n        >>> tensor = torch.ones(1, device=rank) * rank\n        >>> dist.all_reduce(tensor, group=cur_subgroup)\n        >>> tensor\n        tensor([28])  # Assume 8 CUDA devices per machine.  28 is sum(range(8)).\n        >>> # Cleanup.\n        >>> for subgroup in subgroups:\n        >>>     dist.destroy_process_group(subgroup)\n    ",
      "arguments": [
        "group_size",
        "group",
        "timeout",
        "backend",
        "pg_options",
        "group_desc"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Create subgroups of equal size.\n\n    By default, it creates intra-machine subgroups,\n    where each of which contains all the ranks of a machine, based on the assumption\n    that each machine has the same number of devices.\n\n    This is a convenience API that calls ``new_group`` to generate multiple subgroups.\n    It requires that all processes in the main group (i.e. all\n    processes that are part of the distributed job) enter this function, even\n    if they are not going to be members of the group.\n\n    .. warning::\n        If ``group_size`` is passed in, the world size must be divisible by ``group_size``.\n        If no ``group_size`` is passed in, it believe that you are creating a group based\n        on CUDA and determining the group size by number of CUDA devices, and if not all\n        the machines have the same number of devices, the subgroup division will be\n        different across nodes and can cause unexpected behaviors. Therefore, if you are\n        creating a subgroup that does not depend on CUDA (such as Gloo on CPU), please\n        pass in ``group_size`` correctly.\n\n    .. warning::\n        See warning `Safe concurrent usage` for `new_group` API for important details about\n        using multiple process groups concurrently in a safe manner.\n\n    Args:\n        group_size (int, optional): The size of each subgroup. If ``None``,\n            the default subgroup size is equal to the number of devices on each machine,\n            based on the assumption that each machine has exactly the same\n            number of devices. Default is ``None``.\n        timeout (timedelta, optional): see `init_process_group` for details and default value.\n        backend (str or Backend, optional): The backend to use. Depending on\n            build-time configurations, valid values are ``gloo`` and ``nccl``.\n            By default uses the same backend as the global group. This field\n            should be given as a lowercase string (e.g., ``\"gloo\"``), which can\n            also be accessed via :class:`Backend` attributes (e.g.,\n            ``Backend.GLOO``). If ``None`` is passed in, the backend\n            corresponding to the default process group will be used. Default is\n            ``None``.\n        pg_options (ProcessGroupOptions, optional): process group options\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. i.e. for the ``nccl``\n            backend, ``is_high_priority_stream`` can be specified so that\n            process group can pick up high priority cuda streams.\n        group_desc (str, optional): A string describing the group. Each subgroup will\n            inherit its group_desc\n\n    Returns:\n        The subgroup containing the current rank, and all the subgroups used for cleanup.\n\n    Examples:\n        >>> # Create intra-machine subgroups.\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> cur_subgroup, subgroups = dist.new_subgroups()\n        >>> # Allreduce within the machine.\n        >>> rank = dist.get_rank()\n        >>> tensor = torch.ones(1, device=rank) * rank\n        >>> dist.all_reduce(tensor, group=cur_subgroup)\n        >>> tensor\n        tensor([28])  # Assume 8 CUDA devices per machine.  28 is sum(range(8)).\n        >>> # Cleanup.\n        >>> for subgroup in subgroups:\n        >>>     dist.destroy_process_group(subgroup)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.new_subgroups_by_enumeration",
      "signature": "torch.distributed.new_subgroups_by_enumeration(ranks_per_subgroup_list, timeout=None, backend=None, pg_options=None, group_desc=None)",
      "doc": "\n    Create subgroups by dividing the global world.\n\n    The division is specified by a nested list of ranks. The subgroups cannot have\n    overlap, and some ranks may not have to be in any subgroup.\n\n    This is a convenience API that calls ``new_group`` to generate multiple subgroups.\n    It requires that all processes in the main group (i.e. all\n    processes that are part of the distributed job) enter this function, even\n    if they are not going to be members of the group.\n\n    .. warning::\n        See warning `Safe concurrent usage` for `new_group` API for important details about\n        using multiple process groups concurrently in a safe manner.\n\n    Args:\n        ranks_per_subgroup_list (list[list[int]]): A nested list of ranks of\n            group members.\n        timeout (timedelta, optional): see `init_process_group` for details and default value.\n        backend (str or Backend, optional): The backend to use. Depending on\n             build-time configurations, valid values are ``gloo`` and ``nccl``.\n             By default uses the same backend as the global group. This field\n             should be given as a lowercase string (e.g., ``\"gloo\"``), which can\n             also be accessed via :class:`Backend` attributes (e.g.,\n             ``Backend.GLOO``). If ``None`` is passed in, the backend\n             corresponding to the default process group will be used. Default is\n             ``None``.\n        pg_options (ProcessGroupOptions, optional): process group options\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. i.e. for the ``nccl``\n            backend, ``is_high_priority_stream`` can be specified so that\n            process group can pick up high priority cuda streams.\n        group_desc (str, optional): A string describing the group. Each subgroup will\n            inherit its group_desc.\n\n    Returns:\n        The subgroup containing the current rank, and all the subgroups used for cleanup.\n\n    Examples:\n        >>> # Create two subgroups, where each has 2 processes.\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> cur_subgroup, subgroups = dist.new_subgroups(ranks=[[0, 2], [1, 3]])\n        >>> rank = dist.get_rank()\n        >>> tensor = torch.ones(1, device=rank) * rank\n        >>> dist.all_reduce(tensor, group=cur_subgroup)\n        >>> tensor\n        tensor([2])     # Subgroup 0: ranks 0 and 2\n        tensor([4])     # Subgroup 1: ranks 1 and 3\n    ",
      "arguments": [
        "ranks_per_subgroup_list",
        "timeout",
        "backend",
        "pg_options",
        "group_desc"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Create subgroups by dividing the global world.\n\n    The division is specified by a nested list of ranks. The subgroups cannot have\n    overlap, and some ranks may not have to be in any subgroup.\n\n    This is a convenience API that calls ``new_group`` to generate multiple subgroups.\n    It requires that all processes in the main group (i.e. all\n    processes that are part of the distributed job) enter this function, even\n    if they are not going to be members of the group.\n\n    .. warning::\n        See warning `Safe concurrent usage` for `new_group` API for important details about\n        using multiple process groups concurrently in a safe manner.\n\n    Args:\n        ranks_per_subgroup_list (list[list[int]]): A nested list of ranks of\n            group members.\n        timeout (timedelta, optional): see `init_process_group` for details and default value.\n        backend (str or Backend, optional): The backend to use. Depending on\n             build-time configurations, valid values are ``gloo`` and ``nccl``.\n             By default uses the same backend as the global group. This field\n             should be given as a lowercase string (e.g., ``\"gloo\"``), which can\n             also be accessed via :class:`Backend` attributes (e.g.,\n             ``Backend.GLOO``). If ``None`` is passed in, the backend\n             corresponding to the default process group will be used. Default is\n             ``None``.\n        pg_options (ProcessGroupOptions, optional): process group options\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. i.e. for the ``nccl``\n            backend, ``is_high_priority_stream`` can be specified so that\n            process group can pick up high priority cuda streams.\n        group_desc (str, optional): A string describing the group. Each subgroup will\n            inherit its group_desc.\n\n    Returns:\n        The subgroup containing the current rank, and all the subgroups used for cleanup.\n\n    Examples:\n        >>> # Create two subgroups, where each has 2 processes.\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> cur_subgroup, subgroups = dist.new_subgroups(ranks=[[0, 2], [1, 3]])\n        >>> rank = dist.get_rank()\n        >>> tensor = torch.ones(1, device=rank) * rank\n        >>> dist.all_reduce(tensor, group=cur_subgroup)\n        >>> tensor\n        tensor([2])     # Subgroup 0: ranks 0 and 2\n        tensor([4])     # Subgroup 1: ranks 1 and 3\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.recv",
      "signature": "torch.distributed.recv(tensor: torch.Tensor, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, tag: int = 0, group_src: Optional[int] = None) -> int",
      "doc": "\n    Receives a tensor synchronously.\n\n    .. warning::\n        ``tag`` is not supported with the NCCL backend.\n\n    Args:\n        tensor (Tensor): Tensor to fill with received data.\n        src (int, optional): Source rank on global process group (regardless of ``group`` argument).\n            Will receive from any process if unspecified.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        tag (int, optional): Tag to match recv with remote send\n        group_src (int, optional): Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``.\n    Returns:\n        Sender rank\n        -1, if not part of the group\n\n    ",
      "arguments": [
        "tensor",
        "src",
        "group",
        "tag",
        "group_src"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Receives a tensor synchronously.\n\n    .. warning::\n        ``tag`` is not supported with the NCCL backend.\n\n    Args:\n        tensor (Tensor): Tensor to fill with received data.\n        src (int, optional): Source rank on global process group (regardless of ``group`` argument).\n            Will receive from any process if unspecified.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        tag (int, optional): Tag to match recv with remote send\n        group_src (int, optional): Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``.\n    Returns:\n        Sender rank\n        -1, if not part of the group\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.recv_object_list",
      "signature": "torch.distributed.recv_object_list(object_list: list[typing.Any], src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, device: Optional[torch.device] = None, group_src: Optional[int] = None)",
      "doc": "\n    Receives picklable objects in ``object_list`` synchronously.\n\n    Similar to :func:`recv`, but can receive Python objects.\n\n    Args:\n        object_list (List[Any]): List of objects to receive into.\n            Must provide a list of sizes equal to the size of the list being sent.\n        src (int, optional): Source rank from which to recv ``object_list``.\n            Source rank is based on global process group (regardless of ``group`` argument)\n            Will receive from any rank if set to None. Default is ``None``.\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        device (``torch.device``, optional): If not None, receives on this device.\n            Default is ``None``.\n        group_src (int, optional): Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``.\n\n    Returns:\n        Sender rank. -1 if rank is not part of the group. If rank is part of the group,\n        ``object_list`` will contain the sent objects from ``src`` rank.\n\n    .. note:: For NCCL-based process groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsibility to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`recv_object_list` uses ``pickle`` module implicitly, which\n        is known to be insecure. It is possible to construct malicious pickle\n        data which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`recv_object_list` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`recv` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> # Assumes backend is not NCCL\n        >>> device = torch.device(\"cpu\")\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 2.\n        >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>>     dist.send_object_list(objects, dst=1, device=device)\n        >>> else:\n        >>>     objects = [None, None, None]\n        >>>     dist.recv_object_list(objects, src=0, device=device)\n        >>> objects\n        ['foo', 12, {1: 2}]\n    ",
      "arguments": [
        "object_list",
        "src",
        "group",
        "device",
        "group_src"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Receives picklable objects in ``object_list`` synchronously.\n\n    Similar to :func:`recv`, but can receive Python objects.\n\n    Args:\n        object_list (List[Any]): List of objects to receive into.\n            Must provide a list of sizes equal to the size of the list being sent.\n        src (int, optional): Source rank from which to recv ``object_list``.\n            Source rank is based on global process group (regardless of ``group`` argument)\n            Will receive from any rank if set to None. Default is ``None``.\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        device (``torch.device``, optional): If not None, receives on this device.\n            Default is ``None``.\n        group_src (int, optional): Destination rank on ``group``.  Invalid to specify both ``src`` and ``group_src``.\n\n    Returns:\n        Sender rank. -1 if rank is not part of the group. If rank is part of the group,\n        ``object_list`` will contain the sent objects from ``src`` rank.\n\n    .. note:: For NCCL-based process groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsibility to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`recv_object_list` uses ``pickle`` module implicitly, which\n        is known to be insecure. It is possible to construct malicious pickle\n        data which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`recv_object_list` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`recv` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> # Assumes backend is not NCCL\n        >>> device = torch.device(\"cpu\")\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 2.\n        >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>>     dist.send_object_list(objects, dst=1, device=device)\n        >>> else:\n        >>>     objects = [None, None, None]\n        >>>     dist.recv_object_list(objects, src=0, device=device)\n        >>> objects\n        ['foo', 12, {1: 2}]\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.reduce",
      "signature": "torch.distributed.reduce(tensor: torch.Tensor, dst: Optional[int] = None, op=<RedOpType.SUM: 0>, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op: bool = False, group_dst: Optional[int] = None)",
      "doc": "\n    Reduces the tensor data across all machines.\n\n    Only the process with rank ``dst`` is going to receive the final result.\n\n    Args:\n        tensor (Tensor): Input and output of the collective. The function\n            operates in-place.\n        dst (int): Destination rank on global process group (regardless of ``group`` argument)\n        op (optional): One of the values from\n            ``torch.distributed.ReduceOp``\n            enum.  Specifies an operation used for element-wise reductions.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        group_dst (int): Destination rank on ``group``.  Must specify one of ``group_dst``\n            and ``dst`` but not both.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    ",
      "arguments": [
        "tensor",
        "dst",
        "op",
        "group",
        "async_op",
        "group_dst"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reduces the tensor data across all machines.\n\n    Only the process with rank ``dst`` is going to receive the final result.\n\n    Args:\n        tensor (Tensor): Input and output of the collective. The function\n            operates in-place.\n        dst (int): Destination rank on global process group (regardless of ``group`` argument)\n        op (optional): One of the values from\n            ``torch.distributed.ReduceOp``\n            enum.  Specifies an operation used for element-wise reductions.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        group_dst (int): Destination rank on ``group``.  Must specify one of ``group_dst``\n            and ``dst`` but not both.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.reduce_scatter",
      "signature": "torch.distributed.reduce_scatter(output, input_list, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
      "doc": "\n    Reduces, then scatters a list of tensors to all processes in a group.\n\n    Args:\n        output (Tensor): Output tensor.\n        input_list (list[Tensor]): List of tensors to reduce and scatter.\n        op (optional): One of the values from\n            ``torch.distributed.ReduceOp``\n            enum.  Specifies an operation used for element-wise reductions.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    ",
      "arguments": [
        "output",
        "input_list",
        "op",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reduces, then scatters a list of tensors to all processes in a group.\n\n    Args:\n        output (Tensor): Output tensor.\n        input_list (list[Tensor]): List of tensors to reduce and scatter.\n        op (optional): One of the values from\n            ``torch.distributed.ReduceOp``\n            enum.  Specifies an operation used for element-wise reductions.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.reduce_scatter_tensor",
      "signature": "torch.distributed.reduce_scatter_tensor(output, input, op=<RedOpType.SUM: 0>, group=None, async_op=False)",
      "doc": "\n    Reduces, then scatters a tensor to all ranks in a group.\n\n    Args:\n        output (Tensor): Output tensor. It should have the same size across all\n            ranks.\n        input (Tensor): Input tensor to be reduced and scattered. Its size\n            should be output tensor size times the world size. The input tensor\n            can have one of the following shapes:\n            (i) a concatenation of the output tensors along the primary\n            dimension, or\n            (ii) a stack of the output tensors along the primary dimension.\n            For definition of \"concatenation\", see ``torch.cat()``.\n            For definition of \"stack\", see ``torch.stack()``.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n        >>> # We have two ranks.\n        >>> device = torch.device(f\"cuda:{rank}\")\n        >>> tensor_out = torch.zeros(2, dtype=torch.int64, device=device)\n        >>> # Input in concatenation form\n        >>> tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)\n        >>> tensor_in\n        tensor([0, 1, 2, 3], device='cuda:0') # Rank 0\n        tensor([0, 1, 2, 3], device='cuda:1') # Rank 1\n        >>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n        >>> tensor_out\n        tensor([0, 2], device='cuda:0') # Rank 0\n        tensor([4, 6], device='cuda:1') # Rank 1\n        >>> # Input in stack form\n        >>> tensor_in = torch.reshape(tensor_in, (world_size, 2))\n        >>> tensor_in\n        tensor([[0, 1],\n                [2, 3]], device='cuda:0') # Rank 0\n        tensor([[0, 1],\n                [2, 3]], device='cuda:1') # Rank 1\n        >>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n        >>> tensor_out\n        tensor([0, 2], device='cuda:0') # Rank 0\n        tensor([4, 6], device='cuda:1') # Rank 1\n\n    ",
      "arguments": [
        "output",
        "input",
        "op",
        "group",
        "async_op"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Reduces, then scatters a tensor to all ranks in a group.\n\n    Args:\n        output (Tensor): Output tensor. It should have the same size across all\n            ranks.\n        input (Tensor): Input tensor to be reduced and scattered. Its size\n            should be output tensor size times the world size. The input tensor\n            can have one of the following shapes:\n            (i) a concatenation of the output tensors along the primary\n            dimension, or\n            (ii) a stack of the output tensors along the primary dimension.\n            For definition of \"concatenation\", see ``torch.cat()``.\n            For definition of \"stack\", see ``torch.stack()``.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op.\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group.\n\n    Examples:\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n        >>> # We have two ranks.\n        >>> device = torch.device(f\"cuda:{rank}\")\n        >>> tensor_out = torch.zeros(2, dtype=torch.int64, device=device)\n        >>> # Input in concatenation form\n        >>> tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)\n        >>> tensor_in\n        tensor([0, 1, 2, 3], device='cuda:0') # Rank 0\n        tensor([0, 1, 2, 3], device='cuda:1') # Rank 1\n        >>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n        >>> tensor_out\n        tensor([0, 2], device='cuda:0') # Rank 0\n        tensor([4, 6], device='cuda:1') # Rank 1\n        >>> # Input in stack form\n        >>> tensor_in = torch.reshape(tensor_in, (world_size, 2))\n        >>> tensor_in\n        tensor([[0, 1],\n                [2, 3]], device='cuda:0') # Rank 0\n        tensor([[0, 1],\n                [2, 3]], device='cuda:1') # Rank 1\n        >>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n        >>> tensor_out\n        tensor([0, 2], device='cuda:0') # Rank 0\n        tensor([4, 6], device='cuda:1') # Rank 1\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.register_rendezvous_handler",
      "signature": "torch.distributed.register_rendezvous_handler(scheme, handler)",
      "doc": "\n    Register a new rendezvous handler.\n\n    Before we can run collective algorithms, participating processes\n    need to find each other and exchange information to be able to\n    communicate. We call this process rendezvous.\n\n    The outcome of the rendezvous process is a triplet containing a\n    shared key/value store, the rank of the process, and the total\n    number of participating processes.\n\n    If none of the bundled rendezvous methods apply to your execution\n    environment you can opt to register your own rendezvous handler.\n    Pick a unique name and use the URL scheme to identify it when\n    calling the `rendezvous()` function.\n\n    Args:\n        scheme (str): URL scheme to identify your rendezvous handler.\n        handler (function): Handler that is invoked when the\n            `rendezvous()` function is called with a URL that uses\n            the corresponding scheme. It must be a generator function\n            that yields the triplet.\n    ",
      "arguments": [
        "scheme",
        "handler"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Register a new rendezvous handler.\n\n    Before we can run collective algorithms, participating processes\n    need to find each other and exchange information to be able to\n    communicate. We call this process rendezvous.\n\n    The outcome of the rendezvous process is a triplet containing a\n    shared key/value store, the rank of the process, and the total\n    number of participating processes.\n\n    If none of the bundled rendezvous methods apply to your execution\n    environment you can opt to register your own rendezvous handler.\n    Pick a unique name and use the URL scheme to identify it when\n    calling the `rendezvous()` function.\n\n    Args:\n        scheme (str): URL scheme to identify your rendezvous handler.\n        handler (function): Handler that is invoked when the\n            `rendezvous()` function is called with a URL that uses\n            the corresponding scheme. It must be a generator function\n            that yields the triplet.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.rendezvous",
      "signature": "torch.distributed.rendezvous(url: str, rank: int = -1, world_size: int = -1, **kwargs)",
      "doc": "",
      "arguments": [
        "url",
        "rank",
        "world_size",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.scatter",
      "signature": "torch.distributed.scatter(tensor: torch.Tensor, scatter_list: Optional[list[torch.Tensor]] = None, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, async_op: bool = False, group_src: Optional[int] = None)",
      "doc": "\n    Scatters a list of tensors to all processes in a group.\n\n    Each process will receive exactly one tensor and store its data in the\n    ``tensor`` argument.\n\n    Complex tensors are supported.\n\n    Args:\n        tensor (Tensor): Output tensor.\n        scatter_list (list[Tensor]): List of tensors to scatter (default is\n            None, must be specified on the source rank)\n        src (int): Source rank on global process group (regardless of ``group`` argument).\n            (If both ``src`` and ``group_src`` are None, default is global rank 0)\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        group_src (int, optional): Source rank on ``group``.  Invalid to specify both ``src`` and ``group_src``\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    .. note:: Note that all Tensors in scatter_list must have the same size.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> tensor_size = 2\n        >>> device = torch.device(f'cuda:{rank}')\n        >>> output_tensor = torch.zeros(tensor_size, device=device)\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 2.\n        >>>     # Only tensors, all of which must be the same size.\n        >>>     t_ones = torch.ones(tensor_size, device=device)\n        >>>     t_fives = torch.ones(tensor_size, device=device) * 5\n        >>>     scatter_list = [t_ones, t_fives]\n        >>> else:\n        >>>     scatter_list = None\n        >>> dist.scatter(output_tensor, scatter_list, src=0)\n        >>> # Rank i gets scatter_list[i].\n        >>> output_tensor\n        tensor([1., 1.], device='cuda:0') # Rank 0\n        tensor([5., 5.], device='cuda:1') # Rank 1\n\n    ",
      "arguments": [
        "tensor",
        "scatter_list",
        "src",
        "group",
        "async_op",
        "group_src"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Scatters a list of tensors to all processes in a group.\n\n    Each process will receive exactly one tensor and store its data in the\n    ``tensor`` argument.\n\n    Complex tensors are supported.\n\n    Args:\n        tensor (Tensor): Output tensor.\n        scatter_list (list[Tensor]): List of tensors to scatter (default is\n            None, must be specified on the source rank)\n        src (int): Source rank on global process group (regardless of ``group`` argument).\n            (If both ``src`` and ``group_src`` are None, default is global rank 0)\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        async_op (bool, optional): Whether this op should be an async op\n        group_src (int, optional): Source rank on ``group``.  Invalid to specify both ``src`` and ``group_src``\n\n    Returns:\n        Async work handle, if async_op is set to True.\n        None, if not async_op or if not part of the group\n\n    .. note:: Note that all Tensors in scatter_list must have the same size.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> tensor_size = 2\n        >>> device = torch.device(f'cuda:{rank}')\n        >>> output_tensor = torch.zeros(tensor_size, device=device)\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 2.\n        >>>     # Only tensors, all of which must be the same size.\n        >>>     t_ones = torch.ones(tensor_size, device=device)\n        >>>     t_fives = torch.ones(tensor_size, device=device) * 5\n        >>>     scatter_list = [t_ones, t_fives]\n        >>> else:\n        >>>     scatter_list = None\n        >>> dist.scatter(output_tensor, scatter_list, src=0)\n        >>> # Rank i gets scatter_list[i].\n        >>> output_tensor\n        tensor([1., 1.], device='cuda:0') # Rank 0\n        tensor([5., 5.], device='cuda:1') # Rank 1\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.scatter_object_list",
      "signature": "torch.distributed.scatter_object_list(scatter_object_output_list: list[typing.Any], scatter_object_input_list: Optional[list[Any]] = None, src: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, group_src: Optional[int] = None)",
      "doc": "\n    Scatters picklable objects in ``scatter_object_input_list`` to the whole group.\n\n    Similar to :func:`scatter`, but Python objects can be passed in. On\n    each rank, the scattered object will be stored as the first element of\n    ``scatter_object_output_list``. Note that all objects in\n    ``scatter_object_input_list`` must be picklable in order to be scattered.\n\n    Args:\n        scatter_object_output_list (List[Any]): Non-empty list whose first\n            element will store the object scattered to this rank.\n        scatter_object_input_list (List[Any], optional): List of input objects to scatter.\n            Each object must be picklable. Only objects on the ``src`` rank will\n            be scattered, and the argument can be ``None`` for non-src ranks.\n        src (int): Source rank from which to scatter ``scatter_object_input_list``.\n            Source rank is based on global process group (regardless of ``group`` argument).\n            (If both ``src`` and ``group_src`` are None, default is global rank 0)\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        group_src (int, optional): Source rank on ``group``.  Invalid to specify both ``src`` and ``group_src``\n\n    Returns:\n        ``None``. If rank is part of the group, ``scatter_object_output_list``\n        will have its first element set to the scattered object for this rank.\n\n    .. note:: Note that this API differs slightly from the scatter collective\n        since it does not provide an ``async_op`` handle and thus will be a\n        blocking call.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`scatter_object_list` uses ``pickle`` module implicitly, which\n        is known to be insecure. It is possible to construct malicious pickle\n        data which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`scatter_object_list` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`scatter` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 3.\n        >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>> else:\n        >>>     # Can be any list on non-src ranks, elements are not used.\n        >>>     objects = [None, None, None]\n        >>> output_list = [None]\n        >>> dist.scatter_object_list(output_list, objects, src=0)\n        >>> # Rank i gets objects[i]. For example, on rank 2:\n        >>> output_list\n        [{1: 2}]\n    ",
      "arguments": [
        "scatter_object_output_list",
        "scatter_object_input_list",
        "src",
        "group",
        "group_src"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Scatters picklable objects in ``scatter_object_input_list`` to the whole group.\n\n    Similar to :func:`scatter`, but Python objects can be passed in. On\n    each rank, the scattered object will be stored as the first element of\n    ``scatter_object_output_list``. Note that all objects in\n    ``scatter_object_input_list`` must be picklable in order to be scattered.\n\n    Args:\n        scatter_object_output_list (List[Any]): Non-empty list whose first\n            element will store the object scattered to this rank.\n        scatter_object_input_list (List[Any], optional): List of input objects to scatter.\n            Each object must be picklable. Only objects on the ``src`` rank will\n            be scattered, and the argument can be ``None`` for non-src ranks.\n        src (int): Source rank from which to scatter ``scatter_object_input_list``.\n            Source rank is based on global process group (regardless of ``group`` argument).\n            (If both ``src`` and ``group_src`` are None, default is global rank 0)\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        group_src (int, optional): Source rank on ``group``.  Invalid to specify both ``src`` and ``group_src``\n\n    Returns:\n        ``None``. If rank is part of the group, ``scatter_object_output_list``\n        will have its first element set to the scattered object for this rank.\n\n    .. note:: Note that this API differs slightly from the scatter collective\n        since it does not provide an ``async_op`` handle and thus will be a\n        blocking call.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`scatter_object_list` uses ``pickle`` module implicitly, which\n        is known to be insecure. It is possible to construct malicious pickle\n        data which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`scatter_object_list` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`scatter` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 3.\n        >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>> else:\n        >>>     # Can be any list on non-src ranks, elements are not used.\n        >>>     objects = [None, None, None]\n        >>> output_list = [None]\n        >>> dist.scatter_object_list(output_list, objects, src=0)\n        >>> # Rank i gets objects[i]. For example, on rank 2:\n        >>> output_list\n        [{1: 2}]\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.send",
      "signature": "torch.distributed.send(tensor: torch.Tensor, dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, tag: int = 0, group_dst: Optional[int] = None) -> None",
      "doc": "\n    Send a tensor synchronously.\n\n    .. warning::\n        ``tag`` is not supported with the NCCL backend.\n\n    Args:\n        tensor (Tensor): Tensor to send.\n        dst (int): Destination rank on global process group (regardless of ``group`` argument).\n            Destination rank should not be the same as the rank of the current process.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        tag (int, optional): Tag to match send with remote recv\n        group_dst (int, optional): Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``.\n\n    ",
      "arguments": [
        "tensor",
        "dst",
        "group",
        "tag",
        "group_dst"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Send a tensor synchronously.\n\n    .. warning::\n        ``tag`` is not supported with the NCCL backend.\n\n    Args:\n        tensor (Tensor): Tensor to send.\n        dst (int): Destination rank on global process group (regardless of ``group`` argument).\n            Destination rank should not be the same as the rank of the current process.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n        tag (int, optional): Tag to match send with remote recv\n        group_dst (int, optional): Destination rank on ``group``.  Invalid to specify both ``dst`` and ``group_dst``.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.send_object_list",
      "signature": "torch.distributed.send_object_list(object_list: list[typing.Any], dst: Optional[int] = None, group: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, device: Optional[torch.device] = None, group_dst: Optional[int] = None)",
      "doc": "\n    Sends picklable objects in ``object_list`` synchronously.\n\n    Similar to :func:`send`, but Python objects can be passed in.\n    Note that all objects in ``object_list`` must be picklable in order to be\n    sent.\n\n    Args:\n        object_list (List[Any]): List of input objects to sent.\n            Each object must be picklable. Receiver must provide lists of equal sizes.\n        dst (int): Destination rank to send ``object_list`` to.\n            Destination rank is based on global process group (regardless of ``group`` argument)\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        device (``torch.device``, optional): If not None, the objects are\n            serialized and converted to tensors which are moved to the\n            ``device`` before sending. Default is ``None``.\n        group_dst (int, optional): Destination rank on ``group``.\n            Must specify one of ``dst`` and ``group_dst`` but not both\n    Returns:\n        ``None``.\n\n    .. note:: For NCCL-based process groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsibility to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`send_object_list` uses ``pickle`` module implicitly, which\n        is known to be insecure. It is possible to construct malicious pickle\n        data which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`send_object_list` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`send` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> # Assumes backend is not NCCL\n        >>> device = torch.device(\"cpu\")\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 2.\n        >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>>     dist.send_object_list(objects, dst=1, device=device)\n        >>> else:\n        >>>     objects = [None, None, None]\n        >>>     dist.recv_object_list(objects, src=0, device=device)\n        >>> objects\n        ['foo', 12, {1: 2}]\n    ",
      "arguments": [
        "object_list",
        "dst",
        "group",
        "device",
        "group_dst"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Sends picklable objects in ``object_list`` synchronously.\n\n    Similar to :func:`send`, but Python objects can be passed in.\n    Note that all objects in ``object_list`` must be picklable in order to be\n    sent.\n\n    Args:\n        object_list (List[Any]): List of input objects to sent.\n            Each object must be picklable. Receiver must provide lists of equal sizes.\n        dst (int): Destination rank to send ``object_list`` to.\n            Destination rank is based on global process group (regardless of ``group`` argument)\n        group: (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n        device (``torch.device``, optional): If not None, the objects are\n            serialized and converted to tensors which are moved to the\n            ``device`` before sending. Default is ``None``.\n        group_dst (int, optional): Destination rank on ``group``.\n            Must specify one of ``dst`` and ``group_dst`` but not both\n    Returns:\n        ``None``.\n\n    .. note:: For NCCL-based process groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsibility to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n\n    .. warning::\n        Object collectives have a number of serious performance and scalability\n        limitations.  See :ref:`object_collectives` for details.\n\n    .. warning::\n        :func:`send_object_list` uses ``pickle`` module implicitly, which\n        is known to be insecure. It is possible to construct malicious pickle\n        data which will execute arbitrary code during unpickling. Only call this\n        function with data you trust.\n\n    .. warning::\n        Calling :func:`send_object_list` with GPU tensors is not well supported\n        and inefficient as it incurs GPU -> CPU transfer since tensors would be\n        pickled. Please consider using :func:`send` instead.\n\n    Example::\n        >>> # xdoctest: +SKIP(\"need process group init\")\n        >>> # Note: Process group initialization omitted on each rank.\n        >>> import torch.distributed as dist\n        >>> # Assumes backend is not NCCL\n        >>> device = torch.device(\"cpu\")\n        >>> if dist.get_rank() == 0:\n        >>>     # Assumes world_size of 2.\n        >>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n        >>>     dist.send_object_list(objects, dst=1, device=device)\n        >>> else:\n        >>>     objects = [None, None, None]\n        >>>     dist.recv_object_list(objects, src=0, device=device)\n        >>> objects\n        ['foo', 12, {1: 2}]\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.split_group",
      "signature": "torch.distributed.split_group(parent_pg: Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, split_ranks: Optional[list] = None, timeout: Optional[datetime.timedelta] = None, pg_options: Optional[Any] = None, group_desc: Optional[str] = None) -> Optional[torch.distributed.distributed_c10d.ProcessGroup]",
      "doc": "\n    Create a new process group splitted from the given parent process group.\n\n    warning:: This is an experimental API and only the ``NCCL`` backend supports this API.\n    Other backends will raise an error.\n    Users of this API must gurantee that all ranks in the parent group enter this API call,\n    and the split of the sub groups is the same across all ranks in the parent group.\n\n    Args:\n        parent_pg (ProcessGroup, optional): The parent process group. If None,\n            the default process group will be used. Users need to gurantee that\n            the parent group is fully initialized (e.g, communicators are initialized)\n        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n            Users need to make sure the validity of the split ranks such that one\n            split (represented by one inner list of ints) does not overlap with any other split.\n            Note that the ranks in each split is the group rank (instead of global rank)\n            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n            return a non-group member.\n        timeout (timedelta, optional): see `init_process_group` for details and default value.\n        pg_options (ProcessGroupOptions, optional): only ProcessGroupNCCLOptions is supported now.\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. i.e.``is_high_priority_stream``\n            can be specified so that process group can pick up high priority cuda streams.\n            For other availble options to config nccl,\n            See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t\n        group_desc (str, optional): a string to describe the process group.\n\n    Returns:\n        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n        or None if the current rank is not part of any split_ranks`.\n\n    ",
      "arguments": [
        "parent_pg",
        "split_ranks",
        "timeout",
        "pg_options",
        "group_desc"
      ],
      "return_type": "typing.Optional[torch.distributed.distributed_c10d.ProcessGroup]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Create a new process group splitted from the given parent process group.\n\n    warning:: This is an experimental API and only the ``NCCL`` backend supports this API.\n    Other backends will raise an error.\n    Users of this API must gurantee that all ranks in the parent group enter this API call,\n    and the split of the sub groups is the same across all ranks in the parent group.\n\n    Args:\n        parent_pg (ProcessGroup, optional): The parent process group. If None,\n            the default process group will be used. Users need to gurantee that\n            the parent group is fully initialized (e.g, communicators are initialized)\n        split_ranks (list[list[int]]): the split ranks, which is a list of list of ranks.\n            Users need to make sure the validity of the split ranks such that one\n            split (represented by one inner list of ints) does not overlap with any other split.\n            Note that the ranks in each split is the group rank (instead of global rank)\n            in the parent pg. For example, if the parent group has 4 ranks, and split_ranks can be\n            [[0, 1], [2, 3]]. Note [[0,1]] is also a valid split, in which case ranks 2, 3 would\n            return a non-group member.\n        timeout (timedelta, optional): see `init_process_group` for details and default value.\n        pg_options (ProcessGroupOptions, optional): only ProcessGroupNCCLOptions is supported now.\n            specifying what additional options need to be passed in during\n            the construction of specific process groups. i.e.``is_high_priority_stream``\n            can be specified so that process group can pick up high priority cuda streams.\n            For other availble options to config nccl,\n            See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t\n        group_desc (str, optional): a string to describe the process group.\n\n    Returns:\n        ProcessGroup if the current rank is within one split/subgroup given by split_ranks,\n        or None if the current rank is not part of any split_ranks`.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributed.supports_complex",
      "signature": "torch.distributed.supports_complex(reduceOp: torch.distributed.distributed_c10d.ReduceOp) -> bool",
      "doc": "Return true if reduce ops is supported. False otherwise.",
      "arguments": [
        "reduceOp"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return true if reduce ops is supported. False otherwise.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "DISTRIBUTIONS": [
    {
      "function": "torch.distributions.kl_divergence",
      "signature": "torch.distributions.kl_divergence(p: torch.distributions.distribution.Distribution, q: torch.distributions.distribution.Distribution) -> torch.Tensor",
      "doc": "\n    Compute Kullback-Leibler divergence :math:`KL(p \\| q)` between two distributions.\n\n    .. math::\n\n        KL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx\n\n    Args:\n        p (Distribution): A :class:`~torch.distributions.Distribution` object.\n        q (Distribution): A :class:`~torch.distributions.Distribution` object.\n\n    Returns:\n        Tensor: A batch of KL divergences of shape `batch_shape`.\n\n    Raises:\n        NotImplementedError: If the distribution types have not been registered via\n            :meth:`register_kl`.\n    KL divergence is currently implemented for the following distribution pairs:\n\t* :class:`~torch.distributions.Bernoulli` and :class:`~torch.distributions.Bernoulli`\n\t* :class:`~torch.distributions.Bernoulli` and :class:`~torch.distributions.Poisson`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.Binomial` and :class:`~torch.distributions.Binomial`\n\t* :class:`~torch.distributions.Categorical` and :class:`~torch.distributions.Categorical`\n\t* :class:`~torch.distributions.Cauchy` and :class:`~torch.distributions.Cauchy`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.Dirichlet` and :class:`~torch.distributions.Dirichlet`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.ExponentialFamily` and :class:`~torch.distributions.ExponentialFamily`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.Geometric` and :class:`~torch.distributions.Geometric`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.HalfNormal` and :class:`~torch.distributions.HalfNormal`\n\t* :class:`~torch.distributions.Independent` and :class:`~torch.distributions.Independent`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Laplace`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.LowRankMultivariateNormal` and :class:`~torch.distributions.LowRankMultivariateNormal`\n\t* :class:`~torch.distributions.LowRankMultivariateNormal` and :class:`~torch.distributions.MultivariateNormal`\n\t* :class:`~torch.distributions.MultivariateNormal` and :class:`~torch.distributions.LowRankMultivariateNormal`\n\t* :class:`~torch.distributions.MultivariateNormal` and :class:`~torch.distributions.MultivariateNormal`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Laplace`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.OneHotCategorical` and :class:`~torch.distributions.OneHotCategorical`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Bernoulli`\n\t* :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Binomial`\n\t* :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Poisson`\n\t* :class:`~torch.distributions.TransformedDistribution` and :class:`~torch.distributions.TransformedDistribution`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Uniform`",
      "arguments": [
        "p",
        "q"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Compute Kullback-Leibler divergence :math:`KL(p \\| q)` between two distributions.\n\n    .. math::\n\n        KL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx\n\n    Args:\n        p (Distribution): A :class:`~torch.distributions.Distribution` object.\n        q (Distribution): A :class:`~torch.distributions.Distribution` object.\n\n    Returns:\n        Tensor: A batch of KL divergences of shape `batch_shape`.\n\n    Raises:\n        NotImplementedError: If the distribution types have not been registered via\n            :meth:`register_kl`.\n    KL divergence is currently implemented for the following distribution pairs:\n\t* :class:`~torch.distributions.Bernoulli` and :class:`~torch.distributions.Bernoulli`\n\t* :class:`~torch.distributions.Bernoulli` and :class:`~torch.distributions.Poisson`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.Binomial` and :class:`~torch.distributions.Binomial`\n\t* :class:`~torch.distributions.Categorical` and :class:`~torch.distributions.Categorical`\n\t* :class:`~torch.distributions.Cauchy` and :class:`~torch.distributions.Cauchy`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.Dirichlet` and :class:`~torch.distributions.Dirichlet`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.ExponentialFamily` and :class:`~torch.distributions.ExponentialFamily`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.Geometric` and :class:`~torch.distributions.Geometric`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.HalfNormal` and :class:`~torch.distributions.HalfNormal`\n\t* :class:`~torch.distributions.Independent` and :class:`~torch.distributions.Independent`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Laplace`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.LowRankMultivariateNormal` and :class:`~torch.distributions.LowRankMultivariateNormal`\n\t* :class:`~torch.distributions.LowRankMultivariateNormal` and :class:`~torch.distributions.MultivariateNormal`\n\t* :class:`~torch.distributions.MultivariateNormal` and :class:`~torch.distributions.LowRankMultivariateNormal`\n\t* :class:`~torch.distributions.MultivariateNormal` and :class:`~torch.distributions.MultivariateNormal`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Laplace`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.OneHotCategorical` and :class:`~torch.distributions.OneHotCategorical`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Uniform`\n\t* :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Bernoulli`\n\t* :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Binomial`\n\t* :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Poisson`\n\t* :class:`~torch.distributions.TransformedDistribution` and :class:`~torch.distributions.TransformedDistribution`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Beta`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.ContinuousBernoulli`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Exponential`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Gamma`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Gumbel`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Normal`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Pareto`\n\t* :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Uniform`",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.distributions.register_kl",
      "signature": "torch.distributions.register_kl(type_p, type_q)",
      "doc": "\n    Decorator to register a pairwise function with :meth:`kl_divergence`.\n    Usage::\n\n        @register_kl(Normal, Normal)\n        def kl_normal_normal(p, q):\n            # insert implementation here\n\n    Lookup returns the most specific (type,type) match ordered by subclass. If\n    the match is ambiguous, a `RuntimeWarning` is raised. For example to\n    resolve the ambiguous situation::\n\n        @register_kl(BaseP, DerivedQ)\n        def kl_version1(p, q): ...\n        @register_kl(DerivedP, BaseQ)\n        def kl_version2(p, q): ...\n\n    you should register a third most-specific implementation, e.g.::\n\n        register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n\n    Args:\n        type_p (type): A subclass of :class:`~torch.distributions.Distribution`.\n        type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n    ",
      "arguments": [
        "type_p",
        "type_q"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Decorator to register a pairwise function with :meth:`kl_divergence`.\n    Usage::\n\n        @register_kl(Normal, Normal)\n        def kl_normal_normal(p, q):\n            # insert implementation here\n\n    Lookup returns the most specific (type,type) match ordered by subclass. If\n    the match is ambiguous, a `RuntimeWarning` is raised. For example to\n    resolve the ambiguous situation::\n\n        @register_kl(BaseP, DerivedQ)\n        def kl_version1(p, q): ...\n        @register_kl(DerivedP, BaseQ)\n        def kl_version2(p, q): ...\n\n    you should register a third most-specific implementation, e.g.::\n\n        register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n\n    Args:\n        type_p (type): A subclass of :class:`~torch.distributions.Distribution`.\n        type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "MODEL_EXPORT": [
    {
      "function": "torch.export.compatibility",
      "signature": "torch.export.compatibility(is_backward_compatible: bool) -> Callable[[~_T], ~_T]",
      "doc": "",
      "arguments": [
        "is_backward_compatible"
      ],
      "return_type": "typing.Callable[[~_T], ~_T]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.export.default_decompositions",
      "signature": "torch.export.default_decompositions() -> 'CustomDecompTable'",
      "doc": "\n    This is the default decomposition table which contains decomposition of\n    all ATEN operators to core aten opset. Use this API together with\n    :func:`run_decompositions()`\n    ",
      "arguments": [],
      "return_type": "CustomDecompTable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This is the default decomposition table which contains decomposition of\n    all ATEN operators to core aten opset. Use this API together with\n    :func:`run_decompositions()`\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.export.dims",
      "signature": "torch.export.dims(*names: str, min: Optional[int] = None, max: Optional[int] = None) -> tuple[torch.export.dynamic_shapes.Dim, ...]",
      "doc": "\n    Util to create multiple :func:`Dim` types.\n\n    Returns:\n        A tuple of :func:`Dim` types.\n    ",
      "arguments": [
        "names",
        "min",
        "max"
      ],
      "return_type": "tuple[torch.export.dynamic_shapes.Dim, ...]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Util to create multiple :func:`Dim` types.\n\n    Returns:\n        A tuple of :func:`Dim` types.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.export.export",
      "signature": "torch.export.export(mod: torch.nn.modules.module.Module, args: tuple[typing.Any, ...], kwargs: Optional[dict[str, Any]] = None, *, dynamic_shapes: Union[dict[str, Any], tuple[Any], list[Any], NoneType] = None, strict: bool = False, preserve_module_call_signature: tuple[str, ...] = ()) -> torch.export.exported_program.ExportedProgram",
      "doc": "\n    :func:`export` takes any nn.Module along with example inputs, and produces a traced graph representing\n    only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,\n    which can subsequently be executed with different inputs or serialized.  The\n    traced graph (1) produces normalized operators in the functional ATen operator set\n    (as well as any user-specified custom operators), (2) has eliminated all Python control\n    flow and data structures (with certain exceptions), and (3) records the set of\n    shape constraints needed to show that this normalization and control-flow elimination\n    is sound for future inputs.\n\n    **Soundness Guarantee**\n\n    While tracing, :func:`export()` takes note of shape-related assumptions\n    made by the user program and the underlying PyTorch operator kernels.\n    The output :class:`ExportedProgram` is considered valid only when these\n    assumptions hold true.\n\n    Tracing makes assumptions on the shapes (not values) of input tensors.\n    Such assumptions must be validated at graph capture time for :func:`export`\n    to succeed. Specifically:\n\n    - Assumptions on static shapes of input tensors are automatically validated without additional effort.\n    - Assumptions on dynamic shape of input tensors require explicit specification\n      by using the :func:`Dim` API to construct dynamic dimensions and by associating\n      them with example inputs through the ``dynamic_shapes`` argument.\n\n    If any assumption can not be validated, a fatal error will be raised. When that happens,\n    the error message will include suggested fixes to the specification that are needed\n    to validate the assumptions. For example :func:`export` might suggest the\n    following fix to the definition of a dynamic dimension ``dim0_x``, say appearing in the\n    shape associated with input ``x``, that was previously defined as ``Dim(\"dim0_x\")``::\n\n        dim = Dim(\"dim0_x\", max=5)\n\n    This example means the generated code requires dimension 0 of input ``x`` to be less\n    than or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension\n    definitions and then copy them verbatim into your code without needing to change the\n    ``dynamic_shapes`` argument to your :func:`export` call.\n\n    Args:\n        mod: We will trace the forward method of this module.\n\n        args: Example positional inputs.\n\n        kwargs: Optional example keyword inputs.\n\n        dynamic_shapes:\n         An optional argument where the type should either be:\n         1) a dict from argument names of ``f`` to their dynamic shape specifications,\n         2) a tuple that specifies dynamic shape specifications for each input in original order.\n         If you are specifying dynamism on keyword args, you will need to pass them in the order that\n         is defined in the original function signature.\n\n         The dynamic shape of a tensor argument can be specified as either\n         (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n         not required to include static dimension indices in this dict, but when they are,\n         they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n         where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n         are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n         recursively specified by using mappings or sequences of contained specifications.\n\n        strict: When disabled (default), the export function will trace the program through\n         Python runtime, which by itself will not validate some of the implicit assumptions\n         baked into the graph. It will still validate most critical assumptions like shape\n         safety. When enabled (by setting ``strict=True``), the export function will trace\n         the program through TorchDynamo which will ensure the soundness of the resulting\n         graph. TorchDynamo has limited Python feature coverage, thus you may experience more\n         errors. Note that toggling this argument does not affect the resulting IR spec to be\n         different and the model will be serialized in the same way regardless of what value\n         is passed here.\n\n        preserve_module_call_signature: A list of submodule paths for which the original\n         calling conventions are preserved as metadata. The metadata will be used when calling\n         torch.export.unflatten to preserve the original calling conventions of modules.\n\n    Returns:\n        An :class:`ExportedProgram` containing the traced callable.\n\n    **Acceptable input/output types**\n\n    Acceptable types of inputs (for ``args`` and ``kwargs``) and outputs include:\n\n    - Primitive types, i.e. ``torch.Tensor``, ``int``, ``float``, ``bool`` and ``str``.\n    - Dataclasses, but they must be registered by calling :func:`register_dataclass` first.\n    - (Nested) Data structures comprising of ``dict``, ``list``, ``tuple``, ``namedtuple`` and\n      ``OrderedDict`` containing all above types.\n\n    ",
      "arguments": [
        "mod",
        "args",
        "kwargs",
        "dynamic_shapes",
        "strict",
        "preserve_module_call_signature"
      ],
      "return_type": "<class 'torch.export.exported_program.ExportedProgram'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    :func:`export` takes any nn.Module along with example inputs, and produces a traced graph representing\n    only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,\n    which can subsequently be executed with different inputs or serialized.  The\n    traced graph (1) produces normalized operators in the functional ATen operator set\n    (as well as any user-specified custom operators), (2) has eliminated all Python control\n    flow and data structures (with certain exceptions), and (3) records the set of\n    shape constraints needed to show that this normalization and control-flow elimination\n    is sound for future inputs.\n\n    **Soundness Guarantee**\n\n    While tracing, :func:`export()` takes note of shape-related assumptions\n    made by the user program and the underlying PyTorch operator kernels.\n    The output :class:`ExportedProgram` is considered valid only when these\n    assumptions hold true.\n\n    Tracing makes assumptions on the shapes (not values) of input tensors.\n    Such assumptions must be validated at graph capture time for :func:`export`\n    to succeed. Specifically:\n\n    - Assumptions on static shapes of input tensors are automatically validated without additional effort.\n    - Assumptions on dynamic shape of input tensors require explicit specification\n      by using the :func:`Dim` API to construct dynamic dimensions and by associating\n      them with example inputs through the ``dynamic_shapes`` argument.\n\n    If any assumption can not be validated, a fatal error will be raised. When that happens,\n    the error message will include suggested fixes to the specification that are needed\n    to validate the assumptions. For example :func:`export` might suggest the\n    following fix to the definition of a dynamic dimension ``dim0_x``, say appearing in the\n    shape associated with input ``x``, that was previously defined as ``Dim(\"dim0_x\")``::\n\n        dim = Dim(\"dim0_x\", max=5)\n\n    This example means the generated code requires dimension 0 of input ``x`` to be less\n    than or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension\n    definitions and then copy them verbatim into your code without needing to change the\n    ``dynamic_shapes`` argument to your :func:`export` call.\n\n    Args:\n        mod: We will trace the forward method of this module.\n\n        args: Example positional inputs.\n\n        kwargs: Optional example keyword inputs.\n\n        dynamic_shapes:\n         An optional argument where the type should either be:\n         1) a dict from argument names of ``f`` to their dynamic shape specifications,\n         2) a tuple that specifies dynamic shape specifications for each input in original order.\n         If you are specifying dynamism on keyword args, you will need to pass them in the order that\n         is defined in the original function signature.\n\n         The dynamic shape of a tensor argument can be specified as either\n         (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n         not required to include static dimension indices in this dict, but when they are,\n         they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n         where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n         are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n         recursively specified by using mappings or sequences of contained specifications.\n\n        strict: When disabled (default), the export function will trace the program through\n         Python runtime, which by itself will not validate some of the implicit assumptions\n         baked into the graph. It will still validate most critical assumptions like shape\n         safety. When enabled (by setting ``strict=True``), the export function will trace\n         the program through TorchDynamo which will ensure the soundness of the resulting\n         graph. TorchDynamo has limited Python feature coverage, thus you may experience more\n         errors. Note that toggling this argument does not affect the resulting IR spec to be\n         different and the model will be serialized in the same way regardless of what value\n         is passed here.\n\n        preserve_module_call_signature: A list of submodule paths for which the original\n         calling conventions are preserved as metadata. The metadata will be used when calling\n         torch.export.unflatten to preserve the original calling conventions of modules.\n\n    Returns:\n        An :class:`ExportedProgram` containing the traced callable.\n\n    **Acceptable input/output types**\n\n    Acceptable types of inputs (for ``args`` and ``kwargs``) and outputs include:\n\n    - Primitive types, i.e. ``torch.Tensor``, ``int``, ``float``, ``bool`` and ``str``.\n    - Dataclasses, but they must be registered by calling :func:`register_dataclass` first.\n    - (Nested) Data structures comprising of ``dict``, ``list``, ``tuple``, ``namedtuple`` and\n      ``OrderedDict`` containing all above types.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.export.export_for_training",
      "signature": "torch.export.export_for_training(mod: torch.nn.modules.module.Module, args: tuple[typing.Any, ...], kwargs: Optional[dict[str, Any]] = None, *, dynamic_shapes: Union[dict[str, Any], tuple[Any], list[Any], NoneType] = None, strict: bool = False, preserve_module_call_signature: tuple[str, ...] = ()) -> torch.export.exported_program.ExportedProgram",
      "doc": "\n    :func:`export_for_training` takes any nn.Module along with example inputs, and produces a traced graph representing\n    only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,\n    which can subsequently be executed with different inputs or serialized. The\n    traced graph (1) produces normalized operators in the all ATen operator set\n    (as well as any user-specified custom operators), (2) has eliminated all Python control\n    flow and data structures (with certain exceptions), and (3) records the set of\n    shape constraints needed to show that this normalization and control-flow elimination\n    is sound for future inputs. This API is intended for PT2 quantization training use cases\n    and will soon be the default IR of torch.export.export in the near future. To read further about\n    the motivation behind this change, please refer to\n    https://dev-discuss.pytorch.org/t/why-pytorch-does-not-need-a-new-standardized-operator-set/2206\n    With this API, and :func:`run_decompositions()`, you should be able to get inference IR with\n    your custom decomposition behaviour.\n\n    **Soundness Guarantee**\n\n    See :func:`export()` docstring for more details.\n\n    Args:\n        mod: We will trace the forward method of this module.\n\n        args: Example positional inputs.\n\n        kwargs: Optional example keyword inputs.\n\n        dynamic_shapes:\n         An optional argument where the type should either be:\n         1) a dict from argument names of ``f`` to their dynamic shape specifications,\n         2) a tuple that specifies dynamic shape specifications for each input in original order.\n         If you are specifying dynamism on keyword args, you will need to pass them in the order that\n         is defined in the original function signature.\n\n         The dynamic shape of a tensor argument can be specified as either\n         (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n         not required to include static dimension indices in this dict, but when they are,\n         they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n         where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n         are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n         recursively specified by using mappings or sequences of contained specifications.\n\n        strict: When enabled (default), the export function will trace the program through\n         TorchDynamo which will ensure the soundness of the resulting graph. Otherwise, the\n         exported program will not validate the implicit assumptions baked into the graph and\n         may cause behavior divergence between the original model and the exported one. This is\n         useful when users need to workaround bugs in the tracer, or simply want incrementally\n         enable safety in their models. Note that this does not affect the resulting IR spec\n         to be different and the model will be serialized in the same way regardless of what value\n         is passed here.\n         WARNING: This option is experimental and use this at your own risk.\n\n        preserve_module_call_signature: A list of submodule paths for which the original\n         calling conventions are preserved as metadata. The metadata will be used when calling\n         torch.export.unflatten to preserve the original calling conventions of modules.\n\n    Returns:\n        An :class:`ExportedProgram` containing the traced callable.\n\n    **Acceptable input/output types**\n\n    Acceptable types of inputs (for ``args`` and ``kwargs``) and outputs include:\n\n    - Primitive types, i.e. ``torch.Tensor``, ``int``, ``float``, ``bool`` and ``str``.\n    - Dataclasses, but they must be registered by calling :func:`register_dataclass` first.\n    - (Nested) Data structures comprising of ``dict``, ``list``, ``tuple``, ``namedtuple`` and\n      ``OrderedDict`` containing all above types.\n\n    ",
      "arguments": [
        "mod",
        "args",
        "kwargs",
        "dynamic_shapes",
        "strict",
        "preserve_module_call_signature"
      ],
      "return_type": "<class 'torch.export.exported_program.ExportedProgram'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    :func:`export_for_training` takes any nn.Module along with example inputs, and produces a traced graph representing\n    only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,\n    which can subsequently be executed with different inputs or serialized. The\n    traced graph (1) produces normalized operators in the all ATen operator set\n    (as well as any user-specified custom operators), (2) has eliminated all Python control\n    flow and data structures (with certain exceptions), and (3) records the set of\n    shape constraints needed to show that this normalization and control-flow elimination\n    is sound for future inputs. This API is intended for PT2 quantization training use cases\n    and will soon be the default IR of torch.export.export in the near future. To read further about\n    the motivation behind this change, please refer to\n    https://dev-discuss.pytorch.org/t/why-pytorch-does-not-need-a-new-standardized-operator-set/2206\n    With this API, and :func:`run_decompositions()`, you should be able to get inference IR with\n    your custom decomposition behaviour.\n\n    **Soundness Guarantee**\n\n    See :func:`export()` docstring for more details.\n\n    Args:\n        mod: We will trace the forward method of this module.\n\n        args: Example positional inputs.\n\n        kwargs: Optional example keyword inputs.\n\n        dynamic_shapes:\n         An optional argument where the type should either be:\n         1) a dict from argument names of ``f`` to their dynamic shape specifications,\n         2) a tuple that specifies dynamic shape specifications for each input in original order.\n         If you are specifying dynamism on keyword args, you will need to pass them in the order that\n         is defined in the original function signature.\n\n         The dynamic shape of a tensor argument can be specified as either\n         (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n         not required to include static dimension indices in this dict, but when they are,\n         they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n         where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n         are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n         recursively specified by using mappings or sequences of contained specifications.\n\n        strict: When enabled (default), the export function will trace the program through\n         TorchDynamo which will ensure the soundness of the resulting graph. Otherwise, the\n         exported program will not validate the implicit assumptions baked into the graph and\n         may cause behavior divergence between the original model and the exported one. This is\n         useful when users need to workaround bugs in the tracer, or simply want incrementally\n         enable safety in their models. Note that this does not affect the resulting IR spec\n         to be different and the model will be serialized in the same way regardless of what value\n         is passed here.\n         WARNING: This option is experimental and use this at your own risk.\n\n        preserve_module_call_signature: A list of submodule paths for which the original\n         calling conventions are preserved as metadata. The metadata will be used when calling\n         torch.export.unflatten to preserve the original calling conventions of modules.\n\n    Returns:\n        An :class:`ExportedProgram` containing the traced callable.\n\n    **Acceptable input/output types**\n\n    Acceptable types of inputs (for ``args`` and ``kwargs``) and outputs include:\n\n    - Primitive types, i.e. ``torch.Tensor``, ``int``, ``float``, ``bool`` and ``str``.\n    - Dataclasses, but they must be registered by calling :func:`register_dataclass` first.\n    - (Nested) Data structures comprising of ``dict``, ``list``, ``tuple``, ``namedtuple`` and\n      ``OrderedDict`` containing all above types.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.export.load",
      "signature": "torch.export.load(f: Union[str, os.PathLike[str], IO[bytes]], *, extra_files: Optional[dict[str, Any]] = None, expected_opset_version: Optional[dict[str, int]] = None) -> torch.export.exported_program.ExportedProgram",
      "doc": "\n\n    .. warning::\n        Under active development, saved files may not be usable in newer versions\n        of PyTorch.\n\n    Loads an :class:`ExportedProgram` previously saved with\n    :func:`torch.export.save <torch.export.save>`.\n\n    Args:\n        f (str | os.PathLike[str] | IO[bytes]): A file-like object (has to\n         implement write and flush) or a string containing a file name.\n\n        extra_files (Optional[Dict[str, Any]]): The extra filenames given in\n         this map would be loaded and their content would be stored in the\n         provided map.\n\n        expected_opset_version (Optional[Dict[str, int]]): A map of opset names\n         to expected opset versions\n\n    Returns:\n        An :class:`ExportedProgram` object\n\n    Example::\n\n        import torch\n        import io\n\n        # Load ExportedProgram from file\n        ep = torch.export.load('exported_program.pt2')\n\n        # Load ExportedProgram from io.BytesIO object\n        with open('exported_program.pt2', 'rb') as f:\n            buffer = io.BytesIO(f.read())\n        buffer.seek(0)\n        ep = torch.export.load(buffer)\n\n        # Load with extra files.\n        extra_files = {'foo.txt': ''}  # values will be replaced with data\n        ep = torch.export.load('exported_program.pt2', extra_files=extra_files)\n        print(extra_files['foo.txt'])\n        print(ep(torch.randn(5)))\n    ",
      "arguments": [
        "f",
        "extra_files",
        "expected_opset_version"
      ],
      "return_type": "<class 'torch.export.exported_program.ExportedProgram'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n\n    .. warning::\n        Under active development, saved files may not be usable in newer versions\n        of PyTorch.\n\n    Loads an :class:`ExportedProgram` previously saved with\n    :func:`torch.export.save <torch.export.save>`.\n\n    Args:\n        f (str | os.PathLike[str] | IO[bytes]): A file-like object (has to\n         implement write and flush) or a string containing a file name.\n\n        extra_files (Optional[Dict[str, Any]]): The extra filenames given in\n         this map would be loaded and their content would be stored in the\n         provided map.\n\n        expected_opset_version (Optional[Dict[str, int]]): A map of opset names\n         to expected opset versions\n\n    Returns:\n        An :class:`ExportedProgram` object\n\n    Example::\n\n        import torch\n        import io\n\n        # Load ExportedProgram from file\n        ep = torch.export.load('exported_program.pt2')\n\n        # Load ExportedProgram from io.BytesIO object\n        with open('exported_program.pt2', 'rb') as f:\n            buffer = io.BytesIO(f.read())\n        buffer.seek(0)\n        ep = torch.export.load(buffer)\n\n        # Load with extra files.\n        extra_files = {'foo.txt': ''}  # values will be replaced with data\n        ep = torch.export.load('exported_program.pt2', extra_files=extra_files)\n        print(extra_files['foo.txt'])\n        print(ep(torch.randn(5)))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.export.register_dataclass",
      "signature": "torch.export.register_dataclass(cls: type[typing.Any], *, serialized_type_name: Optional[str] = None) -> None",
      "doc": "\n    Registers a dataclass as a valid input/output type for :func:`torch.export.export`.\n\n    Args:\n        cls: the dataclass type to register\n        serialized_type_name: The serialized name for the dataclass. This is\n        required if you want to serialize the pytree TreeSpec containing this\n        dataclass.\n\n    Example::\n\n        import torch\n        from dataclasses import dataclass\n\n        @dataclass\n        class InputDataClass:\n            feature: torch.Tensor\n            bias: int\n\n        @dataclass\n        class OutputDataClass:\n            res: torch.Tensor\n\n        torch.export.register_dataclass(InputDataClass)\n        torch.export.register_dataclass(OutputDataClass)\n\n        class Mod(torch.nn.Module):\n            def forward(self, x: InputDataClass) -> OutputDataClass:\n                res = x.feature + x.bias\n                return OutputDataClass(res=res)\n\n        ep = torch.export.export(Mod(), (InputDataClass(torch.ones(2, 2), 1), ))\n        print(ep)\n\n    ",
      "arguments": [
        "cls",
        "serialized_type_name"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Registers a dataclass as a valid input/output type for :func:`torch.export.export`.\n\n    Args:\n        cls: the dataclass type to register\n        serialized_type_name: The serialized name for the dataclass. This is\n        required if you want to serialize the pytree TreeSpec containing this\n        dataclass.\n\n    Example::\n\n        import torch\n        from dataclasses import dataclass\n\n        @dataclass\n        class InputDataClass:\n            feature: torch.Tensor\n            bias: int\n\n        @dataclass\n        class OutputDataClass:\n            res: torch.Tensor\n\n        torch.export.register_dataclass(InputDataClass)\n        torch.export.register_dataclass(OutputDataClass)\n\n        class Mod(torch.nn.Module):\n            def forward(self, x: InputDataClass) -> OutputDataClass:\n                res = x.feature + x.bias\n                return OutputDataClass(res=res)\n\n        ep = torch.export.export(Mod(), (InputDataClass(torch.ones(2, 2), 1), ))\n        print(ep)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.export.save",
      "signature": "torch.export.save(ep: torch.export.exported_program.ExportedProgram, f: Union[str, os.PathLike[str], IO[bytes]], *, extra_files: Optional[dict[str, Any]] = None, opset_version: Optional[dict[str, int]] = None, pickle_protocol: int = 2) -> None",
      "doc": "\n\n    .. warning::\n        Under active development, saved files may not be usable in newer versions\n        of PyTorch.\n\n    Saves an :class:`ExportedProgram` to a file-like object. It can then be\n    loaded using the Python API :func:`torch.export.load <torch.export.load>`.\n\n    Args:\n        ep (ExportedProgram): The exported program to save.\n\n        f (str | os.PathLike[str] | IO[bytes]) A file-like object (has to\n         implement write and flush) or a string containing a file name.\n\n        extra_files (Optional[Dict[str, Any]]): Map from filename to contents\n         which will be stored as part of f.\n\n        opset_version (Optional[Dict[str, int]]): A map of opset names\n         to the version of this opset\n\n        pickle_protocol: can be specified to override the default protocol\n\n    Example::\n\n        import torch\n        import io\n\n        class MyModule(torch.nn.Module):\n            def forward(self, x):\n                return x + 10\n\n        ep = torch.export.export(MyModule(), (torch.randn(5),))\n\n        # Save to file\n        torch.export.save(ep, 'exported_program.pt2')\n\n        # Save to io.BytesIO buffer\n        buffer = io.BytesIO()\n        torch.export.save(ep, buffer)\n\n        # Save with extra files\n        extra_files = {'foo.txt': b'bar'.decode('utf-8')}\n        torch.export.save(ep, 'exported_program.pt2', extra_files=extra_files)\n\n    ",
      "arguments": [
        "ep",
        "f",
        "extra_files",
        "opset_version",
        "pickle_protocol"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n\n    .. warning::\n        Under active development, saved files may not be usable in newer versions\n        of PyTorch.\n\n    Saves an :class:`ExportedProgram` to a file-like object. It can then be\n    loaded using the Python API :func:`torch.export.load <torch.export.load>`.\n\n    Args:\n        ep (ExportedProgram): The exported program to save.\n\n        f (str | os.PathLike[str] | IO[bytes]) A file-like object (has to\n         implement write and flush) or a string containing a file name.\n\n        extra_files (Optional[Dict[str, Any]]): Map from filename to contents\n         which will be stored as part of f.\n\n        opset_version (Optional[Dict[str, int]]): A map of opset names\n         to the version of this opset\n\n        pickle_protocol: can be specified to override the default protocol\n\n    Example::\n\n        import torch\n        import io\n\n        class MyModule(torch.nn.Module):\n            def forward(self, x):\n                return x + 10\n\n        ep = torch.export.export(MyModule(), (torch.randn(5),))\n\n        # Save to file\n        torch.export.save(ep, 'exported_program.pt2')\n\n        # Save to io.BytesIO buffer\n        buffer = io.BytesIO()\n        torch.export.save(ep, buffer)\n\n        # Save with extra files\n        extra_files = {'foo.txt': b'bar'.decode('utf-8')}\n        torch.export.save(ep, 'exported_program.pt2', extra_files=extra_files)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.export.unflatten",
      "signature": "torch.export.unflatten(module: torch.export.exported_program.ExportedProgram, flat_args_adapter: Optional[torch.export.unflatten.FlatArgsAdapter] = None) -> torch.export.unflatten.UnflattenedModule",
      "doc": "Unflatten an ExportedProgram, producing a module with the same module\n    hierarchy as the original eager module. This can be useful if you are trying\n    to use :mod:`torch.export` with another system that expects a module\n    hierachy instead of the flat graph that :mod:`torch.export` usually produces.\n\n    .. note:: The args/kwargs of unflattened modules will not necessarily match\n        the eager module, so doing a module swap (e.g. :code:`self.submod =\n        new_mod`) will not necessarily work. If you need to swap a module out, you\n        need to set the :code:`preserve_module_call_signature` parameter of\n        :func:`torch.export.export`.\n\n    Args:\n        module (ExportedProgram): The ExportedProgram to unflatten.\n        flat_args_adapter (Optional[FlatArgsAdapter]): Adapt flat args if input TreeSpec does not match with exported module's.\n\n    Returns:\n        An instance of :class:`UnflattenedModule`, which has the same module\n        hierarchy as the original eager module pre-export.\n    ",
      "arguments": [
        "module",
        "flat_args_adapter"
      ],
      "return_type": "<class 'torch.export.unflatten.UnflattenedModule'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Unflatten an ExportedProgram, producing a module with the same module\n    hierarchy as the original eager module. This can be useful if you are trying\n    to use :mod:`torch.export` with another system that expects a module\n    hierachy instead of the flat graph that :mod:`torch.export` usually produces.\n\n    .. note:: The args/kwargs of unflattened modules will not necessarily match\n        the eager module, so doing a module swap (e.g. :code:`self.submod =\n        new_mod`) will not necessarily work. If you need to swap a module out, you\n        need to set the :code:`preserve_module_call_signature` parameter of\n        :func:`torch.export.export`.\n\n    Args:\n        module (ExportedProgram): The ExportedProgram to unflatten.\n        flat_args_adapter (Optional[FlatArgsAdapter]): Adapt flat args if input TreeSpec does not match with exported module's.\n\n    Returns:\n        An instance of :class:`UnflattenedModule`, which has the same module\n        hierarchy as the original eager module pre-export.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "FUNCTIONAL_PROGRAMMING": [
    {
      "function": "torch.func.debug_unwrap",
      "signature": "torch.func.debug_unwrap(tensor: torch.Tensor, *, recurse=True) -> torch.Tensor",
      "doc": "Unwraps a functorch tensor (e.g. BatchedTensor, GradTrackingTensor) to its underlying tensor.\n\n    This function should only be used in a debug setting (e.g. trying to print the\n    value of a Tensor in a debugger). Otherwise, using the result of function\n    inside of a function being transformed will lead to undefined behavior.\n    ",
      "arguments": [
        "tensor",
        "recurse"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Unwraps a functorch tensor (e.g. BatchedTensor, GradTrackingTensor) to its underlying tensor.\n\n    This function should only be used in a debug setting (e.g. trying to print the\n    value of a Tensor in a debugger). Otherwise, using the result of function\n    inside of a function being transformed will lead to undefined behavior.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.functional_call",
      "signature": "torch.func.functional_call(module: 'torch.nn.Module', parameter_and_buffer_dicts: Union[dict[str, torch.Tensor], collections.abc.Sequence[dict[str, torch.Tensor]]], args: Union[Any, tuple, NoneType] = None, kwargs: Optional[dict[str, Any]] = None, *, tie_weights: bool = True, strict: bool = False)",
      "doc": "Performs a functional call on the module by replacing the module parameters\n    and buffers with the provided ones.\n\n    .. note:: If the module has active parametrizations, passing a value in the\n        :attr:`parameter_and_buffer_dicts` argument with the name set to the regular parameter\n        name will completely disable the parametrization.\n        If you want to apply the parametrization function to the value passed\n        please set the key as ``{submodule_name}.parametrizations.{parameter_name}.original``.\n\n    .. note:: If the module performs in-place operations on parameters/buffers, these will be reflected\n        in the ``parameter_and_buffer_dicts`` input.\n\n\n         Example::\n\n            >>> a = {'foo': torch.zeros(())}\n            >>> # xdoctest: +SKIP\n            >>> mod = Foo()  # does self.foo = self.foo + 1\n            >>> print(mod.foo)  # tensor(0.)\n            >>> functional_call(mod, a, torch.ones(()))\n            >>> print(mod.foo)  # tensor(0.)\n            >>> print(a['foo'])  # tensor(1.)\n\n    .. note:: If the module has tied weights, whether or not functional_call respects the tying is determined by the\n        tie_weights flag.\n\n        Example::\n\n            >>> a = {'foo': torch.zeros(())}\n            >>> # xdoctest: +SKIP\n            >>> mod = Foo()  # has both self.foo and self.foo_tied which are tied. Returns x + self.foo + self.foo_tied\n            >>> print(mod.foo)  # tensor(1.)\n            >>> mod(torch.zeros(()))  # tensor(2.)\n            >>> functional_call(mod, a, torch.zeros(()))  # tensor(0.) since it will change self.foo_tied too\n            >>> functional_call(mod, a, torch.zeros(()), tie_weights=False)  # tensor(1.)--self.foo_tied is not updated\n            >>> new_a = {'foo': torch.zeros(()), 'foo_tied': torch.zeros(())}\n            >>> functional_call(mod, new_a, torch.zeros()) # tensor(0.)\n\n    An example of passing multiple dictionaries\n\n    .. code-block:: python\n\n            a = ({'weight': torch.ones(1, 1)}, {'buffer': torch.zeros(1)})  # two separate dictionaries\n            mod = nn.Bar(1, 1)  # return self.weight @ x + self.buffer\n            print(mod.weight)  # tensor(...)\n            print(mod.buffer)  # tensor(...)\n            x = torch.randn((1, 1))\n            print(x)\n            functional_call(mod, a, x)  # same as x\n            print(mod.weight)  # same as before functional_call\n\n\n    And here is an example of applying the grad transform over the parameters\n    of a model.\n\n    .. code-block:: python\n\n        import torch\n        import torch.nn as nn\n        from torch.func import functional_call, grad\n\n        x = torch.randn(4, 3)\n        t = torch.randn(4, 3)\n        model = nn.Linear(3, 3)\n\n        def compute_loss(params, x, t):\n            y = functional_call(model, params, x)\n            return nn.functional.mse_loss(y, t)\n\n        grad_weights = grad(compute_loss)(dict(model.named_parameters()), x, t)\n\n    .. note:: If the user does not need grad tracking outside of grad transforms, they can detach all of the\n        parameters for better performance and memory usage\n\n        Example::\n\n            >>> detached_params = {k: v.detach() for k, v in model.named_parameters()}\n            >>> grad_weights = grad(compute_loss)(detached_params, x, t)\n            >>> grad_weights.grad_fn  # None--it's not tracking gradients outside of grad\n\n        This means that the user cannot call ``grad_weight.backward()``. However, if they don't need autograd tracking\n        outside of the transforms, this will result in less memory usage and faster speeds.\n\n    Args:\n        module (torch.nn.Module): the module to call\n        parameters_and_buffer_dicts (Dict[str, Tensor] or tuple of Dict[str, Tensor]): the parameters that will be used in\n            the module call. If given a tuple of dictionaries, they must have distinct keys so that all dictionaries can\n            be used together\n        args (Any or tuple): arguments to be passed to the module call. If not a tuple, considered a single argument.\n        kwargs (dict): keyword arguments to be passed to the module call\n        tie_weights (bool, optional): If True, then parameters and buffers tied in the original model will be treated as\n            tied in the reparameterized version. Therefore, if True and different values are passed for the tied\n            parameters and buffers, it will error. If False, it will not respect the originally tied parameters and\n            buffers unless the values passed for both weights are the same. Default: True.\n        strict (bool, optional): If True, then the parameters and buffers passed in must match the parameters and\n            buffers in the original module. Therefore, if True and there are any missing or unexpected keys, it will\n            error. Default: False.\n\n    Returns:\n        Any: the result of calling ``module``.\n    ",
      "arguments": [
        "module",
        "parameter_and_buffer_dicts",
        "args",
        "kwargs",
        "tie_weights",
        "strict"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Performs a functional call on the module by replacing the module parameters\n    and buffers with the provided ones.\n\n    .. note:: If the module has active parametrizations, passing a value in the\n        :attr:`parameter_and_buffer_dicts` argument with the name set to the regular parameter\n        name will completely disable the parametrization.\n        If you want to apply the parametrization function to the value passed\n        please set the key as ``{submodule_name}.parametrizations.{parameter_name}.original``.\n\n    .. note:: If the module performs in-place operations on parameters/buffers, these will be reflected\n        in the ``parameter_and_buffer_dicts`` input.\n\n\n         Example::\n\n            >>> a = {'foo': torch.zeros(())}\n            >>> # xdoctest: +SKIP\n            >>> mod = Foo()  # does self.foo = self.foo + 1\n            >>> print(mod.foo)  # tensor(0.)\n            >>> functional_call(mod, a, torch.ones(()))\n            >>> print(mod.foo)  # tensor(0.)\n            >>> print(a['foo'])  # tensor(1.)\n\n    .. note:: If the module has tied weights, whether or not functional_call respects the tying is determined by the\n        tie_weights flag.\n\n        Example::\n\n            >>> a = {'foo': torch.zeros(())}\n            >>> # xdoctest: +SKIP\n            >>> mod = Foo()  # has both self.foo and self.foo_tied which are tied. Returns x + self.foo + self.foo_tied\n            >>> print(mod.foo)  # tensor(1.)\n            >>> mod(torch.zeros(()))  # tensor(2.)\n            >>> functional_call(mod, a, torch.zeros(()))  # tensor(0.) since it will change self.foo_tied too\n            >>> functional_call(mod, a, torch.zeros(()), tie_weights=False)  # tensor(1.)--self.foo_tied is not updated\n            >>> new_a = {'foo': torch.zeros(()), 'foo_tied': torch.zeros(())}\n            >>> functional_call(mod, new_a, torch.zeros()) # tensor(0.)\n\n    An example of passing multiple dictionaries\n\n    .. code-block:: python\n\n            a = ({'weight': torch.ones(1, 1)}, {'buffer': torch.zeros(1)})  # two separate dictionaries\n            mod = nn.Bar(1, 1)  # return self.weight @ x + self.buffer\n            print(mod.weight)  # tensor(...)\n            print(mod.buffer)  # tensor(...)\n            x = torch.randn((1, 1))\n            print(x)\n            functional_call(mod, a, x)  # same as x\n            print(mod.weight)  # same as before functional_call\n\n\n    And here is an example of applying the grad transform over the parameters\n    of a model.\n\n    .. code-block:: python\n\n        import torch\n        import torch.nn as nn\n        from torch.func import functional_call, grad\n\n        x = torch.randn(4, 3)\n        t = torch.randn(4, 3)\n        model = nn.Linear(3, 3)\n\n        def compute_loss(params, x, t):\n            y = functional_call(model, params, x)\n            return nn.functional.mse_loss(y, t)\n\n        grad_weights = grad(compute_loss)(dict(model.named_parameters()), x, t)\n\n    .. note:: If the user does not need grad tracking outside of grad transforms, they can detach all of the\n        parameters for better performance and memory usage\n\n        Example::\n\n            >>> detached_params = {k: v.detach() for k, v in model.named_parameters()}\n            >>> grad_weights = grad(compute_loss)(detached_params, x, t)\n            >>> grad_weights.grad_fn  # None--it's not tracking gradients outside of grad\n\n        This means that the user cannot call ``grad_weight.backward()``. However, if they don't need autograd tracking\n        outside of the transforms, this will result in less memory usage and faster speeds.\n\n    Args:\n        module (torch.nn.Module): the module to call\n        parameters_and_buffer_dicts (Dict[str, Tensor] or tuple of Dict[str, Tensor]): the parameters that will be used in\n            the module call. If given a tuple of dictionaries, they must have distinct keys so that all dictionaries can\n            be used together\n        args (Any or tuple): arguments to be passed to the module call. If not a tuple, considered a single argument.\n        kwargs (dict): keyword arguments to be passed to the module call\n        tie_weights (bool, optional): If True, then parameters and buffers tied in the original model will be treated as\n            tied in the reparameterized version. Therefore, if True and different values are passed for the tied\n            parameters and buffers, it will error. If False, it will not respect the originally tied parameters and\n            buffers unless the values passed for both weights are the same. Default: True.\n        strict (bool, optional): If True, then the parameters and buffers passed in must match the parameters and\n            buffers in the original module. Therefore, if True and there are any missing or unexpected keys, it will\n            error. Default: False.\n\n    Returns:\n        Any: the result of calling ``module``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.functionalize",
      "signature": "torch.func.functionalize(func: Callable, *, remove: str = 'mutations') -> Callable",
      "doc": "\n    functionalize is a transform that can be used to remove (intermediate)\n    mutations and aliasing from a function, while preserving the function's\n    semantics.\n\n    ``functionalize(func)`` returns a new function with the same semantics\n    as ``func``, but with all intermediate mutations removed.\n    Every inplace operation performed on an intermediate tensor:\n    ``intermediate.foo_()``\n    gets replaced by its out-of-place equivalent:\n    ``intermediate_updated = intermediate.foo()``.\n\n    functionalize is useful for shipping a pytorch program off to\n    backends or compilers that aren't able to easily represent\n    mutations or aliasing operators.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments.\n        remove (str): An optional string argument, that takes on either\n            the value 'mutations' or 'mutations_and_views'.\n            If 'mutations' is passed in then all mutating operators\n            will be replaced with their non-mutating equivalents.\n            If 'mutations_and_views' is passed in, then additionally, all aliasing\n            operators will be replaced with their non-aliasing equivalents.\n            Default: 'mutations'.\n\n    Returns:\n        Returns a new \"functionalized\" function. It takes the same inputs as\n        ``func``, and has the same behavior, but any mutations\n        (and optionally aliasing) performed on intermediate tensors\n        in the function will be removed.\n\n    functionalize will also remove mutations (and views) that were performed on function inputs.\n    However to preserve semantics, functionalize will \"fix up\" the mutations after\n    the transform has finished running, by detecting if any tensor inputs \"should have\"\n    been mutated, and copying the new data back to the inputs if necessary.\n\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> import torch\n        >>> from torch.fx.experimental.proxy_tensor import make_fx\n        >>> from torch.func import functionalize\n        >>>\n        >>> # A function that uses mutations and views, but only on intermediate tensors.\n        >>> def f(a):\n        ...     b = a + 1\n        ...     c = b.view(-1)\n        ...     c.add_(1)\n        ...     return b\n        ...\n        >>> inpt = torch.randn(2)\n        >>>\n        >>> out1 = f(inpt)\n        >>> out2 = functionalize(f)(inpt)\n        >>>\n        >>> # semantics are the same (outputs are equivalent)\n        >>> print(torch.allclose(out1, out2))\n        True\n        >>>\n        >>> f_traced = make_fx(f)(inpt)\n        >>> f_no_mutations_traced = make_fx(functionalize(f))(inpt)\n        >>> f_no_mutations_and_views_traced = make_fx(functionalize(f, remove='mutations_and_views'))(inpt)\n        >>>\n        >>> print(f_traced.code)\n\n\n\n        def forward(self, a_1):\n            add = torch.ops.aten.add(a_1, 1);  a_1 = None\n            view = torch.ops.aten.view(add, [-1])\n            add_ = torch.ops.aten.add_(view, 1);  view = None\n            return add\n\n        >>> print(f_no_mutations_traced.code)\n\n\n\n        def forward(self, a_1):\n            add = torch.ops.aten.add(a_1, 1);  a_1 = None\n            view = torch.ops.aten.view(add, [-1]);  add = None\n            add_1 = torch.ops.aten.add(view, 1);  view = None\n            view_1 = torch.ops.aten.view(add_1, [2]);  add_1 = None\n            return view_1\n\n        >>> print(f_no_mutations_and_views_traced.code)\n\n\n\n        def forward(self, a_1):\n            add = torch.ops.aten.add(a_1, 1);  a_1 = None\n            view_copy = torch.ops.aten.view_copy(add, [-1]);  add = None\n            add_1 = torch.ops.aten.add(view_copy, 1);  view_copy = None\n            view_copy_1 = torch.ops.aten.view_copy(add_1, [2]);  add_1 = None\n            return view_copy_1\n\n\n        >>> # A function that mutates its input tensor\n        >>> def f(a):\n        ...     b = a.view(-1)\n        ...     b.add_(1)\n        ...     return a\n        ...\n        >>> f_no_mutations_and_views_traced = make_fx(functionalize(f, remove='mutations_and_views'))(inpt)\n        >>> #\n        >>> # All mutations and views have been removed,\n        >>> # but there is an extra copy_ in the graph to correctly apply the mutation to the input\n        >>> # after the function has completed.\n        >>> print(f_no_mutations_and_views_traced.code)\n\n\n\n        def forward(self, a_1):\n            view_copy = torch.ops.aten.view_copy(a_1, [-1])\n            add = torch.ops.aten.add(view_copy, 1);  view_copy = None\n            view_copy_1 = torch.ops.aten.view_copy(add, [2]);  add = None\n            copy_ = torch.ops.aten.copy_(a_1, view_copy_1);  a_1 = None\n            return view_copy_1\n\n\n    There are a few \"failure modes\" for functionalize that are worth calling out:\n      (1) Like other torch.func transforms, `functionalize()` doesn't work with functions\n          that directly use `.backward()`. The same is true for torch.autograd.grad.\n          If you want to use autograd, you can compute gradients directly\n          with `functionalize(grad(f))`.\n      (2) Like other torch.func transforms, `functionalize()` doesn't work with global state.\n          If you call `functionalize(f)` on a function that takes views / mutations of\n          non-local state, functionalization will simply no-op and pass the view/mutation\n          calls directly to the backend.\n          One way to work around this is is to ensure that any non-local state creation\n          is wrapped into a larger function, which you then call functionalize on.\n      (3) `resize_()` has some limitations: functionalize will only work on programs\n          that use resize_()` as long as the tensor being resized is not a view.\n      (4) `as_strided()` has some limitations: functionalize will not work on\n          `as_strided()` calls that result in tensors with overlapping memory.\n\n\n    Finally, a helpful mental model for understanding functionalization is that\n    most user pytorch programs are writing with the public torch API.\n    When executed, torch operators are generally decomposed into\n    our internal C++ \"ATen\" API.\n    The logic for functionalization happens entirely at the level of ATen.\n    Functionalization knows how to take every aliasing operator in ATen,\n    and map it to its non-aliasing equivalent\n    (e.g. ``tensor.view({-1})`` -> ``at::view_copy(tensor, {-1})``),\n    and how to take every mutating operator in ATen,\n    and map it to its non-mutating equivalent\n    (e.g. ``tensor.add_(1)`` -> ``at::add(tensor, -1)``),\n    while tracking aliases and mutations out-of-line to know when to fix things up.\n    Information about which ATen operators are aliasing or mutating all comes from\n    https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml.\n    ",
      "arguments": [
        "func",
        "remove"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    functionalize is a transform that can be used to remove (intermediate)\n    mutations and aliasing from a function, while preserving the function's\n    semantics.\n\n    ``functionalize(func)`` returns a new function with the same semantics\n    as ``func``, but with all intermediate mutations removed.\n    Every inplace operation performed on an intermediate tensor:\n    ``intermediate.foo_()``\n    gets replaced by its out-of-place equivalent:\n    ``intermediate_updated = intermediate.foo()``.\n\n    functionalize is useful for shipping a pytorch program off to\n    backends or compilers that aren't able to easily represent\n    mutations or aliasing operators.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments.\n        remove (str): An optional string argument, that takes on either\n            the value 'mutations' or 'mutations_and_views'.\n            If 'mutations' is passed in then all mutating operators\n            will be replaced with their non-mutating equivalents.\n            If 'mutations_and_views' is passed in, then additionally, all aliasing\n            operators will be replaced with their non-aliasing equivalents.\n            Default: 'mutations'.\n\n    Returns:\n        Returns a new \"functionalized\" function. It takes the same inputs as\n        ``func``, and has the same behavior, but any mutations\n        (and optionally aliasing) performed on intermediate tensors\n        in the function will be removed.\n\n    functionalize will also remove mutations (and views) that were performed on function inputs.\n    However to preserve semantics, functionalize will \"fix up\" the mutations after\n    the transform has finished running, by detecting if any tensor inputs \"should have\"\n    been mutated, and copying the new data back to the inputs if necessary.\n\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> import torch\n        >>> from torch.fx.experimental.proxy_tensor import make_fx\n        >>> from torch.func import functionalize\n        >>>\n        >>> # A function that uses mutations and views, but only on intermediate tensors.\n        >>> def f(a):\n        ...     b = a + 1\n        ...     c = b.view(-1)\n        ...     c.add_(1)\n        ...     return b\n        ...\n        >>> inpt = torch.randn(2)\n        >>>\n        >>> out1 = f(inpt)\n        >>> out2 = functionalize(f)(inpt)\n        >>>\n        >>> # semantics are the same (outputs are equivalent)\n        >>> print(torch.allclose(out1, out2))\n        True\n        >>>\n        >>> f_traced = make_fx(f)(inpt)\n        >>> f_no_mutations_traced = make_fx(functionalize(f))(inpt)\n        >>> f_no_mutations_and_views_traced = make_fx(functionalize(f, remove='mutations_and_views'))(inpt)\n        >>>\n        >>> print(f_traced.code)\n\n\n\n        def forward(self, a_1):\n            add = torch.ops.aten.add(a_1, 1);  a_1 = None\n            view = torch.ops.aten.view(add, [-1])\n            add_ = torch.ops.aten.add_(view, 1);  view = None\n            return add\n\n        >>> print(f_no_mutations_traced.code)\n\n\n\n        def forward(self, a_1):\n            add = torch.ops.aten.add(a_1, 1);  a_1 = None\n            view = torch.ops.aten.view(add, [-1]);  add = None\n            add_1 = torch.ops.aten.add(view, 1);  view = None\n            view_1 = torch.ops.aten.view(add_1, [2]);  add_1 = None\n            return view_1\n\n        >>> print(f_no_mutations_and_views_traced.code)\n\n\n\n        def forward(self, a_1):\n            add = torch.ops.aten.add(a_1, 1);  a_1 = None\n            view_copy = torch.ops.aten.view_copy(add, [-1]);  add = None\n            add_1 = torch.ops.aten.add(view_copy, 1);  view_copy = None\n            view_copy_1 = torch.ops.aten.view_copy(add_1, [2]);  add_1 = None\n            return view_copy_1\n\n\n        >>> # A function that mutates its input tensor\n        >>> def f(a):\n        ...     b = a.view(-1)\n        ...     b.add_(1)\n        ...     return a\n        ...\n        >>> f_no_mutations_and_views_traced = make_fx(functionalize(f, remove='mutations_and_views'))(inpt)\n        >>> #\n        >>> # All mutations and views have been removed,\n        >>> # but there is an extra copy_ in the graph to correctly apply the mutation to the input\n        >>> # after the function has completed.\n        >>> print(f_no_mutations_and_views_traced.code)\n\n\n\n        def forward(self, a_1):\n            view_copy = torch.ops.aten.view_copy(a_1, [-1])\n            add = torch.ops.aten.add(view_copy, 1);  view_copy = None\n            view_copy_1 = torch.ops.aten.view_copy(add, [2]);  add = None\n            copy_ = torch.ops.aten.copy_(a_1, view_copy_1);  a_1 = None\n            return view_copy_1\n\n\n    There are a few \"failure modes\" for functionalize that are worth calling out:\n      (1) Like other torch.func transforms, `functionalize()` doesn't work with functions\n          that directly use `.backward()`. The same is true for torch.autograd.grad.\n          If you want to use autograd, you can compute gradients directly\n          with `functionalize(grad(f))`.\n      (2) Like other torch.func transforms, `functionalize()` doesn't work with global state.\n          If you call `functionalize(f)` on a function that takes views / mutations of\n          non-local state, functionalization will simply no-op and pass the view/mutation\n          calls directly to the backend.\n          One way to work around this is is to ensure that any non-local state creation\n          is wrapped into a larger function, which you then call functionalize on.\n      (3) `resize_()` has some limitations: functionalize will only work on programs\n          that use resize_()` as long as the tensor being resized is not a view.\n      (4) `as_strided()` has some limitations: functionalize will not work on\n          `as_strided()` calls that result in tensors with overlapping memory.\n\n\n    Finally, a helpful mental model for understanding functionalization is that\n    most user pytorch programs are writing with the public torch API.\n    When executed, torch operators are generally decomposed into\n    our internal C++ \"ATen\" API.\n    The logic for functionalization happens entirely at the level of ATen.\n    Functionalization knows how to take every aliasing operator in ATen,\n    and map it to its non-aliasing equivalent\n    (e.g. ``tensor.view({-1})`` -> ``at::view_copy(tensor, {-1})``),\n    and how to take every mutating operator in ATen,\n    and map it to its non-mutating equivalent\n    (e.g. ``tensor.add_(1)`` -> ``at::add(tensor, -1)``),\n    while tracking aliases and mutations out-of-line to know when to fix things up.\n    Information about which ATen operators are aliasing or mutating all comes from\n    https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.grad",
      "signature": "torch.func.grad(func: Callable, argnums: Union[int, tuple[int, ...]] = 0, has_aux: bool = False) -> Callable",
      "doc": "``grad`` operator helps computing gradients of ``func`` with respect to the\n    input(s) specified by ``argnums``. This operator can be nested to\n    compute higher-order gradients.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments.\n            Must return a single-element Tensor. If specified ``has_aux`` equals ``True``,\n            function can return a tuple of single-element Tensor and other auxiliary objects:\n            ``(output, aux)``.\n        argnums (int or Tuple[int]): Specifies arguments to compute gradients with respect to.\n            ``argnums`` can be single integer or tuple of integers. Default: 0.\n        has_aux (bool): Flag indicating that ``func`` returns a tensor and other\n            auxiliary objects: ``(output, aux)``. Default: False.\n\n    Returns:\n        Function to compute gradients with respect to its inputs. By default, the output of\n        the function is the gradient tensor(s) with respect to the first argument.\n        If specified ``has_aux`` equals ``True``, tuple of gradients and output auxiliary objects\n        is returned. If ``argnums`` is a tuple of integers, a tuple of output gradients with\n        respect to each ``argnums`` value is returned.\n\n    Example of using ``grad``:\n\n        >>> # xdoctest: +SKIP\n        >>> from torch.func import grad\n        >>> x = torch.randn([])\n        >>> cos_x = grad(lambda x: torch.sin(x))(x)\n        >>> assert torch.allclose(cos_x, x.cos())\n        >>>\n        >>> # Second-order gradients\n        >>> neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\n        >>> assert torch.allclose(neg_sin_x, -x.sin())\n\n    When composed with ``vmap``, ``grad`` can be used to compute per-sample-gradients:\n\n        >>> # xdoctest: +SKIP\n        >>> from torch.func import grad, vmap\n        >>> batch_size, feature_size = 3, 5\n        >>>\n        >>> def model(weights, feature_vec):\n        >>>     # Very simple linear model with activation\n        >>>     assert feature_vec.dim() == 1\n        >>>     return feature_vec.dot(weights).relu()\n        >>>\n        >>> def compute_loss(weights, example, target):\n        >>>     y = model(weights, example)\n        >>>     return ((y - target) ** 2).mean()  # MSELoss\n        >>>\n        >>> weights = torch.randn(feature_size, requires_grad=True)\n        >>> examples = torch.randn(batch_size, feature_size)\n        >>> targets = torch.randn(batch_size)\n        >>> inputs = (weights, examples, targets)\n        >>> grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n\n    Example of using ``grad`` with ``has_aux`` and ``argnums``:\n\n        >>> # xdoctest: +SKIP\n        >>> from torch.func import grad\n        >>> def my_loss_func(y, y_pred):\n        >>>    loss_per_sample = (0.5 * y_pred - y) ** 2\n        >>>    loss = loss_per_sample.mean()\n        >>>    return loss, (y_pred, loss_per_sample)\n        >>>\n        >>> fn = grad(my_loss_func, argnums=(0, 1), has_aux=True)\n        >>> y_true = torch.rand(4)\n        >>> y_preds = torch.rand(4, requires_grad=True)\n        >>> out = fn(y_true, y_preds)\n        >>> # > output is ((grads w.r.t y_true, grads w.r.t y_preds), (y_pred, loss_per_sample))\n\n    .. note::\n        Using PyTorch ``torch.no_grad`` together with ``grad``.\n\n        Case 1: Using ``torch.no_grad`` inside a function:\n\n            >>> # xdoctest: +SKIP\n            >>> def f(x):\n            >>>     with torch.no_grad():\n            >>>         c = x ** 2\n            >>>     return x - c\n\n        In this case, ``grad(f)(x)`` will respect the inner ``torch.no_grad``.\n\n        Case 2: Using ``grad`` inside ``torch.no_grad`` context manager:\n\n            >>> # xdoctest: +SKIP\n            >>> with torch.no_grad():\n            >>>     grad(f)(x)\n\n        In this case, ``grad`` will respect the inner ``torch.no_grad``, but not the\n        outer one. This is because ``grad`` is a \"function transform\": its result\n        should not depend on the result of a context manager outside of ``f``.\n\n    ",
      "arguments": [
        "func",
        "argnums",
        "has_aux"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "``grad`` operator helps computing gradients of ``func`` with respect to the\n    input(s) specified by ``argnums``. This operator can be nested to\n    compute higher-order gradients.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments.\n            Must return a single-element Tensor. If specified ``has_aux`` equals ``True``,\n            function can return a tuple of single-element Tensor and other auxiliary objects:\n            ``(output, aux)``.\n        argnums (int or Tuple[int]): Specifies arguments to compute gradients with respect to.\n            ``argnums`` can be single integer or tuple of integers. Default: 0.\n        has_aux (bool): Flag indicating that ``func`` returns a tensor and other\n            auxiliary objects: ``(output, aux)``. Default: False.\n\n    Returns:\n        Function to compute gradients with respect to its inputs. By default, the output of\n        the function is the gradient tensor(s) with respect to the first argument.\n        If specified ``has_aux`` equals ``True``, tuple of gradients and output auxiliary objects\n        is returned. If ``argnums`` is a tuple of integers, a tuple of output gradients with\n        respect to each ``argnums`` value is returned.\n\n    Example of using ``grad``:\n\n        >>> # xdoctest: +SKIP\n        >>> from torch.func import grad\n        >>> x = torch.randn([])\n        >>> cos_x = grad(lambda x: torch.sin(x))(x)\n        >>> assert torch.allclose(cos_x, x.cos())\n        >>>\n        >>> # Second-order gradients\n        >>> neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\n        >>> assert torch.allclose(neg_sin_x, -x.sin())\n\n    When composed with ``vmap``, ``grad`` can be used to compute per-sample-gradients:\n\n        >>> # xdoctest: +SKIP\n        >>> from torch.func import grad, vmap\n        >>> batch_size, feature_size = 3, 5\n        >>>\n        >>> def model(weights, feature_vec):\n        >>>     # Very simple linear model with activation\n        >>>     assert feature_vec.dim() == 1\n        >>>     return feature_vec.dot(weights).relu()\n        >>>\n        >>> def compute_loss(weights, example, target):\n        >>>     y = model(weights, example)\n        >>>     return ((y - target) ** 2).mean()  # MSELoss\n        >>>\n        >>> weights = torch.randn(feature_size, requires_grad=True)\n        >>> examples = torch.randn(batch_size, feature_size)\n        >>> targets = torch.randn(batch_size)\n        >>> inputs = (weights, examples, targets)\n        >>> grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n\n    Example of using ``grad`` with ``has_aux`` and ``argnums``:\n\n        >>> # xdoctest: +SKIP\n        >>> from torch.func import grad\n        >>> def my_loss_func(y, y_pred):\n        >>>    loss_per_sample = (0.5 * y_pred - y) ** 2\n        >>>    loss = loss_per_sample.mean()\n        >>>    return loss, (y_pred, loss_per_sample)\n        >>>\n        >>> fn = grad(my_loss_func, argnums=(0, 1), has_aux=True)\n        >>> y_true = torch.rand(4)\n        >>> y_preds = torch.rand(4, requires_grad=True)\n        >>> out = fn(y_true, y_preds)\n        >>> # > output is ((grads w.r.t y_true, grads w.r.t y_preds), (y_pred, loss_per_sample))\n\n    .. note::\n        Using PyTorch ``torch.no_grad`` together with ``grad``.\n\n        Case 1: Using ``torch.no_grad`` inside a function:\n\n            >>> # xdoctest: +SKIP\n            >>> def f(x):\n            >>>     with torch.no_grad():\n            >>>         c = x ** 2\n            >>>     return x - c\n\n        In this case, ``grad(f)(x)`` will respect the inner ``torch.no_grad``.\n\n        Case 2: Using ``grad`` inside ``torch.no_grad`` context manager:\n\n            >>> # xdoctest: +SKIP\n            >>> with torch.no_grad():\n            >>>     grad(f)(x)\n\n        In this case, ``grad`` will respect the inner ``torch.no_grad``, but not the\n        outer one. This is because ``grad`` is a \"function transform\": its result\n        should not depend on the result of a context manager outside of ``f``.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.grad_and_value",
      "signature": "torch.func.grad_and_value(func: Callable, argnums: Union[int, tuple[int, ...]] = 0, has_aux: bool = False) -> Callable",
      "doc": "\n    Returns a function to compute a tuple of the gradient and primal, or\n    forward, computation.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments.\n            Must return a single-element Tensor. If specified ``has_aux``\n            equals ``True``, function can return a tuple of single-element\n            Tensor and other auxiliary objects: ``(output, aux)``.\n        argnums (int or Tuple[int]): Specifies arguments to compute gradients\n            with respect to. ``argnums`` can be single integer or tuple of\n            integers. Default: 0.\n        has_aux (bool): Flag indicating that ``func`` returns a tensor and\n            other auxiliary objects: ``(output, aux)``. Default: False.\n\n    Returns:\n        Function to compute a tuple of gradients with respect to its inputs\n        and the forward computation. By default, the output of the function is\n        a tuple of the gradient tensor(s) with respect to the first argument\n        and the primal computation. If specified ``has_aux`` equals\n        ``True``, tuple of gradients and tuple of the forward computation with\n        output auxiliary objects is returned. If ``argnums`` is a tuple of\n        integers, a tuple of a tuple of the output gradients with respect to\n        each ``argnums`` value and the forward computation is returned.\n\n    See :func:`grad` for examples\n    ",
      "arguments": [
        "func",
        "argnums",
        "has_aux"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns a function to compute a tuple of the gradient and primal, or\n    forward, computation.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments.\n            Must return a single-element Tensor. If specified ``has_aux``\n            equals ``True``, function can return a tuple of single-element\n            Tensor and other auxiliary objects: ``(output, aux)``.\n        argnums (int or Tuple[int]): Specifies arguments to compute gradients\n            with respect to. ``argnums`` can be single integer or tuple of\n            integers. Default: 0.\n        has_aux (bool): Flag indicating that ``func`` returns a tensor and\n            other auxiliary objects: ``(output, aux)``. Default: False.\n\n    Returns:\n        Function to compute a tuple of gradients with respect to its inputs\n        and the forward computation. By default, the output of the function is\n        a tuple of the gradient tensor(s) with respect to the first argument\n        and the primal computation. If specified ``has_aux`` equals\n        ``True``, tuple of gradients and tuple of the forward computation with\n        output auxiliary objects is returned. If ``argnums`` is a tuple of\n        integers, a tuple of a tuple of the output gradients with respect to\n        each ``argnums`` value and the forward computation is returned.\n\n    See :func:`grad` for examples\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.hessian",
      "signature": "torch.func.hessian(func, argnums=0)",
      "doc": "\n    Computes the Hessian of ``func`` with respect to the arg(s) at index\n    ``argnum`` via a forward-over-reverse strategy.\n\n    The forward-over-reverse strategy (composing ``jacfwd(jacrev(func))``) is\n    a good default for good performance. It is possible to compute Hessians\n    through other compositions of :func:`jacfwd` and :func:`jacrev` like\n    ``jacfwd(jacfwd(func))`` or ``jacrev(jacrev(func))``.\n\n    Args:\n        func (function): A Python function that takes one or more arguments,\n            one of which must be a Tensor, and returns one or more Tensors\n        argnums (int or Tuple[int]): Optional, integer or tuple of integers,\n            saying which arguments to get the Hessian with respect to.\n            Default: 0.\n\n    Returns:\n        Returns a function that takes in the same inputs as ``func`` and\n        returns the Hessian of ``func`` with respect to the arg(s) at\n        ``argnums``.\n\n    .. note::\n        You may see this API error out with \"forward-mode AD not implemented\n        for operator X\". If so, please file a bug report and we will prioritize it.\n        An alternative is to use ``jacrev(jacrev(func))``, which has better\n        operator coverage.\n\n    A basic usage with a R^N -> R^1 function gives a N x N Hessian:\n\n        >>> from torch.func import hessian\n        >>> def f(x):\n        >>>   return x.sin().sum()\n        >>>\n        >>> x = torch.randn(5)\n        >>> hess = hessian(f)(x)  # equivalent to jacfwd(jacrev(f))(x)\n        >>> assert torch.allclose(hess, torch.diag(-x.sin()))\n\n    ",
      "arguments": [
        "func",
        "argnums"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the Hessian of ``func`` with respect to the arg(s) at index\n    ``argnum`` via a forward-over-reverse strategy.\n\n    The forward-over-reverse strategy (composing ``jacfwd(jacrev(func))``) is\n    a good default for good performance. It is possible to compute Hessians\n    through other compositions of :func:`jacfwd` and :func:`jacrev` like\n    ``jacfwd(jacfwd(func))`` or ``jacrev(jacrev(func))``.\n\n    Args:\n        func (function): A Python function that takes one or more arguments,\n            one of which must be a Tensor, and returns one or more Tensors\n        argnums (int or Tuple[int]): Optional, integer or tuple of integers,\n            saying which arguments to get the Hessian with respect to.\n            Default: 0.\n\n    Returns:\n        Returns a function that takes in the same inputs as ``func`` and\n        returns the Hessian of ``func`` with respect to the arg(s) at\n        ``argnums``.\n\n    .. note::\n        You may see this API error out with \"forward-mode AD not implemented\n        for operator X\". If so, please file a bug report and we will prioritize it.\n        An alternative is to use ``jacrev(jacrev(func))``, which has better\n        operator coverage.\n\n    A basic usage with a R^N -> R^1 function gives a N x N Hessian:\n\n        >>> from torch.func import hessian\n        >>> def f(x):\n        >>>   return x.sin().sum()\n        >>>\n        >>> x = torch.randn(5)\n        >>> hess = hessian(f)(x)  # equivalent to jacfwd(jacrev(f))(x)\n        >>> assert torch.allclose(hess, torch.diag(-x.sin()))\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.jacfwd",
      "signature": "torch.func.jacfwd(func: Callable, argnums: Union[int, tuple[int, ...]] = 0, has_aux: bool = False, *, randomness: str = 'error')",
      "doc": "\n    Computes the Jacobian of ``func`` with respect to the arg(s) at index\n    ``argnum`` using forward-mode autodiff\n\n    Args:\n        func (function): A Python function that takes one or more arguments,\n            one of which must be a Tensor, and returns one or more Tensors\n        argnums (int or Tuple[int]): Optional, integer or tuple of integers,\n            saying which arguments to get the Jacobian with respect to.\n            Default: 0.\n        has_aux (bool): Flag indicating that ``func`` returns a\n            ``(output, aux)`` tuple where the first element is the output of\n            the function to be differentiated and the second element is\n            auxiliary objects that will not be differentiated.\n            Default: False.\n        randomness(str): Flag indicating what type of randomness to use.\n            See :func:`vmap` for more detail. Allowed: \"different\", \"same\", \"error\".\n            Default: \"error\"\n\n    Returns:\n        Returns a function that takes in the same inputs as ``func`` and\n        returns the Jacobian of ``func`` with respect to the arg(s) at\n        ``argnums``. If ``has_aux is True``, then the returned function\n        instead returns a ``(jacobian, aux)`` tuple where ``jacobian``\n        is the Jacobian and ``aux`` is auxiliary objects returned by ``func``.\n\n    .. note::\n        You may see this API error out with \"forward-mode AD not implemented\n        for operator X\". If so, please file a bug report and we will prioritize it.\n        An alternative is to use :func:`jacrev`, which has better operator coverage.\n\n    A basic usage with a pointwise, unary operation will give a diagonal array\n    as the Jacobian\n\n        >>> from torch.func import jacfwd\n        >>> x = torch.randn(5)\n        >>> jacobian = jacfwd(torch.sin)(x)\n        >>> expected = torch.diag(torch.cos(x))\n        >>> assert torch.allclose(jacobian, expected)\n\n    :func:`jacfwd` can be composed with vmap to produce batched\n    Jacobians:\n\n        >>> from torch.func import jacfwd, vmap\n        >>> x = torch.randn(64, 5)\n        >>> jacobian = vmap(jacfwd(torch.sin))(x)\n        >>> assert jacobian.shape == (64, 5, 5)\n\n    If you would like to compute the output of the function as well as the\n    jacobian of the function, use the ``has_aux`` flag to return the output\n    as an auxiliary object:\n\n        >>> from torch.func import jacfwd\n        >>> x = torch.randn(5)\n        >>>\n        >>> def f(x):\n        >>>   return x.sin()\n        >>>\n        >>> def g(x):\n        >>>   result = f(x)\n        >>>   return result, result\n        >>>\n        >>> jacobian_f, f_x = jacfwd(g, has_aux=True)(x)\n        >>> assert torch.allclose(f_x, f(x))\n\n    Additionally, :func:`jacrev` can be composed with itself or :func:`jacrev`\n    to produce Hessians\n\n        >>> from torch.func import jacfwd, jacrev\n        >>> def f(x):\n        >>>   return x.sin().sum()\n        >>>\n        >>> x = torch.randn(5)\n        >>> hessian = jacfwd(jacrev(f))(x)\n        >>> assert torch.allclose(hessian, torch.diag(-x.sin()))\n\n    By default, :func:`jacfwd` computes the Jacobian with respect to the first\n    input. However, it can compute the Jacboian with respect to a different\n    argument by using ``argnums``:\n\n        >>> from torch.func import jacfwd\n        >>> def f(x, y):\n        >>>   return x + y ** 2\n        >>>\n        >>> x, y = torch.randn(5), torch.randn(5)\n        >>> jacobian = jacfwd(f, argnums=1)(x, y)\n        >>> expected = torch.diag(2 * y)\n        >>> assert torch.allclose(jacobian, expected)\n\n    Additionally, passing a tuple to ``argnums`` will compute the Jacobian\n    with respect to multiple arguments\n\n        >>> from torch.func import jacfwd\n        >>> def f(x, y):\n        >>>   return x + y ** 2\n        >>>\n        >>> x, y = torch.randn(5), torch.randn(5)\n        >>> jacobian = jacfwd(f, argnums=(0, 1))(x, y)\n        >>> expectedX = torch.diag(torch.ones_like(x))\n        >>> expectedY = torch.diag(2 * y)\n        >>> assert torch.allclose(jacobian[0], expectedX)\n        >>> assert torch.allclose(jacobian[1], expectedY)\n\n    ",
      "arguments": [
        "func",
        "argnums",
        "has_aux",
        "randomness"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the Jacobian of ``func`` with respect to the arg(s) at index\n    ``argnum`` using forward-mode autodiff\n\n    Args:\n        func (function): A Python function that takes one or more arguments,\n            one of which must be a Tensor, and returns one or more Tensors\n        argnums (int or Tuple[int]): Optional, integer or tuple of integers,\n            saying which arguments to get the Jacobian with respect to.\n            Default: 0.\n        has_aux (bool): Flag indicating that ``func`` returns a\n            ``(output, aux)`` tuple where the first element is the output of\n            the function to be differentiated and the second element is\n            auxiliary objects that will not be differentiated.\n            Default: False.\n        randomness(str): Flag indicating what type of randomness to use.\n            See :func:`vmap` for more detail. Allowed: \"different\", \"same\", \"error\".\n            Default: \"error\"\n\n    Returns:\n        Returns a function that takes in the same inputs as ``func`` and\n        returns the Jacobian of ``func`` with respect to the arg(s) at\n        ``argnums``. If ``has_aux is True``, then the returned function\n        instead returns a ``(jacobian, aux)`` tuple where ``jacobian``\n        is the Jacobian and ``aux`` is auxiliary objects returned by ``func``.\n\n    .. note::\n        You may see this API error out with \"forward-mode AD not implemented\n        for operator X\". If so, please file a bug report and we will prioritize it.\n        An alternative is to use :func:`jacrev`, which has better operator coverage.\n\n    A basic usage with a pointwise, unary operation will give a diagonal array\n    as the Jacobian\n\n        >>> from torch.func import jacfwd\n        >>> x = torch.randn(5)\n        >>> jacobian = jacfwd(torch.sin)(x)\n        >>> expected = torch.diag(torch.cos(x))\n        >>> assert torch.allclose(jacobian, expected)\n\n    :func:`jacfwd` can be composed with vmap to produce batched\n    Jacobians:\n\n        >>> from torch.func import jacfwd, vmap\n        >>> x = torch.randn(64, 5)\n        >>> jacobian = vmap(jacfwd(torch.sin))(x)\n        >>> assert jacobian.shape == (64, 5, 5)\n\n    If you would like to compute the output of the function as well as the\n    jacobian of the function, use the ``has_aux`` flag to return the output\n    as an auxiliary object:\n\n        >>> from torch.func import jacfwd\n        >>> x = torch.randn(5)\n        >>>\n        >>> def f(x):\n        >>>   return x.sin()\n        >>>\n        >>> def g(x):\n        >>>   result = f(x)\n        >>>   return result, result\n        >>>\n        >>> jacobian_f, f_x = jacfwd(g, has_aux=True)(x)\n        >>> assert torch.allclose(f_x, f(x))\n\n    Additionally, :func:`jacrev` can be composed with itself or :func:`jacrev`\n    to produce Hessians\n\n        >>> from torch.func import jacfwd, jacrev\n        >>> def f(x):\n        >>>   return x.sin().sum()\n        >>>\n        >>> x = torch.randn(5)\n        >>> hessian = jacfwd(jacrev(f))(x)\n        >>> assert torch.allclose(hessian, torch.diag(-x.sin()))\n\n    By default, :func:`jacfwd` computes the Jacobian with respect to the first\n    input. However, it can compute the Jacboian with respect to a different\n    argument by using ``argnums``:\n\n        >>> from torch.func import jacfwd\n        >>> def f(x, y):\n        >>>   return x + y ** 2\n        >>>\n        >>> x, y = torch.randn(5), torch.randn(5)\n        >>> jacobian = jacfwd(f, argnums=1)(x, y)\n        >>> expected = torch.diag(2 * y)\n        >>> assert torch.allclose(jacobian, expected)\n\n    Additionally, passing a tuple to ``argnums`` will compute the Jacobian\n    with respect to multiple arguments\n\n        >>> from torch.func import jacfwd\n        >>> def f(x, y):\n        >>>   return x + y ** 2\n        >>>\n        >>> x, y = torch.randn(5), torch.randn(5)\n        >>> jacobian = jacfwd(f, argnums=(0, 1))(x, y)\n        >>> expectedX = torch.diag(torch.ones_like(x))\n        >>> expectedY = torch.diag(2 * y)\n        >>> assert torch.allclose(jacobian[0], expectedX)\n        >>> assert torch.allclose(jacobian[1], expectedY)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.jacrev",
      "signature": "torch.func.jacrev(func: Callable, argnums: Union[int, tuple[int]] = 0, *, has_aux=False, chunk_size: Optional[int] = None, _preallocate_and_copy=False)",
      "doc": "\n    Computes the Jacobian of ``func`` with respect to the arg(s) at index\n    ``argnum`` using reverse mode autodiff\n\n    .. note::\n        Using :attr:`chunk_size=1` is equivalent to computing the jacobian\n        row-by-row with a for-loop i.e. the constraints of :func:`vmap` are\n        not applicable.\n\n    Args:\n        func (function): A Python function that takes one or more arguments,\n            one of which must be a Tensor, and returns one or more Tensors\n        argnums (int or Tuple[int]): Optional, integer or tuple of integers,\n            saying which arguments to get the Jacobian with respect to.\n            Default: 0.\n        has_aux (bool): Flag indicating that ``func`` returns a\n            ``(output, aux)`` tuple where the first element is the output of\n            the function to be differentiated and the second element is\n            auxiliary objects that will not be differentiated.\n            Default: False.\n        chunk_size (None or int): If None (default), use the maximum chunk size\n            (equivalent to doing a single vmap over vjp to compute the jacobian).\n            If 1, then compute the jacobian row-by-row with a for-loop.\n            If not None, then compute the jacobian :attr:`chunk_size` rows at a time\n            (equivalent to doing multiple vmap over vjp). If you run into memory issues computing\n            the jacobian, please try to specify a non-None chunk_size.\n\n    Returns:\n        Returns a function that takes in the same inputs as ``func`` and\n        returns the Jacobian of ``func`` with respect to the arg(s) at\n        ``argnums``. If ``has_aux is True``, then the returned function\n        instead returns a ``(jacobian, aux)`` tuple where ``jacobian``\n        is the Jacobian and ``aux`` is auxiliary objects returned by ``func``.\n\n    A basic usage with a pointwise, unary operation will give a diagonal array\n    as the Jacobian\n\n        >>> from torch.func import jacrev\n        >>> x = torch.randn(5)\n        >>> jacobian = jacrev(torch.sin)(x)\n        >>> expected = torch.diag(torch.cos(x))\n        >>> assert torch.allclose(jacobian, expected)\n\n    If you would like to compute the output of the function as well as the\n    jacobian of the function, use the ``has_aux`` flag to return the output\n    as an auxiliary object:\n\n        >>> from torch.func import jacrev\n        >>> x = torch.randn(5)\n        >>>\n        >>> def f(x):\n        >>>   return x.sin()\n        >>>\n        >>> def g(x):\n        >>>   result = f(x)\n        >>>   return result, result\n        >>>\n        >>> jacobian_f, f_x = jacrev(g, has_aux=True)(x)\n        >>> assert torch.allclose(f_x, f(x))\n\n    :func:`jacrev` can be composed with vmap to produce batched\n    Jacobians:\n\n        >>> from torch.func import jacrev, vmap\n        >>> x = torch.randn(64, 5)\n        >>> jacobian = vmap(jacrev(torch.sin))(x)\n        >>> assert jacobian.shape == (64, 5, 5)\n\n    Additionally, :func:`jacrev` can be composed with itself to produce\n    Hessians\n\n        >>> from torch.func import jacrev\n        >>> def f(x):\n        >>>   return x.sin().sum()\n        >>>\n        >>> x = torch.randn(5)\n        >>> hessian = jacrev(jacrev(f))(x)\n        >>> assert torch.allclose(hessian, torch.diag(-x.sin()))\n\n    By default, :func:`jacrev` computes the Jacobian with respect to the first\n    input. However, it can compute the Jacboian with respect to a different\n    argument by using ``argnums``:\n\n        >>> from torch.func import jacrev\n        >>> def f(x, y):\n        >>>   return x + y ** 2\n        >>>\n        >>> x, y = torch.randn(5), torch.randn(5)\n        >>> jacobian = jacrev(f, argnums=1)(x, y)\n        >>> expected = torch.diag(2 * y)\n        >>> assert torch.allclose(jacobian, expected)\n\n    Additionally, passing a tuple to ``argnums`` will compute the Jacobian\n    with respect to multiple arguments\n\n        >>> from torch.func import jacrev\n        >>> def f(x, y):\n        >>>   return x + y ** 2\n        >>>\n        >>> x, y = torch.randn(5), torch.randn(5)\n        >>> jacobian = jacrev(f, argnums=(0, 1))(x, y)\n        >>> expectedX = torch.diag(torch.ones_like(x))\n        >>> expectedY = torch.diag(2 * y)\n        >>> assert torch.allclose(jacobian[0], expectedX)\n        >>> assert torch.allclose(jacobian[1], expectedY)\n\n    .. note::\n        Using PyTorch ``torch.no_grad`` together with ``jacrev``.\n        Case 1: Using ``torch.no_grad`` inside a function:\n\n            >>> def f(x):\n            >>>     with torch.no_grad():\n            >>>         c = x ** 2\n            >>>     return x - c\n\n        In this case, ``jacrev(f)(x)`` will respect the inner ``torch.no_grad``.\n\n        Case 2: Using ``jacrev`` inside ``torch.no_grad`` context manager:\n\n            >>> with torch.no_grad():\n            >>>     jacrev(f)(x)\n\n        In this case, ``jacrev`` will respect the inner ``torch.no_grad``, but not the\n        outer one. This is because ``jacrev`` is a \"function transform\": its result\n        should not depend on the result of a context manager outside of ``f``.\n    ",
      "arguments": [
        "func",
        "argnums",
        "has_aux",
        "chunk_size",
        "_preallocate_and_copy"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Computes the Jacobian of ``func`` with respect to the arg(s) at index\n    ``argnum`` using reverse mode autodiff\n\n    .. note::\n        Using :attr:`chunk_size=1` is equivalent to computing the jacobian\n        row-by-row with a for-loop i.e. the constraints of :func:`vmap` are\n        not applicable.\n\n    Args:\n        func (function): A Python function that takes one or more arguments,\n            one of which must be a Tensor, and returns one or more Tensors\n        argnums (int or Tuple[int]): Optional, integer or tuple of integers,\n            saying which arguments to get the Jacobian with respect to.\n            Default: 0.\n        has_aux (bool): Flag indicating that ``func`` returns a\n            ``(output, aux)`` tuple where the first element is the output of\n            the function to be differentiated and the second element is\n            auxiliary objects that will not be differentiated.\n            Default: False.\n        chunk_size (None or int): If None (default), use the maximum chunk size\n            (equivalent to doing a single vmap over vjp to compute the jacobian).\n            If 1, then compute the jacobian row-by-row with a for-loop.\n            If not None, then compute the jacobian :attr:`chunk_size` rows at a time\n            (equivalent to doing multiple vmap over vjp). If you run into memory issues computing\n            the jacobian, please try to specify a non-None chunk_size.\n\n    Returns:\n        Returns a function that takes in the same inputs as ``func`` and\n        returns the Jacobian of ``func`` with respect to the arg(s) at\n        ``argnums``. If ``has_aux is True``, then the returned function\n        instead returns a ``(jacobian, aux)`` tuple where ``jacobian``\n        is the Jacobian and ``aux`` is auxiliary objects returned by ``func``.\n\n    A basic usage with a pointwise, unary operation will give a diagonal array\n    as the Jacobian\n\n        >>> from torch.func import jacrev\n        >>> x = torch.randn(5)\n        >>> jacobian = jacrev(torch.sin)(x)\n        >>> expected = torch.diag(torch.cos(x))\n        >>> assert torch.allclose(jacobian, expected)\n\n    If you would like to compute the output of the function as well as the\n    jacobian of the function, use the ``has_aux`` flag to return the output\n    as an auxiliary object:\n\n        >>> from torch.func import jacrev\n        >>> x = torch.randn(5)\n        >>>\n        >>> def f(x):\n        >>>   return x.sin()\n        >>>\n        >>> def g(x):\n        >>>   result = f(x)\n        >>>   return result, result\n        >>>\n        >>> jacobian_f, f_x = jacrev(g, has_aux=True)(x)\n        >>> assert torch.allclose(f_x, f(x))\n\n    :func:`jacrev` can be composed with vmap to produce batched\n    Jacobians:\n\n        >>> from torch.func import jacrev, vmap\n        >>> x = torch.randn(64, 5)\n        >>> jacobian = vmap(jacrev(torch.sin))(x)\n        >>> assert jacobian.shape == (64, 5, 5)\n\n    Additionally, :func:`jacrev` can be composed with itself to produce\n    Hessians\n\n        >>> from torch.func import jacrev\n        >>> def f(x):\n        >>>   return x.sin().sum()\n        >>>\n        >>> x = torch.randn(5)\n        >>> hessian = jacrev(jacrev(f))(x)\n        >>> assert torch.allclose(hessian, torch.diag(-x.sin()))\n\n    By default, :func:`jacrev` computes the Jacobian with respect to the first\n    input. However, it can compute the Jacboian with respect to a different\n    argument by using ``argnums``:\n\n        >>> from torch.func import jacrev\n        >>> def f(x, y):\n        >>>   return x + y ** 2\n        >>>\n        >>> x, y = torch.randn(5), torch.randn(5)\n        >>> jacobian = jacrev(f, argnums=1)(x, y)\n        >>> expected = torch.diag(2 * y)\n        >>> assert torch.allclose(jacobian, expected)\n\n    Additionally, passing a tuple to ``argnums`` will compute the Jacobian\n    with respect to multiple arguments\n\n        >>> from torch.func import jacrev\n        >>> def f(x, y):\n        >>>   return x + y ** 2\n        >>>\n        >>> x, y = torch.randn(5), torch.randn(5)\n        >>> jacobian = jacrev(f, argnums=(0, 1))(x, y)\n        >>> expectedX = torch.diag(torch.ones_like(x))\n        >>> expectedY = torch.diag(2 * y)\n        >>> assert torch.allclose(jacobian[0], expectedX)\n        >>> assert torch.allclose(jacobian[1], expectedY)\n\n    .. note::\n        Using PyTorch ``torch.no_grad`` together with ``jacrev``.\n        Case 1: Using ``torch.no_grad`` inside a function:\n\n            >>> def f(x):\n            >>>     with torch.no_grad():\n            >>>         c = x ** 2\n            >>>     return x - c\n\n        In this case, ``jacrev(f)(x)`` will respect the inner ``torch.no_grad``.\n\n        Case 2: Using ``jacrev`` inside ``torch.no_grad`` context manager:\n\n            >>> with torch.no_grad():\n            >>>     jacrev(f)(x)\n\n        In this case, ``jacrev`` will respect the inner ``torch.no_grad``, but not the\n        outer one. This is because ``jacrev`` is a \"function transform\": its result\n        should not depend on the result of a context manager outside of ``f``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.jvp",
      "signature": "torch.func.jvp(func: Callable, primals: Any, tangents: Any, *, strict: bool = False, has_aux: bool = False)",
      "doc": "\n    Standing for the Jacobian-vector product, returns a tuple containing\n    the output of `func(*primals)` and the \"Jacobian of ``func`` evaluated at\n    ``primals``\" times ``tangents``. This is also known as forward-mode autodiff.\n\n    Args:\n        func (function): A Python function that takes one or more arguments,\n            one of which must be a Tensor, and returns one or more Tensors\n        primals (Tensors): Positional arguments to ``func`` that must all be\n            Tensors. The returned function will also be computing the\n            derivative with respect to these arguments\n        tangents (Tensors): The \"vector\" for which Jacobian-vector-product is\n            computed. Must be the same structure and sizes as the inputs to\n            ``func``.\n        has_aux (bool): Flag indicating that ``func`` returns a\n            ``(output, aux)`` tuple where the first element is the output of\n            the function to be differentiated and the second element is\n            other auxiliary objects that will not be differentiated.\n            Default: False.\n\n    Returns:\n        Returns a ``(output, jvp_out)`` tuple containing the output of ``func``\n        evaluated at ``primals`` and the Jacobian-vector product.\n        If ``has_aux is True``, then instead returns a ``(output, jvp_out, aux)`` tuple.\n\n    .. note::\n        You may see this API error out with \"forward-mode AD not implemented\n        for operator X\". If so, please file a bug report and we will prioritize it.\n\n    jvp is useful when you wish to compute gradients of a function R^1 -> R^N\n\n        >>> from torch.func import jvp\n        >>> x = torch.randn([])\n        >>> f = lambda x: x * torch.tensor([1., 2., 3])\n        >>> value, grad = jvp(f, (x,), (torch.tensor(1.),))\n        >>> assert torch.allclose(value, f(x))\n        >>> assert torch.allclose(grad, torch.tensor([1., 2, 3]))\n\n    :func:`jvp` can support functions with multiple inputs by passing in the\n    tangents for each of the inputs\n\n         >>> from torch.func import jvp\n         >>> x = torch.randn(5)\n         >>> y = torch.randn(5)\n         >>> f = lambda x, y: (x * y)\n         >>> _, output = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))\n         >>> assert torch.allclose(output, x + y)\n\n    ",
      "arguments": [
        "func",
        "primals",
        "tangents",
        "strict",
        "has_aux"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Standing for the Jacobian-vector product, returns a tuple containing\n    the output of `func(*primals)` and the \"Jacobian of ``func`` evaluated at\n    ``primals``\" times ``tangents``. This is also known as forward-mode autodiff.\n\n    Args:\n        func (function): A Python function that takes one or more arguments,\n            one of which must be a Tensor, and returns one or more Tensors\n        primals (Tensors): Positional arguments to ``func`` that must all be\n            Tensors. The returned function will also be computing the\n            derivative with respect to these arguments\n        tangents (Tensors): The \"vector\" for which Jacobian-vector-product is\n            computed. Must be the same structure and sizes as the inputs to\n            ``func``.\n        has_aux (bool): Flag indicating that ``func`` returns a\n            ``(output, aux)`` tuple where the first element is the output of\n            the function to be differentiated and the second element is\n            other auxiliary objects that will not be differentiated.\n            Default: False.\n\n    Returns:\n        Returns a ``(output, jvp_out)`` tuple containing the output of ``func``\n        evaluated at ``primals`` and the Jacobian-vector product.\n        If ``has_aux is True``, then instead returns a ``(output, jvp_out, aux)`` tuple.\n\n    .. note::\n        You may see this API error out with \"forward-mode AD not implemented\n        for operator X\". If so, please file a bug report and we will prioritize it.\n\n    jvp is useful when you wish to compute gradients of a function R^1 -> R^N\n\n        >>> from torch.func import jvp\n        >>> x = torch.randn([])\n        >>> f = lambda x: x * torch.tensor([1., 2., 3])\n        >>> value, grad = jvp(f, (x,), (torch.tensor(1.),))\n        >>> assert torch.allclose(value, f(x))\n        >>> assert torch.allclose(grad, torch.tensor([1., 2, 3]))\n\n    :func:`jvp` can support functions with multiple inputs by passing in the\n    tangents for each of the inputs\n\n         >>> from torch.func import jvp\n         >>> x = torch.randn(5)\n         >>> y = torch.randn(5)\n         >>> f = lambda x, y: (x * y)\n         >>> _, output = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))\n         >>> assert torch.allclose(output, x + y)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.linearize",
      "signature": "torch.func.linearize(func: Callable, *primals) -> tuple[typing.Any, typing.Callable]",
      "doc": "\n    Returns the value of ``func`` at ``primals`` and linear approximation\n    at ``primals``.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments.\n        primals (Tensors): Positional arguments to ``func`` that must all be\n            Tensors. These are the values at which the function is linearly approximated.\n\n    Returns:\n        Returns a ``(output, jvp_fn)`` tuple containing the output of ``func``\n        applied to ``primals`` and a function that computes the jvp of\n        ``func`` evaluated at ``primals``.\n\n    linearize is useful if jvp is to be computed multiple times at ``primals``. However,\n    to achieve this, linearize saves intermediate computation and has higher memory requirements\n    than directly applying `jvp`. So, if all the ``tangents`` are known, it maybe more efficient\n    to compute vmap(jvp) instead of using linearize.\n\n    .. note::\n        linearize evaluates ``func`` twice. Please file an issue for an implementation\n        with a single evaluation.\n\n    Example::\n        >>> import torch\n        >>> from torch.func import linearize\n        >>> def fn(x):\n        ...     return x.sin()\n        ...\n        >>> output, jvp_fn = linearize(fn, torch.zeros(3, 3))\n        >>> jvp_fn(torch.ones(3, 3))\n        tensor([[1., 1., 1.],\n                [1., 1., 1.],\n                [1., 1., 1.]])\n        >>>\n\n    ",
      "arguments": [
        "func",
        "primals"
      ],
      "return_type": "tuple[typing.Any, typing.Callable]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns the value of ``func`` at ``primals`` and linear approximation\n    at ``primals``.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments.\n        primals (Tensors): Positional arguments to ``func`` that must all be\n            Tensors. These are the values at which the function is linearly approximated.\n\n    Returns:\n        Returns a ``(output, jvp_fn)`` tuple containing the output of ``func``\n        applied to ``primals`` and a function that computes the jvp of\n        ``func`` evaluated at ``primals``.\n\n    linearize is useful if jvp is to be computed multiple times at ``primals``. However,\n    to achieve this, linearize saves intermediate computation and has higher memory requirements\n    than directly applying `jvp`. So, if all the ``tangents`` are known, it maybe more efficient\n    to compute vmap(jvp) instead of using linearize.\n\n    .. note::\n        linearize evaluates ``func`` twice. Please file an issue for an implementation\n        with a single evaluation.\n\n    Example::\n        >>> import torch\n        >>> from torch.func import linearize\n        >>> def fn(x):\n        ...     return x.sin()\n        ...\n        >>> output, jvp_fn = linearize(fn, torch.zeros(3, 3))\n        >>> jvp_fn(torch.ones(3, 3))\n        tensor([[1., 1., 1.],\n                [1., 1., 1.],\n                [1., 1., 1.]])\n        >>>\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.replace_all_batch_norm_modules_",
      "signature": "torch.func.replace_all_batch_norm_modules_(root: torch.nn.modules.module.Module) -> torch.nn.modules.module.Module",
      "doc": "\n    In place updates :attr:`root` by setting the ``running_mean`` and ``running_var`` to be None and\n    setting track_running_stats to be False for any nn.BatchNorm module in :attr:`root`\n    ",
      "arguments": [
        "root"
      ],
      "return_type": "<class 'torch.nn.modules.module.Module'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    In place updates :attr:`root` by setting the ``running_mean`` and ``running_var`` to be None and\n    setting track_running_stats to be False for any nn.BatchNorm module in :attr:`root`\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.stack_module_state",
      "signature": "torch.func.stack_module_state(models: Union[collections.abc.Sequence[torch.nn.modules.module.Module], torch.nn.modules.container.ModuleList]) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]",
      "doc": "stack_module_state(models) -> params, buffers\n\n    Prepares a list of torch.nn.Modules for ensembling with :func:`vmap`.\n\n    Given a list of ``M`` ``nn.Modules`` of the same class, returns two dictionaries\n    that stack all of their parameters and buffers together, indexed by name.\n    The stacked parameters are optimizable (i.e. they are new leaf nodes in the\n    autograd history that are unrelated to the original parameters and can be\n    passed directly to an optimizer).\n\n    Here's an example of how to ensemble over a very simple model:\n\n    .. code-block:: python\n\n        num_models = 5\n        batch_size = 64\n        in_features, out_features = 3, 3\n        models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]\n        data = torch.randn(batch_size, 3)\n\n        def wrapper(params, buffers, data):\n            return torch.func.functional_call(models[0], (params, buffers), data)\n\n        params, buffers = stack_module_state(models)\n        output = vmap(wrapper, (0, 0, None))(params, buffers, data)\n\n        assert output.shape == (num_models, batch_size, out_features)\n\n    When there's submodules, this follows state dict naming conventions\n\n    .. code-block:: python\n\n        import torch.nn as nn\n        class Foo(nn.Module):\n            def __init__(self, in_features, out_features):\n                super().__init__()\n                hidden = 4\n                self.l1 = nn.Linear(in_features, hidden)\n                self.l2 = nn.Linear(hidden, out_features)\n\n            def forward(self, x):\n                return self.l2(self.l1(x))\n\n        num_models = 5\n        in_features, out_features = 3, 3\n        models = [Foo(in_features, out_features) for i in range(num_models)]\n        params, buffers = stack_module_state(models)\n        print(list(params.keys()))  # \"l1.weight\", \"l1.bias\", \"l2.weight\", \"l2.bias\"\n\n    .. warning::\n        All of the modules being stacked together must be the same (except for\n        the values of their parameters/buffers). For example, they should be in the\n        same mode (training vs eval).\n    ",
      "arguments": [
        "models"
      ],
      "return_type": "tuple[dict[str, typing.Any], dict[str, typing.Any]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "stack_module_state(models) -> params, buffers\n\n    Prepares a list of torch.nn.Modules for ensembling with :func:`vmap`.\n\n    Given a list of ``M`` ``nn.Modules`` of the same class, returns two dictionaries\n    that stack all of their parameters and buffers together, indexed by name.\n    The stacked parameters are optimizable (i.e. they are new leaf nodes in the\n    autograd history that are unrelated to the original parameters and can be\n    passed directly to an optimizer).\n\n    Here's an example of how to ensemble over a very simple model:\n\n    .. code-block:: python\n\n        num_models = 5\n        batch_size = 64\n        in_features, out_features = 3, 3\n        models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]\n        data = torch.randn(batch_size, 3)\n\n        def wrapper(params, buffers, data):\n            return torch.func.functional_call(models[0], (params, buffers), data)\n\n        params, buffers = stack_module_state(models)\n        output = vmap(wrapper, (0, 0, None))(params, buffers, data)\n\n        assert output.shape == (num_models, batch_size, out_features)\n\n    When there's submodules, this follows state dict naming conventions\n\n    .. code-block:: python\n\n        import torch.nn as nn\n        class Foo(nn.Module):\n            def __init__(self, in_features, out_features):\n                super().__init__()\n                hidden = 4\n                self.l1 = nn.Linear(in_features, hidden)\n                self.l2 = nn.Linear(hidden, out_features)\n\n            def forward(self, x):\n                return self.l2(self.l1(x))\n\n        num_models = 5\n        in_features, out_features = 3, 3\n        models = [Foo(in_features, out_features) for i in range(num_models)]\n        params, buffers = stack_module_state(models)\n        print(list(params.keys()))  # \"l1.weight\", \"l1.bias\", \"l2.weight\", \"l2.bias\"\n\n    .. warning::\n        All of the modules being stacked together must be the same (except for\n        the values of their parameters/buffers). For example, they should be in the\n        same mode (training vs eval).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.vjp",
      "signature": "torch.func.vjp(func: Callable, *primals, has_aux: bool = False)",
      "doc": "\n    Standing for the vector-Jacobian product, returns a tuple containing the\n    results of ``func`` applied to ``primals`` and a function that, when\n    given ``cotangents``, computes the reverse-mode Jacobian of ``func`` with\n    respect to ``primals`` times ``cotangents``.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments. Must\n            return one or more Tensors.\n        primals (Tensors): Positional arguments to ``func`` that must all be\n            Tensors. The returned function will also be computing the\n            derivative with respect to these arguments\n        has_aux (bool): Flag indicating that ``func`` returns a\n            ``(output, aux)`` tuple where the first element is the output of\n            the function to be differentiated and the second element is\n            other auxiliary objects that will not be differentiated.\n            Default: False.\n\n    Returns:\n        Returns a ``(output, vjp_fn)`` tuple containing the output of ``func``\n        applied to ``primals`` and a function that computes the vjp of\n        ``func`` with respect to all ``primals`` using the cotangents passed\n        to the returned function. If ``has_aux is True``, then instead returns a\n        ``(output, vjp_fn, aux)`` tuple.\n        The returned ``vjp_fn`` function will return a tuple of each VJP.\n\n    When used in simple cases, :func:`vjp` behaves the same as :func:`grad`\n\n        >>> x = torch.randn([5])\n        >>> f = lambda x: x.sin().sum()\n        >>> (_, vjpfunc) = torch.func.vjp(f, x)\n        >>> grad = vjpfunc(torch.tensor(1.))[0]\n        >>> assert torch.allclose(grad, torch.func.grad(f)(x))\n\n    However, :func:`vjp` can support functions with multiple outputs by\n    passing in the cotangents for each of the outputs\n\n        >>> x = torch.randn([5])\n        >>> f = lambda x: (x.sin(), x.cos())\n        >>> (_, vjpfunc) = torch.func.vjp(f, x)\n        >>> vjps = vjpfunc((torch.ones([5]), torch.ones([5])))\n        >>> assert torch.allclose(vjps[0], x.cos() + -x.sin())\n\n    :func:`vjp` can even support outputs being Python structs\n\n        >>> x = torch.randn([5])\n        >>> f = lambda x: {'first': x.sin(), 'second': x.cos()}\n        >>> (_, vjpfunc) = torch.func.vjp(f, x)\n        >>> cotangents = {'first': torch.ones([5]), 'second': torch.ones([5])}\n        >>> vjps = vjpfunc(cotangents)\n        >>> assert torch.allclose(vjps[0], x.cos() + -x.sin())\n\n    The function returned by :func:`vjp` will compute the partials with\n    respect to each of the ``primals``\n\n        >>> x, y = torch.randn([5, 4]), torch.randn([4, 5])\n        >>> (_, vjpfunc) = torch.func.vjp(torch.matmul, x, y)\n        >>> cotangents = torch.randn([5, 5])\n        >>> vjps = vjpfunc(cotangents)\n        >>> assert len(vjps) == 2\n        >>> assert torch.allclose(vjps[0], torch.matmul(cotangents, y.transpose(0, 1)))\n        >>> assert torch.allclose(vjps[1], torch.matmul(x.transpose(0, 1), cotangents))\n\n    ``primals`` are the positional arguments for ``f``. All kwargs use their\n    default value\n\n        >>> x = torch.randn([5])\n        >>> def f(x, scale=4.):\n        >>>   return x * scale\n        >>>\n        >>> (_, vjpfunc) = torch.func.vjp(f, x)\n        >>> vjps = vjpfunc(torch.ones_like(x))\n        >>> assert torch.allclose(vjps[0], torch.full(x.shape, 4.))\n\n    .. note::\n        Using PyTorch ``torch.no_grad`` together with ``vjp``.\n        Case 1: Using ``torch.no_grad`` inside a function:\n\n            >>> def f(x):\n            >>>     with torch.no_grad():\n            >>>         c = x ** 2\n            >>>     return x - c\n\n        In this case, ``vjp(f)(x)`` will respect the inner ``torch.no_grad``.\n\n        Case 2: Using ``vjp`` inside ``torch.no_grad`` context manager:\n\n            >>> # xdoctest: +SKIP(failing)\n            >>> with torch.no_grad():\n            >>>     vjp(f)(x)\n\n        In this case, ``vjp`` will respect the inner ``torch.no_grad``, but not the\n        outer one. This is because ``vjp`` is a \"function transform\": its result\n        should not depend on the result of a context manager outside of ``f``.\n    ",
      "arguments": [
        "func",
        "primals",
        "has_aux"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Standing for the vector-Jacobian product, returns a tuple containing the\n    results of ``func`` applied to ``primals`` and a function that, when\n    given ``cotangents``, computes the reverse-mode Jacobian of ``func`` with\n    respect to ``primals`` times ``cotangents``.\n\n    Args:\n        func (Callable): A Python function that takes one or more arguments. Must\n            return one or more Tensors.\n        primals (Tensors): Positional arguments to ``func`` that must all be\n            Tensors. The returned function will also be computing the\n            derivative with respect to these arguments\n        has_aux (bool): Flag indicating that ``func`` returns a\n            ``(output, aux)`` tuple where the first element is the output of\n            the function to be differentiated and the second element is\n            other auxiliary objects that will not be differentiated.\n            Default: False.\n\n    Returns:\n        Returns a ``(output, vjp_fn)`` tuple containing the output of ``func``\n        applied to ``primals`` and a function that computes the vjp of\n        ``func`` with respect to all ``primals`` using the cotangents passed\n        to the returned function. If ``has_aux is True``, then instead returns a\n        ``(output, vjp_fn, aux)`` tuple.\n        The returned ``vjp_fn`` function will return a tuple of each VJP.\n\n    When used in simple cases, :func:`vjp` behaves the same as :func:`grad`\n\n        >>> x = torch.randn([5])\n        >>> f = lambda x: x.sin().sum()\n        >>> (_, vjpfunc) = torch.func.vjp(f, x)\n        >>> grad = vjpfunc(torch.tensor(1.))[0]\n        >>> assert torch.allclose(grad, torch.func.grad(f)(x))\n\n    However, :func:`vjp` can support functions with multiple outputs by\n    passing in the cotangents for each of the outputs\n\n        >>> x = torch.randn([5])\n        >>> f = lambda x: (x.sin(), x.cos())\n        >>> (_, vjpfunc) = torch.func.vjp(f, x)\n        >>> vjps = vjpfunc((torch.ones([5]), torch.ones([5])))\n        >>> assert torch.allclose(vjps[0], x.cos() + -x.sin())\n\n    :func:`vjp` can even support outputs being Python structs\n\n        >>> x = torch.randn([5])\n        >>> f = lambda x: {'first': x.sin(), 'second': x.cos()}\n        >>> (_, vjpfunc) = torch.func.vjp(f, x)\n        >>> cotangents = {'first': torch.ones([5]), 'second': torch.ones([5])}\n        >>> vjps = vjpfunc(cotangents)\n        >>> assert torch.allclose(vjps[0], x.cos() + -x.sin())\n\n    The function returned by :func:`vjp` will compute the partials with\n    respect to each of the ``primals``\n\n        >>> x, y = torch.randn([5, 4]), torch.randn([4, 5])\n        >>> (_, vjpfunc) = torch.func.vjp(torch.matmul, x, y)\n        >>> cotangents = torch.randn([5, 5])\n        >>> vjps = vjpfunc(cotangents)\n        >>> assert len(vjps) == 2\n        >>> assert torch.allclose(vjps[0], torch.matmul(cotangents, y.transpose(0, 1)))\n        >>> assert torch.allclose(vjps[1], torch.matmul(x.transpose(0, 1), cotangents))\n\n    ``primals`` are the positional arguments for ``f``. All kwargs use their\n    default value\n\n        >>> x = torch.randn([5])\n        >>> def f(x, scale=4.):\n        >>>   return x * scale\n        >>>\n        >>> (_, vjpfunc) = torch.func.vjp(f, x)\n        >>> vjps = vjpfunc(torch.ones_like(x))\n        >>> assert torch.allclose(vjps[0], torch.full(x.shape, 4.))\n\n    .. note::\n        Using PyTorch ``torch.no_grad`` together with ``vjp``.\n        Case 1: Using ``torch.no_grad`` inside a function:\n\n            >>> def f(x):\n            >>>     with torch.no_grad():\n            >>>         c = x ** 2\n            >>>     return x - c\n\n        In this case, ``vjp(f)(x)`` will respect the inner ``torch.no_grad``.\n\n        Case 2: Using ``vjp`` inside ``torch.no_grad`` context manager:\n\n            >>> # xdoctest: +SKIP(failing)\n            >>> with torch.no_grad():\n            >>>     vjp(f)(x)\n\n        In this case, ``vjp`` will respect the inner ``torch.no_grad``, but not the\n        outer one. This is because ``vjp`` is a \"function transform\": its result\n        should not depend on the result of a context manager outside of ``f``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.func.vmap",
      "signature": "torch.func.vmap(func: Callable, in_dims: Union[int, tuple] = 0, out_dims: Union[int, tuple[int, ...]] = 0, randomness: str = 'error', *, chunk_size=None) -> Callable",
      "doc": "\n    vmap is the vectorizing map; ``vmap(func)`` returns a new function that\n    maps ``func`` over some dimension of the inputs. Semantically, vmap\n    pushes the map into PyTorch operations called by ``func``, effectively\n    vectorizing those operations.\n\n    vmap is useful for handling batch dimensions: one can write a function\n    ``func`` that runs on examples and then lift it to a function that can\n    take batches of examples with ``vmap(func)``. vmap can also be used to\n    compute batched gradients when composed with autograd.\n\n    .. note::\n        :func:`torch.vmap` is aliased to :func:`torch.func.vmap` for\n        convenience. Use whichever one you'd like.\n\n    Args:\n        func (function): A Python function that takes one or more arguments.\n            Must return one or more Tensors.\n        in_dims (int or nested structure): Specifies which dimension of the\n            inputs should be mapped over. ``in_dims`` should have a\n            structure like the inputs. If the ``in_dim`` for a particular\n            input is None, then that indicates there is no map dimension.\n            Default: 0.\n        out_dims (int or Tuple[int]): Specifies where the mapped dimension\n            should appear in the outputs. If ``out_dims`` is a Tuple, then\n            it should have one element per output. Default: 0.\n        randomness (str): Specifies whether the randomness in this\n            vmap should be the same or different across batches. If 'different',\n            the randomness for each batch will be different. If 'same', the\n            randomness will be the same across batches. If 'error', any calls to\n            random functions will error. Default: 'error'. WARNING: this flag\n            only applies to random PyTorch operations and does not apply to\n            Python's random module or numpy randomness.\n        chunk_size (None or int): If None (default), apply a single vmap over inputs.\n            If not None, then compute the vmap :attr:`chunk_size` samples at a time.\n            Note that :attr:`chunk_size=1` is equivalent to computing the vmap with a for-loop.\n            If you run into memory issues computing the vmap, please try a non-None chunk_size.\n\n    Returns:\n        Returns a new \"batched\" function. It takes the same inputs as\n        ``func``, except each input has an extra dimension at the index\n        specified by ``in_dims``. It takes returns the same outputs as\n        ``func``, except each output has an extra dimension at the index\n        specified by ``out_dims``.\n\n    .. warning:\n        :func:`vmap` works best with functional-style code. Please do not\n        perform any side-effects in ``func``, with the exception of\n        in-place PyTorch operations. Examples of side-effects include mutating\n        Python data structures and assigning values to variables not captured\n        in ``func``.\n\n    One example of using :func:`vmap` is to compute batched dot products. PyTorch\n    doesn't provide a batched ``torch.dot`` API; instead of unsuccessfully\n    rummaging through docs, use :func:`vmap` to construct a new function.\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.func.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n        >>> x, y = torch.randn(2, 5), torch.randn(2, 5)\n        >>> batched_dot(x, y)\n\n    :func:`vmap` can be helpful in hiding batch dimensions, leading to a simpler\n    model authoring experience.\n\n        >>> batch_size, feature_size = 3, 5\n        >>> weights = torch.randn(feature_size, requires_grad=True)\n        >>>\n        >>> def model(feature_vec):\n        >>>     # Very simple linear model with activation\n        >>>     return feature_vec.dot(weights).relu()\n        >>>\n        >>> examples = torch.randn(batch_size, feature_size)\n        >>> result = torch.vmap(model)(examples)\n\n    :func:`vmap` can also help vectorize computations that were previously difficult\n    or impossible to batch. One example is higher-order gradient computation.\n    The PyTorch autograd engine computes vjps (vector-Jacobian products).\n    Computing a full Jacobian matrix for some function f: R^N -> R^N usually\n    requires N calls to ``autograd.grad``, one per Jacobian row. Using :func:`vmap`,\n    we can vectorize the whole computation, computing the Jacobian in a single\n    call to ``autograd.grad``.\n\n        >>> # Setup\n        >>> N = 5\n        >>> f = lambda x: x ** 2\n        >>> x = torch.randn(N, requires_grad=True)\n        >>> y = f(x)\n        >>> I_N = torch.eye(N)\n        >>>\n        >>> # Sequential approach\n        >>> jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]\n        >>>                  for v in I_N.unbind()]\n        >>> jacobian = torch.stack(jacobian_rows)\n        >>>\n        >>> # vectorized gradient computation\n        >>> def get_vjp(v):\n        >>>     return torch.autograd.grad(y, x, v)\n        >>> jacobian = torch.vmap(get_vjp)(I_N)\n\n    :func:`vmap` can also be nested, producing an output with multiple batched dimensions\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.vmap(torch.vmap(torch.dot))  # [N1, N0, D], [N1, N0, D] -> [N1, N0]\n        >>> x, y = torch.randn(2, 3, 5), torch.randn(2, 3, 5)\n        >>> batched_dot(x, y) # tensor of size [2, 3]\n\n    If the inputs are not batched along the first dimension, ``in_dims`` specifies\n    the dimension that each inputs are batched along as\n\n        >>> torch.dot                            # [N], [N] -> []\n        >>> batched_dot = torch.vmap(torch.dot, in_dims=1)  # [N, D], [N, D] -> [D]\n        >>> x, y = torch.randn(2, 5), torch.randn(2, 5)\n        >>> batched_dot(x, y)   # output is [5] instead of [2] if batched along the 0th dimension\n\n    If there are multiple inputs each of which is batched along different dimensions,\n    ``in_dims`` must be a tuple with the batch dimension for each input as\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.vmap(torch.dot, in_dims=(0, None))  # [N, D], [D] -> [N]\n        >>> x, y = torch.randn(2, 5), torch.randn(5)\n        >>> batched_dot(x, y) # second arg doesn't have a batch dim because in_dim[1] was None\n\n    If the input is a Python struct, ``in_dims`` must be a tuple containing a struct\n    matching the shape of the input:\n\n        >>> f = lambda dict: torch.dot(dict['x'], dict['y'])\n        >>> x, y = torch.randn(2, 5), torch.randn(5)\n        >>> input = {'x': x, 'y': y}\n        >>> batched_dot = torch.vmap(f, in_dims=({'x': 0, 'y': None},))\n        >>> batched_dot(input)\n\n    By default, the output is batched along the first dimension. However, it can be batched\n    along any dimension by using ``out_dims``\n\n        >>> f = lambda x: x ** 2\n        >>> x = torch.randn(2, 5)\n        >>> batched_pow = torch.vmap(f, out_dims=1)\n        >>> batched_pow(x) # [5, 2]\n\n    For any function that uses kwargs, the returned function will not batch the kwargs but will\n    accept kwargs\n\n        >>> x = torch.randn([2, 5])\n        >>> def fn(x, scale=4.):\n        >>>   return x * scale\n        >>>\n        >>> batched_pow = torch.vmap(fn)\n        >>> assert torch.allclose(batched_pow(x), x * 4)\n        >>> batched_pow(x, scale=x) # scale is not batched, output has shape [2, 2, 5]\n\n    .. note::\n        vmap does not provide general autobatching or handle variable-length\n        sequences out of the box.\n    ",
      "arguments": [
        "func",
        "in_dims",
        "out_dims",
        "randomness",
        "chunk_size"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    vmap is the vectorizing map; ``vmap(func)`` returns a new function that\n    maps ``func`` over some dimension of the inputs. Semantically, vmap\n    pushes the map into PyTorch operations called by ``func``, effectively\n    vectorizing those operations.\n\n    vmap is useful for handling batch dimensions: one can write a function\n    ``func`` that runs on examples and then lift it to a function that can\n    take batches of examples with ``vmap(func)``. vmap can also be used to\n    compute batched gradients when composed with autograd.\n\n    .. note::\n        :func:`torch.vmap` is aliased to :func:`torch.func.vmap` for\n        convenience. Use whichever one you'd like.\n\n    Args:\n        func (function): A Python function that takes one or more arguments.\n            Must return one or more Tensors.\n        in_dims (int or nested structure): Specifies which dimension of the\n            inputs should be mapped over. ``in_dims`` should have a\n            structure like the inputs. If the ``in_dim`` for a particular\n            input is None, then that indicates there is no map dimension.\n            Default: 0.\n        out_dims (int or Tuple[int]): Specifies where the mapped dimension\n            should appear in the outputs. If ``out_dims`` is a Tuple, then\n            it should have one element per output. Default: 0.\n        randomness (str): Specifies whether the randomness in this\n            vmap should be the same or different across batches. If 'different',\n            the randomness for each batch will be different. If 'same', the\n            randomness will be the same across batches. If 'error', any calls to\n            random functions will error. Default: 'error'. WARNING: this flag\n            only applies to random PyTorch operations and does not apply to\n            Python's random module or numpy randomness.\n        chunk_size (None or int): If None (default), apply a single vmap over inputs.\n            If not None, then compute the vmap :attr:`chunk_size` samples at a time.\n            Note that :attr:`chunk_size=1` is equivalent to computing the vmap with a for-loop.\n            If you run into memory issues computing the vmap, please try a non-None chunk_size.\n\n    Returns:\n        Returns a new \"batched\" function. It takes the same inputs as\n        ``func``, except each input has an extra dimension at the index\n        specified by ``in_dims``. It takes returns the same outputs as\n        ``func``, except each output has an extra dimension at the index\n        specified by ``out_dims``.\n\n    .. warning:\n        :func:`vmap` works best with functional-style code. Please do not\n        perform any side-effects in ``func``, with the exception of\n        in-place PyTorch operations. Examples of side-effects include mutating\n        Python data structures and assigning values to variables not captured\n        in ``func``.\n\n    One example of using :func:`vmap` is to compute batched dot products. PyTorch\n    doesn't provide a batched ``torch.dot`` API; instead of unsuccessfully\n    rummaging through docs, use :func:`vmap` to construct a new function.\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.func.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n        >>> x, y = torch.randn(2, 5), torch.randn(2, 5)\n        >>> batched_dot(x, y)\n\n    :func:`vmap` can be helpful in hiding batch dimensions, leading to a simpler\n    model authoring experience.\n\n        >>> batch_size, feature_size = 3, 5\n        >>> weights = torch.randn(feature_size, requires_grad=True)\n        >>>\n        >>> def model(feature_vec):\n        >>>     # Very simple linear model with activation\n        >>>     return feature_vec.dot(weights).relu()\n        >>>\n        >>> examples = torch.randn(batch_size, feature_size)\n        >>> result = torch.vmap(model)(examples)\n\n    :func:`vmap` can also help vectorize computations that were previously difficult\n    or impossible to batch. One example is higher-order gradient computation.\n    The PyTorch autograd engine computes vjps (vector-Jacobian products).\n    Computing a full Jacobian matrix for some function f: R^N -> R^N usually\n    requires N calls to ``autograd.grad``, one per Jacobian row. Using :func:`vmap`,\n    we can vectorize the whole computation, computing the Jacobian in a single\n    call to ``autograd.grad``.\n\n        >>> # Setup\n        >>> N = 5\n        >>> f = lambda x: x ** 2\n        >>> x = torch.randn(N, requires_grad=True)\n        >>> y = f(x)\n        >>> I_N = torch.eye(N)\n        >>>\n        >>> # Sequential approach\n        >>> jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]\n        >>>                  for v in I_N.unbind()]\n        >>> jacobian = torch.stack(jacobian_rows)\n        >>>\n        >>> # vectorized gradient computation\n        >>> def get_vjp(v):\n        >>>     return torch.autograd.grad(y, x, v)\n        >>> jacobian = torch.vmap(get_vjp)(I_N)\n\n    :func:`vmap` can also be nested, producing an output with multiple batched dimensions\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.vmap(torch.vmap(torch.dot))  # [N1, N0, D], [N1, N0, D] -> [N1, N0]\n        >>> x, y = torch.randn(2, 3, 5), torch.randn(2, 3, 5)\n        >>> batched_dot(x, y) # tensor of size [2, 3]\n\n    If the inputs are not batched along the first dimension, ``in_dims`` specifies\n    the dimension that each inputs are batched along as\n\n        >>> torch.dot                            # [N], [N] -> []\n        >>> batched_dot = torch.vmap(torch.dot, in_dims=1)  # [N, D], [N, D] -> [D]\n        >>> x, y = torch.randn(2, 5), torch.randn(2, 5)\n        >>> batched_dot(x, y)   # output is [5] instead of [2] if batched along the 0th dimension\n\n    If there are multiple inputs each of which is batched along different dimensions,\n    ``in_dims`` must be a tuple with the batch dimension for each input as\n\n        >>> torch.dot                            # [D], [D] -> []\n        >>> batched_dot = torch.vmap(torch.dot, in_dims=(0, None))  # [N, D], [D] -> [N]\n        >>> x, y = torch.randn(2, 5), torch.randn(5)\n        >>> batched_dot(x, y) # second arg doesn't have a batch dim because in_dim[1] was None\n\n    If the input is a Python struct, ``in_dims`` must be a tuple containing a struct\n    matching the shape of the input:\n\n        >>> f = lambda dict: torch.dot(dict['x'], dict['y'])\n        >>> x, y = torch.randn(2, 5), torch.randn(5)\n        >>> input = {'x': x, 'y': y}\n        >>> batched_dot = torch.vmap(f, in_dims=({'x': 0, 'y': None},))\n        >>> batched_dot(input)\n\n    By default, the output is batched along the first dimension. However, it can be batched\n    along any dimension by using ``out_dims``\n\n        >>> f = lambda x: x ** 2\n        >>> x = torch.randn(2, 5)\n        >>> batched_pow = torch.vmap(f, out_dims=1)\n        >>> batched_pow(x) # [5, 2]\n\n    For any function that uses kwargs, the returned function will not batch the kwargs but will\n    accept kwargs\n\n        >>> x = torch.randn([2, 5])\n        >>> def fn(x, scale=4.):\n        >>>   return x * scale\n        >>>\n        >>> batched_pow = torch.vmap(fn)\n        >>> assert torch.allclose(batched_pow(x), x * 4)\n        >>> batched_pow(x, scale=x) # scale is not batched, output has shape [2, 2, 5]\n\n    .. note::\n        vmap does not provide general autobatching or handle variable-length\n        sequences out of the box.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "ASYNCHRONOUS": [
    {
      "function": "torch.futures.cast",
      "signature": "torch.futures.cast(typ, val)",
      "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
      "arguments": [
        "typ",
        "val"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.futures.collect_all",
      "signature": "torch.futures.collect_all(futures: 'list[Future]') -> 'Future[list[Future]]'",
      "doc": "\n    Collects the provided :class:`~torch.futures.Future` objects into a single\n    combined :class:`~torch.futures.Future` that is completed when all of the\n    sub-futures are completed.\n\n    Args:\n        futures (list): a list of :class:`~torch.futures.Future` objects.\n\n    Returns:\n        Returns a :class:`~torch.futures.Future` object to a list of the passed\n        in Futures.\n\n    Example::\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_FUTURES)\n        >>> fut0 = torch.futures.Future()\n        >>> fut1 = torch.futures.Future()\n        >>> fut = torch.futures.collect_all([fut0, fut1])\n        >>> fut0.set_result(0)\n        >>> fut1.set_result(1)\n        >>> fut_list = fut.wait()\n        >>> print(f\"fut0 result = {fut_list[0].wait()}\")\n        fut0 result = 0\n        >>> print(f\"fut1 result = {fut_list[1].wait()}\")\n        fut1 result = 1\n    ",
      "arguments": [
        "futures"
      ],
      "return_type": "Future[list[Future]]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Collects the provided :class:`~torch.futures.Future` objects into a single\n    combined :class:`~torch.futures.Future` that is completed when all of the\n    sub-futures are completed.\n\n    Args:\n        futures (list): a list of :class:`~torch.futures.Future` objects.\n\n    Returns:\n        Returns a :class:`~torch.futures.Future` object to a list of the passed\n        in Futures.\n\n    Example::\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_FUTURES)\n        >>> fut0 = torch.futures.Future()\n        >>> fut1 = torch.futures.Future()\n        >>> fut = torch.futures.collect_all([fut0, fut1])\n        >>> fut0.set_result(0)\n        >>> fut1.set_result(1)\n        >>> fut_list = fut.wait()\n        >>> print(f\"fut0 result = {fut_list[0].wait()}\")\n        fut0 result = 0\n        >>> print(f\"fut1 result = {fut_list[1].wait()}\")\n        fut1 result = 1\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.futures.wait_all",
      "signature": "torch.futures.wait_all(futures: 'list[Future]') -> 'list'",
      "doc": "\n    Waits for all provided futures to be complete, and returns\n    the list of completed values. If any of the futures encounters an error,\n    the method will exit early and report the error not waiting for other\n    futures to complete.\n\n    Args:\n        futures (list): a list of :class:`~torch.futures.Future` object.\n\n    Returns:\n        A list of the completed :class:`~torch.futures.Future` results. This\n        method will throw an error if ``wait`` on any\n        :class:`~torch.futures.Future` throws.\n    ",
      "arguments": [
        "futures"
      ],
      "return_type": "list",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Waits for all provided futures to be complete, and returns\n    the list of completed values. If any of the futures encounters an error,\n    the method will exit early and report the error not waiting for other\n    futures to complete.\n\n    Args:\n        futures (list): a list of :class:`~torch.futures.Future` object.\n\n    Returns:\n        A list of the completed :class:`~torch.futures.Future` results. This\n        method will throw an error if ``wait`` on any\n        :class:`~torch.futures.Future` throws.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "GRAPH_TRANSFORMATIONS": [
    {
      "function": "torch.fx.has_side_effect",
      "signature": "torch.fx.has_side_effect(fn: Callable[~_P, ~_R]) -> Callable[~_P, ~_R]",
      "doc": "\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.\n",
      "arguments": [
        "fn"
      ],
      "return_type": "typing.Callable[~_P, ~_R]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n.. warning::\n    This API is experimental and is *NOT* backward-compatible.\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.fx.map_arg",
      "signature": "torch.fx.map_arg(a: ~ArgumentT, fn: Callable[[torch.fx.node.Node], Union[tuple['Argument', ...], collections.abc.Sequence['Argument'], collections.abc.Mapping[str, 'Argument'], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]]) -> ~ArgumentT",
      "doc": "\nApply fn recursively to each Node appearing in arg.\n\narg may be a list, tuple, slice, or dict with string keys: the return value will\nhave the same type and structure.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.\n",
      "arguments": [
        "a",
        "fn"
      ],
      "return_type": "~ArgumentT",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\nApply fn recursively to each Node appearing in arg.\n\narg may be a list, tuple, slice, or dict with string keys: the return value will\nhave the same type and structure.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.fx.replace_pattern",
      "signature": "torch.fx.replace_pattern(gm: torch.fx.graph_module.GraphModule, pattern: Union[Callable, torch.fx.graph_module.GraphModule], replacement: Union[Callable, torch.fx.graph_module.GraphModule]) -> list[torch.fx.subgraph_rewriter.Match]",
      "doc": "\nMatches all possible non-overlapping sets of operators and their\ndata dependencies (``pattern``) in the Graph of a GraphModule\n(``gm``), then replaces each of these matched subgraphs with another\nsubgraph (``replacement``).\n\nArgs:\n    ``gm``: The GraphModule that wraps the Graph to operate on\n    ``pattern``: The subgraph to match in ``gm`` for replacement\n    ``replacement``: The subgraph to replace ``pattern`` with\n\nReturns:\n    List[Match]: A list of ``Match`` objects representing the places\n    in the original graph that ``pattern`` was matched to. The list\n    is empty if there are no matches. ``Match`` is defined as:\n\n    .. code-block:: python\n\n        class Match(NamedTuple):\n            # Node from which the match was found\n            anchor: Node\n            # Maps nodes in the pattern subgraph to nodes in the larger graph\n            nodes_map: Dict[Node, Node]\n\nExamples:\n\n.. code-block:: python\n\n    import torch\n    from torch.fx import symbolic_trace, subgraph_rewriter\n\n\n    class M(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n\n        def forward(self, x, w1, w2):\n            m1 = torch.cat([w1, w2]).sum()\n            m2 = torch.cat([w1, w2]).sum()\n            return x + torch.max(m1) + torch.max(m2)\n\n\n    def pattern(w1, w2):\n        return torch.cat([w1, w2])\n\n\n    def replacement(w1, w2):\n        return torch.stack([w1, w2])\n\n\n    traced_module = symbolic_trace(M())\n\n    subgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n\nThe above code will first match ``pattern`` in the ``forward``\nmethod of ``traced_module``. Pattern-matching is done based on\nuse-def relationships, not node names. For example, if you had\n``p = torch.cat([a, b])`` in ``pattern``, you could match\n``m = torch.cat([a, b])`` in the original ``forward`` function,\ndespite the variable names being different (``p`` vs ``m``).\n\nThe ``return`` statement in ``pattern`` is matched based on its\nvalue only; it may or may not match to the ``return`` statement in\nthe larger graph. In other words, the pattern doesn't have to extend\nto the end of the larger graph.\n\nWhen the pattern is matched, it will be removed from the larger\nfunction and replaced by ``replacement``. If there are multiple\nmatches for ``pattern`` in the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\"First\" here being defined as the first in a topological ordering\nof the Nodes' use-def relationships. In most cases, the first Node\nis the parameter that appears directly after ``self``, while the\nlast Node is whatever the function returns.)\n\nOne important thing to note is that the parameters of the\n``pattern`` Callable must be used in the Callable itself,\nand the parameters of the ``replacement`` Callable must match\nthe pattern. The first rule is why, in the above code block, the\n``forward`` function has parameters ``x, w1, w2``, but the\n``pattern`` function only has parameters ``w1, w2``. ``pattern``\ndoesn't use ``x``, so it shouldn't specify ``x`` as a parameter.\nAs an example of the second rule, consider replacing\n\n.. code-block:: python\n\n    def pattern(x, y):\n        return torch.neg(x) + torch.relu(y)\n\nwith\n\n.. code-block:: python\n\n    def replacement(x, y):\n        return torch.relu(x)\n\nIn this case, ``replacement`` needs the same number of parameters\nas ``pattern`` (both ``x`` and ``y``), even though the parameter\n``y`` isn't used in ``replacement``.\n\nAfter calling ``subgraph_rewriter.replace_pattern``, the generated\nPython code looks like this:\n\n.. code-block:: python\n\n    def forward(self, x, w1, w2):\n        stack_1 = torch.stack([w1, w2])\n        sum_1 = stack_1.sum()\n        stack_2 = torch.stack([w1, w2])\n        sum_2 = stack_2.sum()\n        max_1 = torch.max(sum_1)\n        add_1 = x + max_1\n        max_2 = torch.max(sum_2)\n        add_2 = add_1 + max_2\n        return add_2\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.\n",
      "arguments": [
        "gm",
        "pattern",
        "replacement"
      ],
      "return_type": "list[torch.fx.subgraph_rewriter.Match]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\nMatches all possible non-overlapping sets of operators and their\ndata dependencies (``pattern``) in the Graph of a GraphModule\n(``gm``), then replaces each of these matched subgraphs with another\nsubgraph (``replacement``).\n\nArgs:\n    ``gm``: The GraphModule that wraps the Graph to operate on\n    ``pattern``: The subgraph to match in ``gm`` for replacement\n    ``replacement``: The subgraph to replace ``pattern`` with\n\nReturns:\n    List[Match]: A list of ``Match`` objects representing the places\n    in the original graph that ``pattern`` was matched to. The list\n    is empty if there are no matches. ``Match`` is defined as:\n\n    .. code-block:: python\n\n        class Match(NamedTuple):\n            # Node from which the match was found\n            anchor: Node\n            # Maps nodes in the pattern subgraph to nodes in the larger graph\n            nodes_map: Dict[Node, Node]\n\nExamples:\n\n.. code-block:: python\n\n    import torch\n    from torch.fx import symbolic_trace, subgraph_rewriter\n\n\n    class M(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n\n        def forward(self, x, w1, w2):\n            m1 = torch.cat([w1, w2]).sum()\n            m2 = torch.cat([w1, w2]).sum()\n            return x + torch.max(m1) + torch.max(m2)\n\n\n    def pattern(w1, w2):\n        return torch.cat([w1, w2])\n\n\n    def replacement(w1, w2):\n        return torch.stack([w1, w2])\n\n\n    traced_module = symbolic_trace(M())\n\n    subgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n\nThe above code will first match ``pattern`` in the ``forward``\nmethod of ``traced_module``. Pattern-matching is done based on\nuse-def relationships, not node names. For example, if you had\n``p = torch.cat([a, b])`` in ``pattern``, you could match\n``m = torch.cat([a, b])`` in the original ``forward`` function,\ndespite the variable names being different (``p`` vs ``m``).\n\nThe ``return`` statement in ``pattern`` is matched based on its\nvalue only; it may or may not match to the ``return`` statement in\nthe larger graph. In other words, the pattern doesn't have to extend\nto the end of the larger graph.\n\nWhen the pattern is matched, it will be removed from the larger\nfunction and replaced by ``replacement``. If there are multiple\nmatches for ``pattern`` in the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\"First\" here being defined as the first in a topological ordering\nof the Nodes' use-def relationships. In most cases, the first Node\nis the parameter that appears directly after ``self``, while the\nlast Node is whatever the function returns.)\n\nOne important thing to note is that the parameters of the\n``pattern`` Callable must be used in the Callable itself,\nand the parameters of the ``replacement`` Callable must match\nthe pattern. The first rule is why, in the above code block, the\n``forward`` function has parameters ``x, w1, w2``, but the\n``pattern`` function only has parameters ``w1, w2``. ``pattern``\ndoesn't use ``x``, so it shouldn't specify ``x`` as a parameter.\nAs an example of the second rule, consider replacing\n\n.. code-block:: python\n\n    def pattern(x, y):\n        return torch.neg(x) + torch.relu(y)\n\nwith\n\n.. code-block:: python\n\n    def replacement(x, y):\n        return torch.relu(x)\n\nIn this case, ``replacement`` needs the same number of parameters\nas ``pattern`` (both ``x`` and ``y``), even though the parameter\n``y`` isn't used in ``replacement``.\n\nAfter calling ``subgraph_rewriter.replace_pattern``, the generated\nPython code looks like this:\n\n.. code-block:: python\n\n    def forward(self, x, w1, w2):\n        stack_1 = torch.stack([w1, w2])\n        sum_1 = stack_1.sum()\n        stack_2 = torch.stack([w1, w2])\n        sum_2 = stack_2.sum()\n        max_1 = torch.max(sum_1)\n        add_1 = x + max_1\n        max_2 = torch.max(sum_2)\n        add_2 = add_1 + max_2\n        return add_2\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.fx.symbolic_trace",
      "signature": "torch.fx.symbolic_trace(root: Union[torch.nn.modules.module.Module, Callable[..., Any]], concrete_args: Optional[dict[str, Any]] = None) -> torch.fx.graph_module.GraphModule",
      "doc": "\nSymbolic tracing API\n\nGiven an ``nn.Module`` or function instance ``root``, this function will return a ``GraphModule``\nconstructed by recording operations seen while tracing through ``root``.\n\n``concrete_args`` allows you to partially specialize your function, whether it's to remove control flow or data structures.\n\nFor example::\n\n    def f(a, b):\n        if b == True:\n            return a\n        else:\n            return a * 2\n\nFX can typically not trace through this due to the presence of control\nflow. However, we can use `concrete_args` to specialize on the value of\n`b` to trace through this::\n\n    f = fx.symbolic_trace(f, concrete_args={\"b\": False})\n    assert f(3, False) == 6\n\nNote that although you can still pass in different values of `b`, they will be ignored.\n\nWe can also use `concrete_args` to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in `fx.PH` for values that shouldn't be\nspecialized. For example::\n\n    def f(x):\n        out = 0\n        for v in x.values():\n            out += v\n        return out\n\n\n    f = fx.symbolic_trace(f, concrete_args={\"x\": {\"a\": fx.PH, \"b\": fx.PH, \"c\": fx.PH}})\n    assert f({\"a\": 1, \"b\": 2, \"c\": 4}) == 7\n\n\nArgs:\n    root (Union[torch.nn.Module, Callable]): Module or function to be traced and converted\n        into a Graph representation.\n    concrete_args (Optional[Dict[str, any]]): Inputs to be partially specialized\n\nReturns:\n    GraphModule: a Module created from the recorded operations from ``root``.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.\n",
      "arguments": [
        "root",
        "concrete_args"
      ],
      "return_type": "<class 'torch.fx.graph_module.GraphModule'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\nSymbolic tracing API\n\nGiven an ``nn.Module`` or function instance ``root``, this function will return a ``GraphModule``\nconstructed by recording operations seen while tracing through ``root``.\n\n``concrete_args`` allows you to partially specialize your function, whether it's to remove control flow or data structures.\n\nFor example::\n\n    def f(a, b):\n        if b == True:\n            return a\n        else:\n            return a * 2\n\nFX can typically not trace through this due to the presence of control\nflow. However, we can use `concrete_args` to specialize on the value of\n`b` to trace through this::\n\n    f = fx.symbolic_trace(f, concrete_args={\"b\": False})\n    assert f(3, False) == 6\n\nNote that although you can still pass in different values of `b`, they will be ignored.\n\nWe can also use `concrete_args` to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in `fx.PH` for values that shouldn't be\nspecialized. For example::\n\n    def f(x):\n        out = 0\n        for v in x.values():\n            out += v\n        return out\n\n\n    f = fx.symbolic_trace(f, concrete_args={\"x\": {\"a\": fx.PH, \"b\": fx.PH, \"c\": fx.PH}})\n    assert f({\"a\": 1, \"b\": 2, \"c\": 4}) == 7\n\n\nArgs:\n    root (Union[torch.nn.Module, Callable]): Module or function to be traced and converted\n        into a Graph representation.\n    concrete_args (Optional[Dict[str, any]]): Inputs to be partially specialized\n\nReturns:\n    GraphModule: a Module created from the recorded operations from ``root``.\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.fx.wrap",
      "signature": "torch.fx.wrap(fn_or_name: Union[str, Callable])",
      "doc": "\nThis function can be called at module-level scope to register fn_or_name as a \"leaf function\".\nA \"leaf function\" will be preserved as a CallFunction node in the FX trace instead of being\ntraced through::\n\n    # foo/bar/baz.py\n    def my_custom_function(x, y):\n        return x * x + y * y\n\n\n    torch.fx.wrap(\"my_custom_function\")\n\n\n    def fn_to_be_traced(x, y):\n        # When symbolic tracing, the below call to my_custom_function will be inserted into\n        # the graph rather than tracing it.\n        return my_custom_function(x, y)\n\nThis function can also equivalently be used as a decorator::\n\n    # foo/bar/baz.py\n    @torch.fx.wrap\n    def my_custom_function(x, y):\n        return x * x + y * y\n\nA wrapped function can be thought of a \"leaf function\", analogous to the concept of\n\"leaf modules\", that is, they are functions that are left as calls in the FX trace\nrather than traced through.\n\nArgs:\n\n    fn_or_name (Union[str, Callable]): The function or name of the global function to insert into the\n        graph when it's called\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.\n",
      "arguments": [
        "fn_or_name"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\nThis function can be called at module-level scope to register fn_or_name as a \"leaf function\".\nA \"leaf function\" will be preserved as a CallFunction node in the FX trace instead of being\ntraced through::\n\n    # foo/bar/baz.py\n    def my_custom_function(x, y):\n        return x * x + y * y\n\n\n    torch.fx.wrap(\"my_custom_function\")\n\n\n    def fn_to_be_traced(x, y):\n        # When symbolic tracing, the below call to my_custom_function will be inserted into\n        # the graph rather than tracing it.\n        return my_custom_function(x, y)\n\nThis function can also equivalently be used as a decorator::\n\n    # foo/bar/baz.py\n    @torch.fx.wrap\n    def my_custom_function(x, y):\n        return x * x + y * y\n\nA wrapped function can be thought of a \"leaf function\", analogous to the concept of\n\"leaf modules\", that is, they are functions that are left as calls in the FX trace\nrather than traced through.\n\nArgs:\n\n    fn_or_name (Union[str, Callable]): The function or name of the global function to insert into the\n        graph when it's called\n\n.. note::\n    Backwards-compatibility for this API is guaranteed.\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "JIT_COMPILATION": [
    {
      "function": "torch.jit.annotate",
      "signature": "torch.jit.annotate(the_type, the_value)",
      "doc": "Use to give type of `the_value` in TorchScript compiler.\n\n    This method is a pass-through function that returns `the_value`, used to hint TorchScript\n    compiler the type of `the_value`. It is a no-op when running outside of TorchScript.\n\n    Though TorchScript can infer correct type for most Python expressions, there are some cases where\n    type inference can be wrong, including:\n\n    - Empty containers like `[]` and `{}`, which TorchScript assumes to be container of `Tensor`\n    - Optional types like `Optional[T]` but assigned a valid value of type `T`, TorchScript would assume\n      it is type `T` rather than `Optional[T]`\n\n    Note that `annotate()` does not help in `__init__` method of `torch.nn.Module` subclasses because it\n    is executed in eager mode. To annotate types of `torch.nn.Module` attributes,\n    use :meth:`~torch.jit.Attribute` instead.\n\n    Example:\n\n    .. testcode::\n\n        import torch\n        from typing import Dict\n\n        @torch.jit.script\n        def fn():\n            # Telling TorchScript that this empty dictionary is a (str -> int) dictionary\n            # instead of default dictionary type of (str -> Tensor).\n            d = torch.jit.annotate(Dict[str, int], {})\n\n            # Without `torch.jit.annotate` above, following statement would fail because of\n            # type mismatch.\n            d[\"name\"] = 20\n\n    .. testcleanup::\n\n        del fn\n\n    Args:\n        the_type: Python type that should be passed to TorchScript compiler as type hint for `the_value`\n        the_value: Value or expression to hint type for.\n\n    Returns:\n        `the_value` is passed back as return value.\n    ",
      "arguments": [
        "the_type",
        "the_value"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Use to give type of `the_value` in TorchScript compiler.\n\n    This method is a pass-through function that returns `the_value`, used to hint TorchScript\n    compiler the type of `the_value`. It is a no-op when running outside of TorchScript.\n\n    Though TorchScript can infer correct type for most Python expressions, there are some cases where\n    type inference can be wrong, including:\n\n    - Empty containers like `[]` and `{}`, which TorchScript assumes to be container of `Tensor`\n    - Optional types like `Optional[T]` but assigned a valid value of type `T`, TorchScript would assume\n      it is type `T` rather than `Optional[T]`\n\n    Note that `annotate()` does not help in `__init__` method of `torch.nn.Module` subclasses because it\n    is executed in eager mode. To annotate types of `torch.nn.Module` attributes,\n    use :meth:`~torch.jit.Attribute` instead.\n\n    Example:\n\n    .. testcode::\n\n        import torch\n        from typing import Dict\n\n        @torch.jit.script\n        def fn():\n            # Telling TorchScript that this empty dictionary is a (str -> int) dictionary\n            # instead of default dictionary type of (str -> Tensor).\n            d = torch.jit.annotate(Dict[str, int], {})\n\n            # Without `torch.jit.annotate` above, following statement would fail because of\n            # type mismatch.\n            d[\"name\"] = 20\n\n    .. testcleanup::\n\n        del fn\n\n    Args:\n        the_type: Python type that should be passed to TorchScript compiler as type hint for `the_value`\n        the_value: Value or expression to hint type for.\n\n    Returns:\n        `the_value` is passed back as return value.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.contextmanager",
      "signature": "torch.jit.contextmanager(func)",
      "doc": "@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    ",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.enable_onednn_fusion",
      "signature": "torch.jit.enable_onednn_fusion(enabled: bool)",
      "doc": "Enable or disables onednn JIT fusion based on the parameter `enabled`.",
      "arguments": [
        "enabled"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Enable or disables onednn JIT fusion based on the parameter `enabled`.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.export",
      "signature": "torch.jit.export(fn)",
      "doc": "\n    This decorator indicates that a method on an ``nn.Module`` is used as an entry point into a\n    :class:`ScriptModule` and should be compiled.\n\n    ``forward`` implicitly is assumed to be an entry point, so it does not need this decorator.\n    Functions and methods called from ``forward`` are compiled as they are seen\n    by the compiler, so they do not need this decorator either.\n\n    Example (using ``@torch.jit.export`` on a method):\n\n    .. testcode::\n\n        import torch\n        import torch.nn as nn\n\n        class MyModule(nn.Module):\n            def implicitly_compiled_method(self, x):\n                return x + 99\n\n            # `forward` is implicitly decorated with `@torch.jit.export`,\n            # so adding it here would have no effect\n            def forward(self, x):\n                return x + 10\n\n            @torch.jit.export\n            def another_forward(self, x):\n                # When the compiler sees this call, it will compile\n                # `implicitly_compiled_method`\n                return self.implicitly_compiled_method(x)\n\n            def unused_method(self, x):\n                return x - 20\n\n        # `m` will contain compiled methods:\n        #     `forward`\n        #     `another_forward`\n        #     `implicitly_compiled_method`\n        # `unused_method` will not be compiled since it was not called from\n        # any compiled methods and wasn't decorated with `@torch.jit.export`\n        m = torch.jit.script(MyModule())\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This decorator indicates that a method on an ``nn.Module`` is used as an entry point into a\n    :class:`ScriptModule` and should be compiled.\n\n    ``forward`` implicitly is assumed to be an entry point, so it does not need this decorator.\n    Functions and methods called from ``forward`` are compiled as they are seen\n    by the compiler, so they do not need this decorator either.\n\n    Example (using ``@torch.jit.export`` on a method):\n\n    .. testcode::\n\n        import torch\n        import torch.nn as nn\n\n        class MyModule(nn.Module):\n            def implicitly_compiled_method(self, x):\n                return x + 99\n\n            # `forward` is implicitly decorated with `@torch.jit.export`,\n            # so adding it here would have no effect\n            def forward(self, x):\n                return x + 10\n\n            @torch.jit.export\n            def another_forward(self, x):\n                # When the compiler sees this call, it will compile\n                # `implicitly_compiled_method`\n                return self.implicitly_compiled_method(x)\n\n            def unused_method(self, x):\n                return x - 20\n\n        # `m` will contain compiled methods:\n        #     `forward`\n        #     `another_forward`\n        #     `implicitly_compiled_method`\n        # `unused_method` will not be compiled since it was not called from\n        # any compiled methods and wasn't decorated with `@torch.jit.export`\n        m = torch.jit.script(MyModule())\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.export_opnames",
      "signature": "torch.jit.export_opnames(m)",
      "doc": "\n    Generate new bytecode for a Script module.\n\n    Returns what the op list would be for a Script Module based off the current code base.\n\n    If you have a LiteScriptModule and want to get the currently present\n    list of ops call _export_operator_list instead.\n    ",
      "arguments": [
        "m"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Generate new bytecode for a Script module.\n\n    Returns what the op list would be for a Script Module based off the current code base.\n\n    If you have a LiteScriptModule and want to get the currently present\n    list of ops call _export_operator_list instead.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.fork",
      "signature": "torch.jit.fork(func, *args, **kwargs)",
      "doc": "\n    Create an asynchronous task executing `func` and a reference to the value of the result of this execution.\n\n    `fork` will return immediately, so the return value of `func` may not have been computed yet. To force completion\n    of the task and access the return value invoke `torch.jit.wait` on the Future. `fork` invoked\n    with a `func` which returns `T` is typed as `torch.jit.Future[T]`. `fork` calls can be arbitrarily\n    nested, and may be invoked with positional and keyword arguments.\n    Asynchronous execution will only occur when run in TorchScript. If run in pure python,\n    `fork` will not execute in parallel. `fork` will also not execute in parallel when invoked\n    while tracing, however the `fork` and `wait` calls will be captured in the exported IR Graph.\n\n    .. warning::\n        `fork` tasks will execute non-deterministically. We recommend only spawning\n        parallel fork tasks for pure functions that do not modify their inputs,\n        module attributes, or global state.\n\n    Args:\n        func (callable or torch.nn.Module):  A Python function or `torch.nn.Module`\n            that will be invoked. If executed in TorchScript, it will execute asynchronously,\n            otherwise it will not. Traced invocations of fork will be captured in the IR.\n        ``*args``, ``**kwargs``: arguments to invoke `func` with.\n    Returns:\n        `torch.jit.Future[T]`: a reference to the execution of `func`. The value `T`\n        can only be accessed by forcing completion of `func` through `torch.jit.wait`.\n\n    Example (fork a free function):\n\n    .. code-block:: python\n\n        import torch\n        from torch import Tensor\n\n\n        def foo(a: Tensor, b: int) -> Tensor:\n            return a + b\n\n\n        def bar(a):\n            fut: torch.jit.Future[Tensor] = torch.jit.fork(foo, a, b=2)\n            return torch.jit.wait(fut)\n\n\n        script_bar = torch.jit.script(bar)\n        input = torch.tensor(2)\n        # only the scripted version executes asynchronously\n        assert script_bar(input) == bar(input)\n        # trace is not run asynchronously, but fork is captured in IR\n        graph = torch.jit.trace(bar, (input,)).graph\n        assert \"fork\" in str(graph)\n\n    Example (fork a module method):\n\n    .. code-block:: python\n\n        import torch\n        from torch import Tensor\n\n\n        class AddMod(torch.nn.Module):\n            def forward(self, a: Tensor, b: int):\n                return a + b\n\n\n        class Mod(torch.nn.Module):\n            def __init__(self) -> None:\n                super(self).__init__()\n                self.mod = AddMod()\n\n            def forward(self, input):\n                fut = torch.jit.fork(self.mod, a, b=2)\n                return torch.jit.wait(fut)\n\n\n        input = torch.tensor(2)\n        mod = Mod()\n        assert mod(input) == torch.jit.script(mod).forward(input)\n    ",
      "arguments": [
        "func",
        "args",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Create an asynchronous task executing `func` and a reference to the value of the result of this execution.\n\n    `fork` will return immediately, so the return value of `func` may not have been computed yet. To force completion\n    of the task and access the return value invoke `torch.jit.wait` on the Future. `fork` invoked\n    with a `func` which returns `T` is typed as `torch.jit.Future[T]`. `fork` calls can be arbitrarily\n    nested, and may be invoked with positional and keyword arguments.\n    Asynchronous execution will only occur when run in TorchScript. If run in pure python,\n    `fork` will not execute in parallel. `fork` will also not execute in parallel when invoked\n    while tracing, however the `fork` and `wait` calls will be captured in the exported IR Graph.\n\n    .. warning::\n        `fork` tasks will execute non-deterministically. We recommend only spawning\n        parallel fork tasks for pure functions that do not modify their inputs,\n        module attributes, or global state.\n\n    Args:\n        func (callable or torch.nn.Module):  A Python function or `torch.nn.Module`\n            that will be invoked. If executed in TorchScript, it will execute asynchronously,\n            otherwise it will not. Traced invocations of fork will be captured in the IR.\n        ``*args``, ``**kwargs``: arguments to invoke `func` with.\n    Returns:\n        `torch.jit.Future[T]`: a reference to the execution of `func`. The value `T`\n        can only be accessed by forcing completion of `func` through `torch.jit.wait`.\n\n    Example (fork a free function):\n\n    .. code-block:: python\n\n        import torch\n        from torch import Tensor\n\n\n        def foo(a: Tensor, b: int) -> Tensor:\n            return a + b\n\n\n        def bar(a):\n            fut: torch.jit.Future[Tensor] = torch.jit.fork(foo, a, b=2)\n            return torch.jit.wait(fut)\n\n\n        script_bar = torch.jit.script(bar)\n        input = torch.tensor(2)\n        # only the scripted version executes asynchronously\n        assert script_bar(input) == bar(input)\n        # trace is not run asynchronously, but fork is captured in IR\n        graph = torch.jit.trace(bar, (input,)).graph\n        assert \"fork\" in str(graph)\n\n    Example (fork a module method):\n\n    .. code-block:: python\n\n        import torch\n        from torch import Tensor\n\n\n        class AddMod(torch.nn.Module):\n            def forward(self, a: Tensor, b: int):\n                return a + b\n\n\n        class Mod(torch.nn.Module):\n            def __init__(self) -> None:\n                super(self).__init__()\n                self.mod = AddMod()\n\n            def forward(self, input):\n                fut = torch.jit.fork(self.mod, a, b=2)\n                return torch.jit.wait(fut)\n\n\n        input = torch.tensor(2)\n        mod = Mod()\n        assert mod(input) == torch.jit.script(mod).forward(input)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.freeze",
      "signature": "torch.jit.freeze(mod, preserved_attrs: Optional[list[str]] = None, optimize_numerics: bool = True)",
      "doc": "Freeze ScriptModule, inline submodules, and attributes as constants.\n\n    Freezing a :class:`ScriptModule` will clone it and attempt to inline the cloned\n    module's submodules, parameters, and attributes as constants in the TorchScript IR Graph.\n    By default, `forward` will be preserved, as well as attributes & methods specified in\n    `preserved_attrs`. Additionally, any attribute that is modified within a preserved\n    method will be preserved.\n\n    Freezing currently only accepts ScriptModules that are in eval mode.\n\n    Freezing applies generic optimization that will speed up your model regardless of machine.\n    To further optimize using server-specific settings, run `optimize_for_inference` after\n    freezing.\n\n    Args:\n        mod (:class:`ScriptModule`): a module to be frozen\n        preserved_attrs (Optional[List[str]]): a list of attributes to preserve in addition to the forward method.\n            Attributes modified in preserved methods will also be preserved.\n        optimize_numerics (bool): If ``True``, a set of optimization passes will be run that does not strictly\n            preserve numerics. Full details of optimization can be found at `torch.jit.run_frozen_optimizations`.\n\n    Returns:\n        Frozen :class:`ScriptModule`.\n\n    Example (Freezing a simple module with a Parameter):\n\n    .. testcode::\n        import torch\n        class MyModule(torch.nn.Module):\n            def __init__(self, N, M):\n                super().__init__()\n                self.weight = torch.nn.Parameter(torch.rand(N, M))\n                self.linear = torch.nn.Linear(N, M)\n\n            def forward(self, input):\n                output = self.weight.mm(input)\n                output = self.linear(output)\n                return output\n\n        scripted_module = torch.jit.script(MyModule(2, 3).eval())\n        frozen_module = torch.jit.freeze(scripted_module)\n        # parameters have been removed and inlined into the Graph as constants\n        assert len(list(frozen_module.named_parameters())) == 0\n        # See the compiled graph as Python code\n        print(frozen_module.code)\n\n    Example (Freezing a module with preserved attributes)\n\n    .. testcode::\n        import torch\n        class MyModule2(torch.nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.modified_tensor = torch.tensor(10.)\n                self.version = 1\n\n            def forward(self, input):\n                self.modified_tensor += 1\n                return input + self.modified_tensor\n\n        scripted_module = torch.jit.script(MyModule2().eval())\n        frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"version\"])\n        # we've manually preserved `version`, so it still exists on the frozen module and can be modified\n        assert frozen_module.version == 1\n        frozen_module.version = 2\n        # `modified_tensor` is detected as being mutated in the forward, so freezing preserves\n        # it to retain model semantics\n        assert frozen_module(torch.tensor(1)) == torch.tensor(12)\n        # now that we've run it once, the next result will be incremented by one\n        assert frozen_module(torch.tensor(1)) == torch.tensor(13)\n\n    Note:\n        Freezing submodule attributes is also supported:\n        frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"submodule.version\"])\n\n    Note:\n        If you're not sure why an attribute is not being inlined as a constant, you can run\n        `dump_alias_db` on frozen_module.forward.graph to see if freezing has detected the\n        attribute is being modified.\n\n    Note:\n        Because freezing makes weights constants and removes module hierarchy, `to` and other\n        nn.Module methods to manipulate device or dtype no longer work. As a workaround,\n        You can remap devices by specifying `map_location` in `torch.jit.load`, however\n        device-specific logic may have been baked into the model.\n    ",
      "arguments": [
        "mod",
        "preserved_attrs",
        "optimize_numerics"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Freeze ScriptModule, inline submodules, and attributes as constants.\n\n    Freezing a :class:`ScriptModule` will clone it and attempt to inline the cloned\n    module's submodules, parameters, and attributes as constants in the TorchScript IR Graph.\n    By default, `forward` will be preserved, as well as attributes & methods specified in\n    `preserved_attrs`. Additionally, any attribute that is modified within a preserved\n    method will be preserved.\n\n    Freezing currently only accepts ScriptModules that are in eval mode.\n\n    Freezing applies generic optimization that will speed up your model regardless of machine.\n    To further optimize using server-specific settings, run `optimize_for_inference` after\n    freezing.\n\n    Args:\n        mod (:class:`ScriptModule`): a module to be frozen\n        preserved_attrs (Optional[List[str]]): a list of attributes to preserve in addition to the forward method.\n            Attributes modified in preserved methods will also be preserved.\n        optimize_numerics (bool): If ``True``, a set of optimization passes will be run that does not strictly\n            preserve numerics. Full details of optimization can be found at `torch.jit.run_frozen_optimizations`.\n\n    Returns:\n        Frozen :class:`ScriptModule`.\n\n    Example (Freezing a simple module with a Parameter):\n\n    .. testcode::\n        import torch\n        class MyModule(torch.nn.Module):\n            def __init__(self, N, M):\n                super().__init__()\n                self.weight = torch.nn.Parameter(torch.rand(N, M))\n                self.linear = torch.nn.Linear(N, M)\n\n            def forward(self, input):\n                output = self.weight.mm(input)\n                output = self.linear(output)\n                return output\n\n        scripted_module = torch.jit.script(MyModule(2, 3).eval())\n        frozen_module = torch.jit.freeze(scripted_module)\n        # parameters have been removed and inlined into the Graph as constants\n        assert len(list(frozen_module.named_parameters())) == 0\n        # See the compiled graph as Python code\n        print(frozen_module.code)\n\n    Example (Freezing a module with preserved attributes)\n\n    .. testcode::\n        import torch\n        class MyModule2(torch.nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.modified_tensor = torch.tensor(10.)\n                self.version = 1\n\n            def forward(self, input):\n                self.modified_tensor += 1\n                return input + self.modified_tensor\n\n        scripted_module = torch.jit.script(MyModule2().eval())\n        frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"version\"])\n        # we've manually preserved `version`, so it still exists on the frozen module and can be modified\n        assert frozen_module.version == 1\n        frozen_module.version = 2\n        # `modified_tensor` is detected as being mutated in the forward, so freezing preserves\n        # it to retain model semantics\n        assert frozen_module(torch.tensor(1)) == torch.tensor(12)\n        # now that we've run it once, the next result will be incremented by one\n        assert frozen_module(torch.tensor(1)) == torch.tensor(13)\n\n    Note:\n        Freezing submodule attributes is also supported:\n        frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"submodule.version\"])\n\n    Note:\n        If you're not sure why an attribute is not being inlined as a constant, you can run\n        `dump_alias_db` on frozen_module.forward.graph to see if freezing has detected the\n        attribute is being modified.\n\n    Note:\n        Because freezing makes weights constants and removes module hierarchy, `to` and other\n        nn.Module methods to manipulate device or dtype no longer work. As a workaround,\n        You can remap devices by specifying `map_location` in `torch.jit.load`, however\n        device-specific logic may have been baked into the model.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.fuser",
      "signature": "torch.jit.fuser(name)",
      "doc": "Context manager that facilitates switching between backend fusers.\n\n    Valid names:\n    * ``fuser0`` - enables only legacy fuser\n    * ``fuser1`` - enables only NNC\n    * ``fuser2`` - enables only nvFuser\n    * ``fuser3`` - enables oneDNN Graph\n    ",
      "arguments": [
        "name"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Context manager that facilitates switching between backend fusers.\n\n    Valid names:\n    * ``fuser0`` - enables only legacy fuser\n    * ``fuser1`` - enables only NNC\n    * ``fuser2`` - enables only nvFuser\n    * ``fuser3`` - enables oneDNN Graph\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.ignore",
      "signature": "torch.jit.ignore(drop=False, **kwargs)",
      "doc": "\n    This decorator indicates to the compiler that a function or method should\n    be ignored and left as a Python function. This allows you to leave code in\n    your model that is not yet TorchScript compatible. If called from TorchScript,\n    ignored functions will dispatch the call to the Python interpreter. Models with ignored\n    functions cannot be exported; use :func:`@torch.jit.unused <torch.jit.unused>` instead.\n\n    Example (using ``@torch.jit.ignore`` on a method)::\n\n        import torch\n        import torch.nn as nn\n\n\n        class MyModule(nn.Module):\n            @torch.jit.ignore\n            def debugger(self, x):\n                import pdb\n\n                pdb.set_trace()\n\n            def forward(self, x):\n                x += 10\n                # The compiler would normally try to compile `debugger`,\n                # but since it is `@ignore`d, it will be left as a call\n                # to Python\n                self.debugger(x)\n                return x\n\n\n        m = torch.jit.script(MyModule())\n\n        # Error! The call `debugger` cannot be saved since it calls into Python\n        m.save(\"m.pt\")\n\n    Example (using ``@torch.jit.ignore(drop=True)`` on a method):\n\n    .. testcode::\n\n        import torch\n        import torch.nn as nn\n\n        class MyModule(nn.Module):\n            @torch.jit.ignore(drop=True)\n            def training_method(self, x):\n                import pdb\n                pdb.set_trace()\n\n            def forward(self, x):\n                if self.training:\n                    self.training_method(x)\n                return x\n\n        m = torch.jit.script(MyModule())\n\n        # This is OK since `training_method` is not saved, the call is replaced\n        # with a `raise`.\n        m.save(\"m.pt\")\n\n    .. testcleanup::\n\n        import os\n        os.remove('m.pt')\n    ",
      "arguments": [
        "drop",
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This decorator indicates to the compiler that a function or method should\n    be ignored and left as a Python function. This allows you to leave code in\n    your model that is not yet TorchScript compatible. If called from TorchScript,\n    ignored functions will dispatch the call to the Python interpreter. Models with ignored\n    functions cannot be exported; use :func:`@torch.jit.unused <torch.jit.unused>` instead.\n\n    Example (using ``@torch.jit.ignore`` on a method)::\n\n        import torch\n        import torch.nn as nn\n\n\n        class MyModule(nn.Module):\n            @torch.jit.ignore\n            def debugger(self, x):\n                import pdb\n\n                pdb.set_trace()\n\n            def forward(self, x):\n                x += 10\n                # The compiler would normally try to compile `debugger`,\n                # but since it is `@ignore`d, it will be left as a call\n                # to Python\n                self.debugger(x)\n                return x\n\n\n        m = torch.jit.script(MyModule())\n\n        # Error! The call `debugger` cannot be saved since it calls into Python\n        m.save(\"m.pt\")\n\n    Example (using ``@torch.jit.ignore(drop=True)`` on a method):\n\n    .. testcode::\n\n        import torch\n        import torch.nn as nn\n\n        class MyModule(nn.Module):\n            @torch.jit.ignore(drop=True)\n            def training_method(self, x):\n                import pdb\n                pdb.set_trace()\n\n            def forward(self, x):\n                if self.training:\n                    self.training_method(x)\n                return x\n\n        m = torch.jit.script(MyModule())\n\n        # This is OK since `training_method` is not saved, the call is replaced\n        # with a `raise`.\n        m.save(\"m.pt\")\n\n    .. testcleanup::\n\n        import os\n        os.remove('m.pt')\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.interface",
      "signature": "torch.jit.interface(obj)",
      "doc": "Decorate to annotate classes or modules of different types.\n\n    This decorator can be used to define an interface that can be used to annotate\n    classes or modules of different types. This can be used for to annotate a submodule\n    or attribute class that could have different types that implement the same\n    interface, or which could be swapped at runtime; or to store a list of modules or\n    classes of varying types.\n\n    It is sometimes used to implement \"Callables\" - functions or modules that implement\n    an interface but whose implementations differ and which can be swapped out.\n\n    Example:\n    .. testcode::\n\n        import torch\n        from typing import List\n\n        @torch.jit.interface\n        class InterfaceType:\n            def run(self, x: torch.Tensor) -> torch.Tensor:\n                pass\n\n        # implements InterfaceType\n        @torch.jit.script\n        class Impl1:\n            def run(self, x: torch.Tensor) -> torch.Tensor:\n                return x.relu()\n\n        class Impl2(torch.nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.val = torch.rand(())\n\n            @torch.jit.export\n            def run(self, x: torch.Tensor) -> torch.Tensor:\n                return x + self.val\n\n        def user_fn(impls: List[InterfaceType], idx: int, val: torch.Tensor) -> torch.Tensor:\n            return impls[idx].run(val)\n\n        user_fn_jit = torch.jit.script(user_fn)\n\n        impls = [Impl1(), torch.jit.script(Impl2())]\n        val = torch.rand(4, 4)\n        user_fn_jit(impls, 0, val)\n        user_fn_jit(impls, 1, val)\n    ",
      "arguments": [
        "obj"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Decorate to annotate classes or modules of different types.\n\n    This decorator can be used to define an interface that can be used to annotate\n    classes or modules of different types. This can be used for to annotate a submodule\n    or attribute class that could have different types that implement the same\n    interface, or which could be swapped at runtime; or to store a list of modules or\n    classes of varying types.\n\n    It is sometimes used to implement \"Callables\" - functions or modules that implement\n    an interface but whose implementations differ and which can be swapped out.\n\n    Example:\n    .. testcode::\n\n        import torch\n        from typing import List\n\n        @torch.jit.interface\n        class InterfaceType:\n            def run(self, x: torch.Tensor) -> torch.Tensor:\n                pass\n\n        # implements InterfaceType\n        @torch.jit.script\n        class Impl1:\n            def run(self, x: torch.Tensor) -> torch.Tensor:\n                return x.relu()\n\n        class Impl2(torch.nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.val = torch.rand(())\n\n            @torch.jit.export\n            def run(self, x: torch.Tensor) -> torch.Tensor:\n                return x + self.val\n\n        def user_fn(impls: List[InterfaceType], idx: int, val: torch.Tensor) -> torch.Tensor:\n            return impls[idx].run(val)\n\n        user_fn_jit = torch.jit.script(user_fn)\n\n        impls = [Impl1(), torch.jit.script(Impl2())]\n        val = torch.rand(4, 4)\n        user_fn_jit(impls, 0, val)\n        user_fn_jit(impls, 1, val)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.is_scripting",
      "signature": "torch.jit.is_scripting() -> bool",
      "doc": "\n    Function that returns True when in compilation and False otherwise. This\n    is useful especially with the @unused decorator to leave code in your\n    model that is not yet TorchScript compatible.\n    .. testcode::\n\n        import torch\n\n        @torch.jit.unused\n        def unsupported_linear_op(x):\n            return x\n\n        def linear(x):\n            if torch.jit.is_scripting():\n                return torch.linear(x)\n            else:\n                return unsupported_linear_op(x)\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Function that returns True when in compilation and False otherwise. This\n    is useful especially with the @unused decorator to leave code in your\n    model that is not yet TorchScript compatible.\n    .. testcode::\n\n        import torch\n\n        @torch.jit.unused\n        def unsupported_linear_op(x):\n            return x\n\n        def linear(x):\n            if torch.jit.is_scripting():\n                return torch.linear(x)\n            else:\n                return unsupported_linear_op(x)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.is_tracing",
      "signature": "torch.jit.is_tracing()",
      "doc": "Return a boolean value.\n\n    Returns ``True`` in tracing (if a function is called during the\n    tracing of code with ``torch.jit.trace``) and ``False`` otherwise.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a boolean value.\n\n    Returns ``True`` in tracing (if a function is called during the\n    tracing of code with ``torch.jit.trace``) and ``False`` otherwise.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.isinstance",
      "signature": "torch.jit.isinstance(obj, target_type)",
      "doc": "\n    Provide container type refinement in TorchScript.\n\n    It can refine parameterized containers of the List, Dict, Tuple, and Optional types. E.g. ``List[str]``,\n    ``Dict[str, List[torch.Tensor]]``, ``Optional[Tuple[int,str,int]]``. It can also\n    refine basic types such as bools and ints that are available in TorchScript.\n\n    Args:\n        obj: object to refine the type of\n        target_type: type to try to refine obj to\n    Returns:\n        ``bool``: True if obj was successfully refined to the type of target_type,\n            False otherwise with no new type refinement\n\n\n    Example (using ``torch.jit.isinstance`` for type refinement):\n    .. testcode::\n\n        import torch\n        from typing import Any, Dict, List\n\n        class MyModule(torch.nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n\n            def forward(self, input: Any): # note the Any type\n                if torch.jit.isinstance(input, List[torch.Tensor]):\n                    for t in input:\n                        y = t.clamp(0, 0.5)\n                elif torch.jit.isinstance(input, Dict[str, str]):\n                    for val in input.values():\n                        print(val)\n\n        m = torch.jit.script(MyModule())\n        x = [torch.rand(3,3), torch.rand(4,3)]\n        m(x)\n        y = {\"key1\":\"val1\",\"key2\":\"val2\"}\n        m(y)\n    ",
      "arguments": [
        "obj",
        "target_type"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Provide container type refinement in TorchScript.\n\n    It can refine parameterized containers of the List, Dict, Tuple, and Optional types. E.g. ``List[str]``,\n    ``Dict[str, List[torch.Tensor]]``, ``Optional[Tuple[int,str,int]]``. It can also\n    refine basic types such as bools and ints that are available in TorchScript.\n\n    Args:\n        obj: object to refine the type of\n        target_type: type to try to refine obj to\n    Returns:\n        ``bool``: True if obj was successfully refined to the type of target_type,\n            False otherwise with no new type refinement\n\n\n    Example (using ``torch.jit.isinstance`` for type refinement):\n    .. testcode::\n\n        import torch\n        from typing import Any, Dict, List\n\n        class MyModule(torch.nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n\n            def forward(self, input: Any): # note the Any type\n                if torch.jit.isinstance(input, List[torch.Tensor]):\n                    for t in input:\n                        y = t.clamp(0, 0.5)\n                elif torch.jit.isinstance(input, Dict[str, str]):\n                    for val in input.values():\n                        print(val)\n\n        m = torch.jit.script(MyModule())\n        x = [torch.rand(3,3), torch.rand(4,3)]\n        m(x)\n        y = {\"key1\":\"val1\",\"key2\":\"val2\"}\n        m(y)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.jit_module_from_flatbuffer",
      "signature": "torch.jit.jit_module_from_flatbuffer(f)",
      "doc": "",
      "arguments": [
        "f"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.load",
      "signature": "torch.jit.load(f, map_location=None, _extra_files=None, _restore_shapes=False)",
      "doc": "\n    Load a :class:`ScriptModule` or :class:`ScriptFunction` previously saved with :func:`torch.jit.save <torch.jit.save>`.\n\n    All previously saved modules, no matter their device, are first loaded onto CPU,\n    and then are moved to the devices they were saved from. If this fails (e.g.\n    because the run time system doesn't have certain devices), an exception is\n    raised.\n\n    Args:\n        f: a file-like object (has to implement read, readline, tell, and seek),\n            or a string containing a file name\n        map_location (string or torch.device): A simplified version of\n            ``map_location`` in `torch.jit.save` used to dynamically remap\n            storages to an alternative set of devices.\n        _extra_files (dictionary of filename to content): The extra\n            filenames given in the map would be loaded and their content\n            would be stored in the provided map.\n        _restore_shapes (bool): Whether or not to retrace the module on load using stored inputs\n\n    Returns:\n        A :class:`ScriptModule` object.\n\n    .. warning::\n        It is possible to construct malicious pickle data which will execute arbitrary code\n        during func:`torch.jit.load`. Never load data that could have come from an untrusted\n        source, or that could have been tampered with. **Only load data you trust**.\n\n    Example:\n    .. testcode::\n\n        import torch\n        import io\n\n        torch.jit.load('scriptmodule.pt')\n\n        # Load ScriptModule from io.BytesIO object\n        with open('scriptmodule.pt', 'rb') as f:\n            buffer = io.BytesIO(f.read())\n\n        # Load all tensors to the original device\n        torch.jit.load(buffer)\n\n        # Load all tensors onto CPU, using a device\n        buffer.seek(0)\n        torch.jit.load(buffer, map_location=torch.device('cpu'))\n\n        # Load all tensors onto CPU, using a string\n        buffer.seek(0)\n        torch.jit.load(buffer, map_location='cpu')\n\n        # Load with extra files.\n        extra_files = {'foo.txt': ''}  # values will be replaced with data\n        torch.jit.load('scriptmodule.pt', _extra_files=extra_files)\n        print(extra_files['foo.txt'])\n\n    .. testoutput::\n        :hide:\n\n        ...\n\n    .. testcleanup::\n\n        import os\n        os.remove(\"scriptmodule.pt\")\n    ",
      "arguments": [
        "f",
        "map_location",
        "_extra_files",
        "_restore_shapes"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Load a :class:`ScriptModule` or :class:`ScriptFunction` previously saved with :func:`torch.jit.save <torch.jit.save>`.\n\n    All previously saved modules, no matter their device, are first loaded onto CPU,\n    and then are moved to the devices they were saved from. If this fails (e.g.\n    because the run time system doesn't have certain devices), an exception is\n    raised.\n\n    Args:\n        f: a file-like object (has to implement read, readline, tell, and seek),\n            or a string containing a file name\n        map_location (string or torch.device): A simplified version of\n            ``map_location`` in `torch.jit.save` used to dynamically remap\n            storages to an alternative set of devices.\n        _extra_files (dictionary of filename to content): The extra\n            filenames given in the map would be loaded and their content\n            would be stored in the provided map.\n        _restore_shapes (bool): Whether or not to retrace the module on load using stored inputs\n\n    Returns:\n        A :class:`ScriptModule` object.\n\n    .. warning::\n        It is possible to construct malicious pickle data which will execute arbitrary code\n        during func:`torch.jit.load`. Never load data that could have come from an untrusted\n        source, or that could have been tampered with. **Only load data you trust**.\n\n    Example:\n    .. testcode::\n\n        import torch\n        import io\n\n        torch.jit.load('scriptmodule.pt')\n\n        # Load ScriptModule from io.BytesIO object\n        with open('scriptmodule.pt', 'rb') as f:\n            buffer = io.BytesIO(f.read())\n\n        # Load all tensors to the original device\n        torch.jit.load(buffer)\n\n        # Load all tensors onto CPU, using a device\n        buffer.seek(0)\n        torch.jit.load(buffer, map_location=torch.device('cpu'))\n\n        # Load all tensors onto CPU, using a string\n        buffer.seek(0)\n        torch.jit.load(buffer, map_location='cpu')\n\n        # Load with extra files.\n        extra_files = {'foo.txt': ''}  # values will be replaced with data\n        torch.jit.load('scriptmodule.pt', _extra_files=extra_files)\n        print(extra_files['foo.txt'])\n\n    .. testoutput::\n        :hide:\n\n        ...\n\n    .. testcleanup::\n\n        import os\n        os.remove(\"scriptmodule.pt\")\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.onednn_fusion_enabled",
      "signature": "torch.jit.onednn_fusion_enabled()",
      "doc": "Return whether onednn JIT fusion is enabled.",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return whether onednn JIT fusion is enabled.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.optimize_for_inference",
      "signature": "torch.jit.optimize_for_inference(mod: torch.jit._script.ScriptModule, other_methods: Optional[list[str]] = None) -> torch.jit._script.ScriptModule",
      "doc": "\n    Perform a set of optimization passes to optimize a model for the purposes of inference.\n\n    If the model is not already frozen, optimize_for_inference\n    will invoke `torch.jit.freeze` automatically.\n\n    In addition to generic optimizations that should speed up your model regardless\n    of environment, prepare for inference will also bake in build specific settings\n    such as the presence of CUDNN or MKLDNN, and may in the future make transformations\n    which speed things up on one machine but slow things down on another. Accordingly,\n    serialization is not implemented following invoking `optimize_for_inference` and\n    is not guaranteed.\n\n    This is still in prototype, and may have the potential to slow down your model.\n    Primary use cases that have been targeted so far have been vision models on cpu\n    and gpu to a lesser extent.\n\n    Example (optimizing a module with Conv->Batchnorm)::\n\n        import torch\n\n        in_channels, out_channels = 3, 32\n        conv = torch.nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=2, bias=True\n        )\n        bn = torch.nn.BatchNorm2d(out_channels, eps=0.001)\n        mod = torch.nn.Sequential(conv, bn)\n        frozen_mod = torch.jit.optimize_for_inference(torch.jit.script(mod.eval()))\n        assert \"batch_norm\" not in str(frozen_mod.graph)\n        # if built with MKLDNN, convolution will be run with MKLDNN weights\n        assert \"MKLDNN\" in frozen_mod.graph\n    ",
      "arguments": [
        "mod",
        "other_methods"
      ],
      "return_type": "<class 'torch.jit._script.ScriptModule'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Perform a set of optimization passes to optimize a model for the purposes of inference.\n\n    If the model is not already frozen, optimize_for_inference\n    will invoke `torch.jit.freeze` automatically.\n\n    In addition to generic optimizations that should speed up your model regardless\n    of environment, prepare for inference will also bake in build specific settings\n    such as the presence of CUDNN or MKLDNN, and may in the future make transformations\n    which speed things up on one machine but slow things down on another. Accordingly,\n    serialization is not implemented following invoking `optimize_for_inference` and\n    is not guaranteed.\n\n    This is still in prototype, and may have the potential to slow down your model.\n    Primary use cases that have been targeted so far have been vision models on cpu\n    and gpu to a lesser extent.\n\n    Example (optimizing a module with Conv->Batchnorm)::\n\n        import torch\n\n        in_channels, out_channels = 3, 32\n        conv = torch.nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=2, bias=True\n        )\n        bn = torch.nn.BatchNorm2d(out_channels, eps=0.001)\n        mod = torch.nn.Sequential(conv, bn)\n        frozen_mod = torch.jit.optimize_for_inference(torch.jit.script(mod.eval()))\n        assert \"batch_norm\" not in str(frozen_mod.graph)\n        # if built with MKLDNN, convolution will be run with MKLDNN weights\n        assert \"MKLDNN\" in frozen_mod.graph\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.optimized_execution",
      "signature": "torch.jit.optimized_execution(should_optimize)",
      "doc": "Context manager that controls whether the JIT's executor will run optimizations before executing a function.",
      "arguments": [
        "should_optimize"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Context manager that controls whether the JIT's executor will run optimizations before executing a function.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.run_frozen_optimizations",
      "signature": "torch.jit.run_frozen_optimizations(mod, optimize_numerics: bool = True, preserved_methods: Optional[list[str]] = None)",
      "doc": "\n    Run a series of optimizations looking for patterns that occur in frozen graphs.\n\n    The current set of optimizations includes:\n        - Dropout Removal\n        - Pretranspose Linear Layers\n        - Concat Linear Layers with same input Tensor\n        - Conv -> Batchnorm folding\n        - Conv -> Add/Sub folding\n        - Conv -> Mul/Div folding\n\n    Args:\n        mod (:class:`ScriptModule`): a frozen module to be optimized\n\n        optimize_numerics (bool): If ``True``, a set of optimization passes will be run that does not strictly\n        preserve numerics. These optimizations preserve default rtol and atol of `torch.testing.assert_close`\n        when applied on a single transformation, however in a module where many transformations are applied\n        the rtol or atol may no longer fall within the default `assert_close` tolerance. Conv -> Batchnorm folding,\n        Conv-Add/Sub, and Conv -> Mul/Div folding all may alter numerics.\n\n    Returns:\n        None\n\n    Note:\n        In rare occassions, this can result in slower execution.\n\n    Example (Freezing a module with Conv->Batchnorm)\n    .. code-block:: python\n        import torch\n\n        in_channels, out_channels = 3, 32\n        conv = torch.nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=2, bias=True\n        )\n        bn = torch.nn.BatchNorm2d(out_channels, eps=0.001)\n        mod = torch.nn.Sequential(conv, bn)\n        # set optimize to False here, by default freezing runs run_frozen_optimizations\n        frozen_mod = torch.jit.freeze(torch.jit.script(mod.eval()), optimize=False)\n        # inspect frozen mod\n        assert \"batch_norm\" in str(frozen_mod.graph)\n        torch.jit.run_frozen_optimizations(frozen_mod)\n        assert \"batch_norm\" not in str(frozen_mod.graph)\n\n    ",
      "arguments": [
        "mod",
        "optimize_numerics",
        "preserved_methods"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Run a series of optimizations looking for patterns that occur in frozen graphs.\n\n    The current set of optimizations includes:\n        - Dropout Removal\n        - Pretranspose Linear Layers\n        - Concat Linear Layers with same input Tensor\n        - Conv -> Batchnorm folding\n        - Conv -> Add/Sub folding\n        - Conv -> Mul/Div folding\n\n    Args:\n        mod (:class:`ScriptModule`): a frozen module to be optimized\n\n        optimize_numerics (bool): If ``True``, a set of optimization passes will be run that does not strictly\n        preserve numerics. These optimizations preserve default rtol and atol of `torch.testing.assert_close`\n        when applied on a single transformation, however in a module where many transformations are applied\n        the rtol or atol may no longer fall within the default `assert_close` tolerance. Conv -> Batchnorm folding,\n        Conv-Add/Sub, and Conv -> Mul/Div folding all may alter numerics.\n\n    Returns:\n        None\n\n    Note:\n        In rare occassions, this can result in slower execution.\n\n    Example (Freezing a module with Conv->Batchnorm)\n    .. code-block:: python\n        import torch\n\n        in_channels, out_channels = 3, 32\n        conv = torch.nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=2, bias=True\n        )\n        bn = torch.nn.BatchNorm2d(out_channels, eps=0.001)\n        mod = torch.nn.Sequential(conv, bn)\n        # set optimize to False here, by default freezing runs run_frozen_optimizations\n        frozen_mod = torch.jit.freeze(torch.jit.script(mod.eval()), optimize=False)\n        # inspect frozen mod\n        assert \"batch_norm\" in str(frozen_mod.graph)\n        torch.jit.run_frozen_optimizations(frozen_mod)\n        assert \"batch_norm\" not in str(frozen_mod.graph)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.save",
      "signature": "torch.jit.save(m, f, _extra_files=None)",
      "doc": "\n    Save an offline version of this module for use in a separate process.\n\n    The saved module serializes all of the methods, submodules, parameters, and\n    attributes of this module. It can be loaded into the C++ API using\n    ``torch::jit::load(filename)`` or into the Python API with\n    :func:`torch.jit.load <torch.jit.load>`.\n\n    To be able to save a module, it must not make any calls to native Python\n    functions.  This means that all submodules must be subclasses of\n    :class:`ScriptModule` as well.\n\n    .. DANGER::\n        All modules, no matter their device, are always loaded onto the CPU\n        during loading.  This is different from :func:`torch.load`'s semantics\n        and may change in the future.\n\n    Args:\n        m: A :class:`ScriptModule` to save.\n        f: A file-like object (has to implement write and flush) or a string\n           containing a file name.\n        _extra_files: Map from filename to contents which will be stored as part of `f`.\n\n    .. note::\n        torch.jit.save attempts to preserve the behavior of some operators\n        across versions. For example, dividing two integer tensors in\n        PyTorch 1.5 performed floor division, and if the module\n        containing that code is saved in PyTorch 1.5 and loaded in PyTorch 1.6\n        its division behavior will be preserved. The same module saved in\n        PyTorch 1.6 will fail to load in PyTorch 1.5, however, since the\n        behavior of division changed in 1.6, and 1.5 does not know how to\n        replicate the 1.6 behavior.\n\n    Example:\n    .. testcode::\n\n        import torch\n        import io\n\n        class MyModule(torch.nn.Module):\n            def forward(self, x):\n                return x + 10\n\n        m = torch.jit.script(MyModule())\n\n        # Save to file\n        torch.jit.save(m, 'scriptmodule.pt')\n        # This line is equivalent to the previous\n        m.save(\"scriptmodule.pt\")\n\n        # Save to io.BytesIO buffer\n        buffer = io.BytesIO()\n        torch.jit.save(m, buffer)\n\n        # Save with extra files\n        extra_files = {'foo.txt': b'bar'}\n        torch.jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)\n    ",
      "arguments": [
        "m",
        "f",
        "_extra_files"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Save an offline version of this module for use in a separate process.\n\n    The saved module serializes all of the methods, submodules, parameters, and\n    attributes of this module. It can be loaded into the C++ API using\n    ``torch::jit::load(filename)`` or into the Python API with\n    :func:`torch.jit.load <torch.jit.load>`.\n\n    To be able to save a module, it must not make any calls to native Python\n    functions.  This means that all submodules must be subclasses of\n    :class:`ScriptModule` as well.\n\n    .. DANGER::\n        All modules, no matter their device, are always loaded onto the CPU\n        during loading.  This is different from :func:`torch.load`'s semantics\n        and may change in the future.\n\n    Args:\n        m: A :class:`ScriptModule` to save.\n        f: A file-like object (has to implement write and flush) or a string\n           containing a file name.\n        _extra_files: Map from filename to contents which will be stored as part of `f`.\n\n    .. note::\n        torch.jit.save attempts to preserve the behavior of some operators\n        across versions. For example, dividing two integer tensors in\n        PyTorch 1.5 performed floor division, and if the module\n        containing that code is saved in PyTorch 1.5 and loaded in PyTorch 1.6\n        its division behavior will be preserved. The same module saved in\n        PyTorch 1.6 will fail to load in PyTorch 1.5, however, since the\n        behavior of division changed in 1.6, and 1.5 does not know how to\n        replicate the 1.6 behavior.\n\n    Example:\n    .. testcode::\n\n        import torch\n        import io\n\n        class MyModule(torch.nn.Module):\n            def forward(self, x):\n                return x + 10\n\n        m = torch.jit.script(MyModule())\n\n        # Save to file\n        torch.jit.save(m, 'scriptmodule.pt')\n        # This line is equivalent to the previous\n        m.save(\"scriptmodule.pt\")\n\n        # Save to io.BytesIO buffer\n        buffer = io.BytesIO()\n        torch.jit.save(m, buffer)\n\n        # Save with extra files\n        extra_files = {'foo.txt': b'bar'}\n        torch.jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.save_jit_module_to_flatbuffer",
      "signature": "torch.jit.save_jit_module_to_flatbuffer(m, f, _extra_files=None)",
      "doc": "\n    Save an offline version of this module for use in a separate process.\n\n    The saved module serializes all of the methods, submodules, parameters, and\n    attributes of this module. It can be loaded into the C++ API using\n    ``torch::jit::load_jit_module_from_file(filename)`` or into the Python API with\n    :func:`torch.jit.jit_module_from_flatbuffer<torch.jit.jit_module_from_flatbuffer>`.\n\n    To be able to save a module, it must not make any calls to native Python\n    functions.  This means that all submodules must be subclasses of\n    :class:`ScriptModule` as well.\n\n    .. DANGER::\n        All modules, no matter their device, are always loaded onto the CPU\n        during loading.  This is different from :func:`torch.load`'s semantics\n        and may change in the future.\n\n    Args:\n        m: A :class:`ScriptModule` to save.\n        f: A string for file path\n\n\n    Example:\n    .. testcode::\n\n        import torch\n        import io\n\n        class MyModule(torch.nn.Module):\n            def forward(self, x):\n                return x + 10\n\n        m = torch.jit.script(MyModule())\n\n        # Save to file\n        torch.jit.save_jit_module_to_flatbuffer(m, 'scriptmodule.ff')\n    ",
      "arguments": [
        "m",
        "f",
        "_extra_files"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Save an offline version of this module for use in a separate process.\n\n    The saved module serializes all of the methods, submodules, parameters, and\n    attributes of this module. It can be loaded into the C++ API using\n    ``torch::jit::load_jit_module_from_file(filename)`` or into the Python API with\n    :func:`torch.jit.jit_module_from_flatbuffer<torch.jit.jit_module_from_flatbuffer>`.\n\n    To be able to save a module, it must not make any calls to native Python\n    functions.  This means that all submodules must be subclasses of\n    :class:`ScriptModule` as well.\n\n    .. DANGER::\n        All modules, no matter their device, are always loaded onto the CPU\n        during loading.  This is different from :func:`torch.load`'s semantics\n        and may change in the future.\n\n    Args:\n        m: A :class:`ScriptModule` to save.\n        f: A string for file path\n\n\n    Example:\n    .. testcode::\n\n        import torch\n        import io\n\n        class MyModule(torch.nn.Module):\n            def forward(self, x):\n                return x + 10\n\n        m = torch.jit.script(MyModule())\n\n        # Save to file\n        torch.jit.save_jit_module_to_flatbuffer(m, 'scriptmodule.ff')\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.script",
      "signature": "torch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None, example_inputs: Union[list[tuple], dict[Callable, list[tuple]], NoneType] = None)",
      "doc": "Script the function.\n\n    Scripting a function or ``nn.Module`` will inspect the source code, compile\n    it as TorchScript code using the TorchScript compiler, and return a :class:`ScriptModule` or\n    :class:`ScriptFunction`. TorchScript itself is a subset of the Python language, so not all\n    features in Python work, but we provide enough functionality to compute on\n    tensors and do control-dependent operations. For a complete guide, see the\n    :ref:`language-reference`.\n\n    Scripting a dictionary or list copies the data inside it into a TorchScript instance than can be\n    subsequently passed by reference between Python and TorchScript with zero copy overhead.\n\n    ``torch.jit.script`` can be used as a function for modules, functions, dictionaries and lists\n     and as a decorator ``@torch.jit.script`` for :ref:`torchscript-classes` and functions.\n\n    Args:\n        obj (Callable, class, or nn.Module):  The ``nn.Module``, function, class type,\n                                                  dictionary, or list to compile.\n        example_inputs (Union[List[Tuple], Dict[Callable, List[Tuple]], None]): Provide example inputs\n            to annotate the arguments for a function or ``nn.Module``.\n\n    Returns:\n        If ``obj`` is ``nn.Module``, ``script`` returns\n        a :class:`ScriptModule` object. The returned :class:`ScriptModule` will\n        have the same set of sub-modules and parameters as the\n        original ``nn.Module``. If ``obj`` is a standalone function,\n        a :class:`ScriptFunction` will be returned. If ``obj`` is a ``dict``, then\n        ``script`` returns an instance of `torch._C.ScriptDict`. If ``obj`` is a ``list``,\n        then ``script`` returns an instance of `torch._C.ScriptList`.\n\n    **Scripting a function**\n        The ``@torch.jit.script`` decorator will construct a :class:`ScriptFunction`\n        by compiling the body of the function.\n\n        Example (scripting a function):\n\n        .. testcode::\n\n            import torch\n\n            @torch.jit.script\n            def foo(x, y):\n                if x.max() > y.max():\n                    r = x\n                else:\n                    r = y\n                return r\n\n            print(type(foo))  # torch.jit.ScriptFunction\n\n            # See the compiled graph as Python code\n            print(foo.code)\n\n            # Call the function using the TorchScript interpreter\n            foo(torch.ones(2, 2), torch.ones(2, 2))\n\n        .. testoutput::\n            :hide:\n\n            ...\n\n    ****Scripting a function using example_inputs**\n        Example inputs can be used to annotate a function arguments.\n\n        Example (annotating a function before scripting):\n\n        .. testcode::\n\n            import torch\n\n            def test_sum(a, b):\n                return a + b\n\n            # Annotate the arguments to be int\n            scripted_fn = torch.jit.script(test_sum, example_inputs=[(3, 4)])\n\n            print(type(scripted_fn))  # torch.jit.ScriptFunction\n\n            # See the compiled graph as Python code\n            print(scripted_fn.code)\n\n            # Call the function using the TorchScript interpreter\n            scripted_fn(20, 100)\n\n        .. testoutput::\n            :hide:\n\n            ...\n\n    **Scripting an nn.Module**\n        Scripting an ``nn.Module`` by default will compile the ``forward`` method and recursively\n        compile any methods, submodules, and functions called by ``forward``. If a ``nn.Module`` only uses\n        features supported in TorchScript, no changes to the original module code should be necessary. ``script``\n        will construct :class:`ScriptModule` that has copies of the attributes, parameters, and methods of\n        the original module.\n\n        Example (scripting a simple module with a Parameter):\n\n        .. testcode::\n\n            import torch\n\n            class MyModule(torch.nn.Module):\n                def __init__(self, N, M):\n                    super().__init__()\n                    # This parameter will be copied to the new ScriptModule\n                    self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n                    # When this submodule is used, it will be compiled\n                    self.linear = torch.nn.Linear(N, M)\n\n                def forward(self, input):\n                    output = self.weight.mv(input)\n\n                    # This calls the `forward` method of the `nn.Linear` module, which will\n                    # cause the `self.linear` submodule to be compiled to a `ScriptModule` here\n                    output = self.linear(output)\n                    return output\n\n            scripted_module = torch.jit.script(MyModule(2, 3))\n\n        Example (scripting a module with traced submodules):\n\n        .. testcode::\n\n            import torch\n            import torch.nn as nn\n            import torch.nn.functional as F\n\n            class MyModule(nn.Module):\n                def __init__(self) -> None:\n                    super().__init__()\n                    # torch.jit.trace produces a ScriptModule's conv1 and conv2\n                    self.conv1 = torch.jit.trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16))\n                    self.conv2 = torch.jit.trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16))\n\n                def forward(self, input):\n                    input = F.relu(self.conv1(input))\n                    input = F.relu(self.conv2(input))\n                    return input\n\n            scripted_module = torch.jit.script(MyModule())\n\n        To compile a method other than ``forward`` (and recursively compile anything it calls), add\n        the :func:`@torch.jit.export <torch.jit.export>` decorator to the method. To opt out of compilation\n        use :func:`@torch.jit.ignore <torch.jit.ignore>` or :func:`@torch.jit.unused <torch.jit.unused>`.\n\n        Example (an exported and ignored method in a module)::\n\n            import torch\n            import torch.nn as nn\n\n\n            class MyModule(nn.Module):\n                def __init__(self) -> None:\n                    super().__init__()\n\n                @torch.jit.export\n                def some_entry_point(self, input):\n                    return input + 10\n\n                @torch.jit.ignore\n                def python_only_fn(self, input):\n                    # This function won't be compiled, so any\n                    # Python APIs can be used\n                    import pdb\n\n                    pdb.set_trace()\n\n                def forward(self, input):\n                    if self.training:\n                        self.python_only_fn(input)\n                    return input * 99\n\n\n            scripted_module = torch.jit.script(MyModule())\n            print(scripted_module.some_entry_point(torch.randn(2, 2)))\n            print(scripted_module(torch.randn(2, 2)))\n\n        Example ( Annotating forward of nn.Module using example_inputs)::\n\n            import torch\n            import torch.nn as nn\n            from typing import NamedTuple\n\n            class MyModule(NamedTuple):\n            result: List[int]\n\n            class TestNNModule(torch.nn.Module):\n                def forward(self, a) -> MyModule:\n                    result = MyModule(result=a)\n                    return result\n\n            pdt_model = TestNNModule()\n\n            # Runs the pdt_model in eager model with the inputs provided and annotates the arguments of forward\n            scripted_model = torch.jit.script(pdt_model, example_inputs={pdt_model: [([10, 20, ], ), ], })\n\n            # Run the scripted_model with actual inputs\n            print(scripted_model([20]))\n    ",
      "arguments": [
        "obj",
        "optimize",
        "_frames_up",
        "_rcb",
        "example_inputs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Script the function.\n\n    Scripting a function or ``nn.Module`` will inspect the source code, compile\n    it as TorchScript code using the TorchScript compiler, and return a :class:`ScriptModule` or\n    :class:`ScriptFunction`. TorchScript itself is a subset of the Python language, so not all\n    features in Python work, but we provide enough functionality to compute on\n    tensors and do control-dependent operations. For a complete guide, see the\n    :ref:`language-reference`.\n\n    Scripting a dictionary or list copies the data inside it into a TorchScript instance than can be\n    subsequently passed by reference between Python and TorchScript with zero copy overhead.\n\n    ``torch.jit.script`` can be used as a function for modules, functions, dictionaries and lists\n     and as a decorator ``@torch.jit.script`` for :ref:`torchscript-classes` and functions.\n\n    Args:\n        obj (Callable, class, or nn.Module):  The ``nn.Module``, function, class type,\n                                                  dictionary, or list to compile.\n        example_inputs (Union[List[Tuple], Dict[Callable, List[Tuple]], None]): Provide example inputs\n            to annotate the arguments for a function or ``nn.Module``.\n\n    Returns:\n        If ``obj`` is ``nn.Module``, ``script`` returns\n        a :class:`ScriptModule` object. The returned :class:`ScriptModule` will\n        have the same set of sub-modules and parameters as the\n        original ``nn.Module``. If ``obj`` is a standalone function,\n        a :class:`ScriptFunction` will be returned. If ``obj`` is a ``dict``, then\n        ``script`` returns an instance of `torch._C.ScriptDict`. If ``obj`` is a ``list``,\n        then ``script`` returns an instance of `torch._C.ScriptList`.\n\n    **Scripting a function**\n        The ``@torch.jit.script`` decorator will construct a :class:`ScriptFunction`\n        by compiling the body of the function.\n\n        Example (scripting a function):\n\n        .. testcode::\n\n            import torch\n\n            @torch.jit.script\n            def foo(x, y):\n                if x.max() > y.max():\n                    r = x\n                else:\n                    r = y\n                return r\n\n            print(type(foo))  # torch.jit.ScriptFunction\n\n            # See the compiled graph as Python code\n            print(foo.code)\n\n            # Call the function using the TorchScript interpreter\n            foo(torch.ones(2, 2), torch.ones(2, 2))\n\n        .. testoutput::\n            :hide:\n\n            ...\n\n    ****Scripting a function using example_inputs**\n        Example inputs can be used to annotate a function arguments.\n\n        Example (annotating a function before scripting):\n\n        .. testcode::\n\n            import torch\n\n            def test_sum(a, b):\n                return a + b\n\n            # Annotate the arguments to be int\n            scripted_fn = torch.jit.script(test_sum, example_inputs=[(3, 4)])\n\n            print(type(scripted_fn))  # torch.jit.ScriptFunction\n\n            # See the compiled graph as Python code\n            print(scripted_fn.code)\n\n            # Call the function using the TorchScript interpreter\n            scripted_fn(20, 100)\n\n        .. testoutput::\n            :hide:\n\n            ...\n\n    **Scripting an nn.Module**\n        Scripting an ``nn.Module`` by default will compile the ``forward`` method and recursively\n        compile any methods, submodules, and functions called by ``forward``. If a ``nn.Module`` only uses\n        features supported in TorchScript, no changes to the original module code should be necessary. ``script``\n        will construct :class:`ScriptModule` that has copies of the attributes, parameters, and methods of\n        the original module.\n\n        Example (scripting a simple module with a Parameter):\n\n        .. testcode::\n\n            import torch\n\n            class MyModule(torch.nn.Module):\n                def __init__(self, N, M):\n                    super().__init__()\n                    # This parameter will be copied to the new ScriptModule\n                    self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n                    # When this submodule is used, it will be compiled\n                    self.linear = torch.nn.Linear(N, M)\n\n                def forward(self, input):\n                    output = self.weight.mv(input)\n\n                    # This calls the `forward` method of the `nn.Linear` module, which will\n                    # cause the `self.linear` submodule to be compiled to a `ScriptModule` here\n                    output = self.linear(output)\n                    return output\n\n            scripted_module = torch.jit.script(MyModule(2, 3))\n\n        Example (scripting a module with traced submodules):\n\n        .. testcode::\n\n            import torch\n            import torch.nn as nn\n            import torch.nn.functional as F\n\n            class MyModule(nn.Module):\n                def __init__(self) -> None:\n                    super().__init__()\n                    # torch.jit.trace produces a ScriptModule's conv1 and conv2\n                    self.conv1 = torch.jit.trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16))\n                    self.conv2 = torch.jit.trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16))\n\n                def forward(self, input):\n                    input = F.relu(self.conv1(input))\n                    input = F.relu(self.conv2(input))\n                    return input\n\n            scripted_module = torch.jit.script(MyModule())\n\n        To compile a method other than ``forward`` (and recursively compile anything it calls), add\n        the :func:`@torch.jit.export <torch.jit.export>` decorator to the method. To opt out of compilation\n        use :func:`@torch.jit.ignore <torch.jit.ignore>` or :func:`@torch.jit.unused <torch.jit.unused>`.\n\n        Example (an exported and ignored method in a module)::\n\n            import torch\n            import torch.nn as nn\n\n\n            class MyModule(nn.Module):\n                def __init__(self) -> None:\n                    super().__init__()\n\n                @torch.jit.export\n                def some_entry_point(self, input):\n                    return input + 10\n\n                @torch.jit.ignore\n                def python_only_fn(self, input):\n                    # This function won't be compiled, so any\n                    # Python APIs can be used\n                    import pdb\n\n                    pdb.set_trace()\n\n                def forward(self, input):\n                    if self.training:\n                        self.python_only_fn(input)\n                    return input * 99\n\n\n            scripted_module = torch.jit.script(MyModule())\n            print(scripted_module.some_entry_point(torch.randn(2, 2)))\n            print(scripted_module(torch.randn(2, 2)))\n\n        Example ( Annotating forward of nn.Module using example_inputs)::\n\n            import torch\n            import torch.nn as nn\n            from typing import NamedTuple\n\n            class MyModule(NamedTuple):\n            result: List[int]\n\n            class TestNNModule(torch.nn.Module):\n                def forward(self, a) -> MyModule:\n                    result = MyModule(result=a)\n                    return result\n\n            pdt_model = TestNNModule()\n\n            # Runs the pdt_model in eager model with the inputs provided and annotates the arguments of forward\n            scripted_model = torch.jit.script(pdt_model, example_inputs={pdt_model: [([10, 20, ], ), ], })\n\n            # Run the scripted_model with actual inputs\n            print(scripted_model([20]))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.script_if_tracing",
      "signature": "torch.jit.script_if_tracing(fn)",
      "doc": "\n    Compiles ``fn`` when it is first called during tracing.\n\n    ``torch.jit.script`` has a non-negligible start up time when it is first called due to\n    lazy-initializations of many compiler builtins. Therefore you should not use\n    it in library code. However, you may want to have parts of your library work\n    in tracing even if they use control flow. In these cases, you should use\n    ``@torch.jit.script_if_tracing`` to substitute for\n    ``torch.jit.script``.\n\n    Args:\n        fn: A function to compile.\n\n    Returns:\n        If called during tracing, a :class:`ScriptFunction` created by `torch.jit.script` is returned.\n        Otherwise, the original function `fn` is returned.\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Compiles ``fn`` when it is first called during tracing.\n\n    ``torch.jit.script`` has a non-negligible start up time when it is first called due to\n    lazy-initializations of many compiler builtins. Therefore you should not use\n    it in library code. However, you may want to have parts of your library work\n    in tracing even if they use control flow. In these cases, you should use\n    ``@torch.jit.script_if_tracing`` to substitute for\n    ``torch.jit.script``.\n\n    Args:\n        fn: A function to compile.\n\n    Returns:\n        If called during tracing, a :class:`ScriptFunction` created by `torch.jit.script` is returned.\n        Otherwise, the original function `fn` is returned.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.script_method",
      "signature": "torch.jit.script_method(fn)",
      "doc": "",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.set_fusion_strategy",
      "signature": "torch.jit.set_fusion_strategy(strategy: list[tuple[str, int]])",
      "doc": "Set the type and number of specializations that can occur during fusion.\n\n    Usage: provide a list of pairs (type, depth) where type is one of \"STATIC\" or \"DYNAMIC\"\n    and depth is an integer.\n\n    Behavior - static vs dynamic:\n        In STATIC fusion, fused ops are compiled to have fixed input shapes. The shape is determined\n        based on some initial profiling runs.\n        In DYNAMIC fusion, fused ops are compiled to have variable input shapes, so that multiple\n        shapes are possible.\n\n    In both cases, we also recompile on new striding behavior, device, or dtype.\n\n    Behavior - fallback functions & depth:\n        When an input doesn't match the format required by the specialized compiled op, it will run\n        a fallback function. Fallback functions are recursively be compiled and specialized based\n        on the observed tensor shapes. Since compilation can be slow, the \"depth\" parameter is provided to\n        limit the number of specializations that can be compiled, before giving up on recompiling and\n        falling back to a completely un-fused, un-specialized implementation.\n\n    The list of (type, depth) pairs controls the type of specializations and the number of\n    specializations. For example: [(\"STATIC\", 2), (\"DYNAMIC\", 2)] indicates that the first\n    two specializations will use static fusions, the following two specializations will use\n    dynamic fusion, and any inputs that satisfy none of the 4 options will run an\n    unfused implementation.\n\n    NB: in the future, if more as more fusion backends are added there may be more granular\n    apis for specific fusers.\n    ",
      "arguments": [
        "strategy"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the type and number of specializations that can occur during fusion.\n\n    Usage: provide a list of pairs (type, depth) where type is one of \"STATIC\" or \"DYNAMIC\"\n    and depth is an integer.\n\n    Behavior - static vs dynamic:\n        In STATIC fusion, fused ops are compiled to have fixed input shapes. The shape is determined\n        based on some initial profiling runs.\n        In DYNAMIC fusion, fused ops are compiled to have variable input shapes, so that multiple\n        shapes are possible.\n\n    In both cases, we also recompile on new striding behavior, device, or dtype.\n\n    Behavior - fallback functions & depth:\n        When an input doesn't match the format required by the specialized compiled op, it will run\n        a fallback function. Fallback functions are recursively be compiled and specialized based\n        on the observed tensor shapes. Since compilation can be slow, the \"depth\" parameter is provided to\n        limit the number of specializations that can be compiled, before giving up on recompiling and\n        falling back to a completely un-fused, un-specialized implementation.\n\n    The list of (type, depth) pairs controls the type of specializations and the number of\n    specializations. For example: [(\"STATIC\", 2), (\"DYNAMIC\", 2)] indicates that the first\n    two specializations will use static fusions, the following two specializations will use\n    dynamic fusion, and any inputs that satisfy none of the 4 options will run an\n    unfused implementation.\n\n    NB: in the future, if more as more fusion backends are added there may be more granular\n    apis for specific fusers.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.set_module",
      "signature": "torch.jit.set_module(obj, mod)",
      "doc": "\n    Set the module attribute on a python object for a given object for nicer printing\n    ",
      "arguments": [
        "obj",
        "mod"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Set the module attribute on a python object for a given object for nicer printing\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.trace",
      "signature": "torch.jit.trace(func, example_inputs=None, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object at 0x111f73e70>, example_kwarg_inputs=None, _store_inputs=True)",
      "doc": "\n    Trace a function and return an executable  or :class:`ScriptFunction` that will be optimized using just-in-time compilation.\n\n    Tracing is ideal for code that operates only on\n    ``Tensor``\\\\s and lists, dictionaries, and\n    tuples of ``Tensor``\\\\s.\n\n    Using `torch.jit.trace` and `torch.jit.trace_module`, you can turn an\n    existing module or Python function into a TorchScript\n    :class:`ScriptFunction` or :class:`ScriptModule`. You must provide example\n    inputs, and we run the function, recording the operations performed on all\n    the tensors.\n\n    * The resulting recording of a standalone function produces `ScriptFunction`.\n    * The resulting recording of `nn.Module.forward` or `nn.Module` produces\n      `ScriptModule`.\n\n    This module also contains any parameters that the original\n    module had as well.\n\n    Warning:\n        Tracing only correctly records functions and modules which are not data\n        dependent (e.g., do not have conditionals on data in tensors) and do not have\n        any untracked external dependencies (e.g., perform input/output or\n        access global variables). Tracing only records operations done when the given\n        function is run on the given tensors. Therefore, the returned\n        `ScriptModule` will always run the same traced graph on any input. This\n        has some important implications when your module is expected to run\n        different sets of operations, depending on the input and/or the module\n        state. For example,\n\n        * Tracing will not record any control-flow like if-statements or loops.\n          When this control-flow is constant across your module, this is fine\n          and it often inlines the control-flow decisions. But sometimes the\n          control-flow is actually part of the model itself. For instance, a\n          recurrent network is a loop over the (possibly dynamic) length of an\n          input sequence.\n        * In the returned :class:`ScriptModule`, operations that have different\n          behaviors in ``training`` and ``eval`` modes will always behave as if\n          it is in the mode it was in during tracing, no matter which mode the\n          `ScriptModule` is in.\n\n        In cases like these, tracing would not be appropriate and\n        :func:`scripting <torch.jit.script>` is a better choice. If you trace\n        such models, you may silently get incorrect results on subsequent\n        invocations of the model. The tracer will try to emit warnings when\n        doing something that may cause an incorrect trace to be produced.\n\n    Args:\n        func (callable or torch.nn.Module):  A Python function or `torch.nn.Module`\n            that will be run with `example_inputs`. `func` arguments and return\n            values  must be tensors or (possibly nested) tuples that contain\n            tensors. When a module is passed `torch.jit.trace`, only the\n            ``forward`` method is run and traced (see :func:`torch.jit.trace\n            <torch.jit.trace_module>` for details).\n\n    Keyword arguments:\n        example_inputs (tuple or torch.Tensor or None, optional): A tuple of example\n            inputs that will be passed to the function while tracing.\n            Default: ``None``. Either this argument or ``example_kwarg_inputs``\n            should be specified. The resulting trace can be run with inputs of\n            different types and shapes assuming the traced operations support those\n            types and shapes. `example_inputs` may also be a single Tensor in which\n            case it is automatically wrapped in a tuple. When the value is None,\n            ``example_kwarg_inputs`` should be specified.\n\n        check_trace (``bool``, optional): Check if the same inputs run through\n            traced code produce the same outputs. Default: ``True``. You might want\n            to disable this if, for example, your network contains non-\n            deterministic ops or if you are sure that the network is correct despite\n            a checker failure.\n\n        check_inputs (list of tuples, optional): A list of tuples of input\n            arguments that should be used to check the trace against what is\n            expected. Each tuple is equivalent to a set of input arguments that\n            would be specified in ``example_inputs``. For best results, pass in\n            a set of checking inputs representative of the space of shapes and\n            types of inputs you expect the network to see.  If not specified,\n            the original ``example_inputs`` are used for checking\n        check_tolerance (float, optional): Floating-point comparison tolerance\n            to use in the checker procedure.  This can be used to relax the\n            checker strictness in the event that results diverge numerically\n            for a known reason, such as operator fusion.\n        strict (``bool``, optional): run the tracer in a strict mode or not\n            (default: ``True``). Only turn this off when you want the tracer to\n            record your mutable container types (currently ``list``/``dict``)\n            and you are sure that the container you are using in your\n            problem is a ``constant`` structure and does not get used as\n            control flow (if, for) conditions.\n        example_kwarg_inputs (dict, optional): This parameter is a pack of keyword\n            arguments of example inputs that will be passed to the function while\n            tracing. Default: ``None``. Either this argument or ``example_inputs``\n            should be specified. The dict will be unpacking by the arguments name\n            of the traced function. If the keys of the dict don't not match with\n            the traced function's arguments name, a runtime exception will be raised.\n\n    Returns:\n        If `func` is `nn.Module` or ``forward`` of `nn.Module`, `trace` returns\n        a :class:`ScriptModule` object with a single ``forward`` method\n        containing the traced code.  The returned `ScriptModule` will\n        have the same set of sub-modules and parameters as the original\n        ``nn.Module``.  If ``func`` is a standalone function, ``trace``\n        returns `ScriptFunction`.\n\n    Example (tracing a function):\n\n    .. testcode::\n\n        import torch\n\n        def foo(x, y):\n            return 2 * x + y\n\n        # Run `foo` with the provided inputs and record the tensor operations\n        traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n        # `traced_foo` can now be run with the TorchScript interpreter or saved\n        # and loaded in a Python-free environment\n\n    Example (tracing an existing module)::\n\n        import torch\n        import torch.nn as nn\n\n\n        class Net(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.conv = nn.Conv2d(1, 1, 3)\n\n            def forward(self, x):\n                return self.conv(x)\n\n\n        n = Net()\n        example_weight = torch.rand(1, 1, 3, 3)\n        example_forward_input = torch.rand(1, 1, 3, 3)\n\n        # Trace a specific method and construct `ScriptModule` with\n        # a single `forward` method\n        module = torch.jit.trace(n.forward, example_forward_input)\n\n        # Trace a module (implicitly traces `forward`) and construct a\n        # `ScriptModule` with a single `forward` method\n        module = torch.jit.trace(n, example_forward_input)\n\n    ",
      "arguments": [
        "func",
        "example_inputs",
        "optimize",
        "check_trace",
        "check_inputs",
        "check_tolerance",
        "strict",
        "_force_outplace",
        "_module_class",
        "_compilation_unit",
        "example_kwarg_inputs",
        "_store_inputs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Trace a function and return an executable  or :class:`ScriptFunction` that will be optimized using just-in-time compilation.\n\n    Tracing is ideal for code that operates only on\n    ``Tensor``\\\\s and lists, dictionaries, and\n    tuples of ``Tensor``\\\\s.\n\n    Using `torch.jit.trace` and `torch.jit.trace_module`, you can turn an\n    existing module or Python function into a TorchScript\n    :class:`ScriptFunction` or :class:`ScriptModule`. You must provide example\n    inputs, and we run the function, recording the operations performed on all\n    the tensors.\n\n    * The resulting recording of a standalone function produces `ScriptFunction`.\n    * The resulting recording of `nn.Module.forward` or `nn.Module` produces\n      `ScriptModule`.\n\n    This module also contains any parameters that the original\n    module had as well.\n\n    Warning:\n        Tracing only correctly records functions and modules which are not data\n        dependent (e.g., do not have conditionals on data in tensors) and do not have\n        any untracked external dependencies (e.g., perform input/output or\n        access global variables). Tracing only records operations done when the given\n        function is run on the given tensors. Therefore, the returned\n        `ScriptModule` will always run the same traced graph on any input. This\n        has some important implications when your module is expected to run\n        different sets of operations, depending on the input and/or the module\n        state. For example,\n\n        * Tracing will not record any control-flow like if-statements or loops.\n          When this control-flow is constant across your module, this is fine\n          and it often inlines the control-flow decisions. But sometimes the\n          control-flow is actually part of the model itself. For instance, a\n          recurrent network is a loop over the (possibly dynamic) length of an\n          input sequence.\n        * In the returned :class:`ScriptModule`, operations that have different\n          behaviors in ``training`` and ``eval`` modes will always behave as if\n          it is in the mode it was in during tracing, no matter which mode the\n          `ScriptModule` is in.\n\n        In cases like these, tracing would not be appropriate and\n        :func:`scripting <torch.jit.script>` is a better choice. If you trace\n        such models, you may silently get incorrect results on subsequent\n        invocations of the model. The tracer will try to emit warnings when\n        doing something that may cause an incorrect trace to be produced.\n\n    Args:\n        func (callable or torch.nn.Module):  A Python function or `torch.nn.Module`\n            that will be run with `example_inputs`. `func` arguments and return\n            values  must be tensors or (possibly nested) tuples that contain\n            tensors. When a module is passed `torch.jit.trace`, only the\n            ``forward`` method is run and traced (see :func:`torch.jit.trace\n            <torch.jit.trace_module>` for details).\n\n    Keyword arguments:\n        example_inputs (tuple or torch.Tensor or None, optional): A tuple of example\n            inputs that will be passed to the function while tracing.\n            Default: ``None``. Either this argument or ``example_kwarg_inputs``\n            should be specified. The resulting trace can be run with inputs of\n            different types and shapes assuming the traced operations support those\n            types and shapes. `example_inputs` may also be a single Tensor in which\n            case it is automatically wrapped in a tuple. When the value is None,\n            ``example_kwarg_inputs`` should be specified.\n\n        check_trace (``bool``, optional): Check if the same inputs run through\n            traced code produce the same outputs. Default: ``True``. You might want\n            to disable this if, for example, your network contains non-\n            deterministic ops or if you are sure that the network is correct despite\n            a checker failure.\n\n        check_inputs (list of tuples, optional): A list of tuples of input\n            arguments that should be used to check the trace against what is\n            expected. Each tuple is equivalent to a set of input arguments that\n            would be specified in ``example_inputs``. For best results, pass in\n            a set of checking inputs representative of the space of shapes and\n            types of inputs you expect the network to see.  If not specified,\n            the original ``example_inputs`` are used for checking\n        check_tolerance (float, optional): Floating-point comparison tolerance\n            to use in the checker procedure.  This can be used to relax the\n            checker strictness in the event that results diverge numerically\n            for a known reason, such as operator fusion.\n        strict (``bool``, optional): run the tracer in a strict mode or not\n            (default: ``True``). Only turn this off when you want the tracer to\n            record your mutable container types (currently ``list``/``dict``)\n            and you are sure that the container you are using in your\n            problem is a ``constant`` structure and does not get used as\n            control flow (if, for) conditions.\n        example_kwarg_inputs (dict, optional): This parameter is a pack of keyword\n            arguments of example inputs that will be passed to the function while\n            tracing. Default: ``None``. Either this argument or ``example_inputs``\n            should be specified. The dict will be unpacking by the arguments name\n            of the traced function. If the keys of the dict don't not match with\n            the traced function's arguments name, a runtime exception will be raised.\n\n    Returns:\n        If `func` is `nn.Module` or ``forward`` of `nn.Module`, `trace` returns\n        a :class:`ScriptModule` object with a single ``forward`` method\n        containing the traced code.  The returned `ScriptModule` will\n        have the same set of sub-modules and parameters as the original\n        ``nn.Module``.  If ``func`` is a standalone function, ``trace``\n        returns `ScriptFunction`.\n\n    Example (tracing a function):\n\n    .. testcode::\n\n        import torch\n\n        def foo(x, y):\n            return 2 * x + y\n\n        # Run `foo` with the provided inputs and record the tensor operations\n        traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n        # `traced_foo` can now be run with the TorchScript interpreter or saved\n        # and loaded in a Python-free environment\n\n    Example (tracing an existing module)::\n\n        import torch\n        import torch.nn as nn\n\n\n        class Net(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.conv = nn.Conv2d(1, 1, 3)\n\n            def forward(self, x):\n                return self.conv(x)\n\n\n        n = Net()\n        example_weight = torch.rand(1, 1, 3, 3)\n        example_forward_input = torch.rand(1, 1, 3, 3)\n\n        # Trace a specific method and construct `ScriptModule` with\n        # a single `forward` method\n        module = torch.jit.trace(n.forward, example_forward_input)\n\n        # Trace a module (implicitly traces `forward`) and construct a\n        # `ScriptModule` with a single `forward` method\n        module = torch.jit.trace(n, example_forward_input)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.trace_module",
      "signature": "torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object at 0x111f73e70>, example_inputs_is_kwarg=False, _store_inputs=True)",
      "doc": "\n    Trace a module and return an executable :class:`ScriptModule` that will be optimized using just-in-time compilation.\n\n    When a module is passed to :func:`torch.jit.trace <torch.jit.trace>`, only\n    the ``forward`` method is run and traced. With ``trace_module``, you can specify a dictionary of\n    method names to example inputs to trace (see the ``inputs``) argument below.\n\n    See :func:`torch.jit.trace <torch.jit.trace>` for more information on tracing.\n\n    Args:\n        mod (torch.nn.Module):  A ``torch.nn.Module`` containing methods whose names are\n                                specified in ``inputs``. The given methods will be compiled\n                                as a part of a single `ScriptModule`.\n        inputs (dict):  A dict containing sample inputs indexed by method names in ``mod``.\n                                The inputs will be passed to methods whose names correspond to inputs'\n                                keys while tracing.\n                                ``{ 'forward' : example_forward_input, 'method2': example_method2_input}``\n    Keyword arguments:\n        check_trace (``bool``, optional): Check if the same inputs run through\n                                      traced code produce the same outputs. Default: ``True``. You might want\n                                      to disable this if, for example, your network contains non-\n                                      deterministic ops or if you are sure that the network is correct despite\n                                      a checker failure.\n\n        check_inputs (list of dicts, optional): A list of dicts of input arguments that should be used\n                                                 to check the trace against what is expected. Each tuple\n                                                 is equivalent to a set of input arguments that would\n                                                 be specified in ``inputs``. For best results, pass in a\n                                                 set of checking inputs representative of the space of\n                                                 shapes and types of inputs you expect the network to see.\n                                                 If not specified, the original ``inputs`` are used for checking\n        check_tolerance (float, optional): Floating-point comparison tolerance to use in the checker procedure.\n                                           This can be used to relax the checker strictness in the event that\n                                           results diverge numerically for a known reason, such as operator fusion.\n        example_inputs_is_kwarg (``bool``, optional): This parameter indicate whether the example inputs is a pack\n                                           pack of keyword arguments. Default: ``False``.\n\n    Returns:\n        A :class:`ScriptModule` object with a single ``forward`` method containing the traced code.\n        When ``func`` is a ``torch.nn.Module``, the returned :class:`ScriptModule` will have the same set of\n        sub-modules and parameters as ``func``.\n\n    Example (tracing a module with multiple methods)::\n\n        import torch\n        import torch.nn as nn\n\n\n        class Net(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.conv = nn.Conv2d(1, 1, 3)\n\n            def forward(self, x):\n                return self.conv(x)\n\n            def weighted_kernel_sum(self, weight):\n                return weight * self.conv.weight\n\n\n        n = Net()\n        example_weight = torch.rand(1, 1, 3, 3)\n        example_forward_input = torch.rand(1, 1, 3, 3)\n\n        # Trace a specific method and construct `ScriptModule` with\n        # a single `forward` method\n        module = torch.jit.trace(n.forward, example_forward_input)\n\n        # Trace a module (implicitly traces `forward`) and construct a\n        # `ScriptModule` with a single `forward` method\n        module = torch.jit.trace(n, example_forward_input)\n\n        # Trace specific methods on a module (specified in `inputs`), constructs\n        # a `ScriptModule` with `forward` and `weighted_kernel_sum` methods\n        inputs = {\"forward\": example_forward_input, \"weighted_kernel_sum\": example_weight}\n        module = torch.jit.trace_module(n, inputs)\n\n    ",
      "arguments": [
        "mod",
        "inputs",
        "optimize",
        "check_trace",
        "check_inputs",
        "check_tolerance",
        "strict",
        "_force_outplace",
        "_module_class",
        "_compilation_unit",
        "example_inputs_is_kwarg",
        "_store_inputs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Trace a module and return an executable :class:`ScriptModule` that will be optimized using just-in-time compilation.\n\n    When a module is passed to :func:`torch.jit.trace <torch.jit.trace>`, only\n    the ``forward`` method is run and traced. With ``trace_module``, you can specify a dictionary of\n    method names to example inputs to trace (see the ``inputs``) argument below.\n\n    See :func:`torch.jit.trace <torch.jit.trace>` for more information on tracing.\n\n    Args:\n        mod (torch.nn.Module):  A ``torch.nn.Module`` containing methods whose names are\n                                specified in ``inputs``. The given methods will be compiled\n                                as a part of a single `ScriptModule`.\n        inputs (dict):  A dict containing sample inputs indexed by method names in ``mod``.\n                                The inputs will be passed to methods whose names correspond to inputs'\n                                keys while tracing.\n                                ``{ 'forward' : example_forward_input, 'method2': example_method2_input}``\n    Keyword arguments:\n        check_trace (``bool``, optional): Check if the same inputs run through\n                                      traced code produce the same outputs. Default: ``True``. You might want\n                                      to disable this if, for example, your network contains non-\n                                      deterministic ops or if you are sure that the network is correct despite\n                                      a checker failure.\n\n        check_inputs (list of dicts, optional): A list of dicts of input arguments that should be used\n                                                 to check the trace against what is expected. Each tuple\n                                                 is equivalent to a set of input arguments that would\n                                                 be specified in ``inputs``. For best results, pass in a\n                                                 set of checking inputs representative of the space of\n                                                 shapes and types of inputs you expect the network to see.\n                                                 If not specified, the original ``inputs`` are used for checking\n        check_tolerance (float, optional): Floating-point comparison tolerance to use in the checker procedure.\n                                           This can be used to relax the checker strictness in the event that\n                                           results diverge numerically for a known reason, such as operator fusion.\n        example_inputs_is_kwarg (``bool``, optional): This parameter indicate whether the example inputs is a pack\n                                           pack of keyword arguments. Default: ``False``.\n\n    Returns:\n        A :class:`ScriptModule` object with a single ``forward`` method containing the traced code.\n        When ``func`` is a ``torch.nn.Module``, the returned :class:`ScriptModule` will have the same set of\n        sub-modules and parameters as ``func``.\n\n    Example (tracing a module with multiple methods)::\n\n        import torch\n        import torch.nn as nn\n\n\n        class Net(nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n                self.conv = nn.Conv2d(1, 1, 3)\n\n            def forward(self, x):\n                return self.conv(x)\n\n            def weighted_kernel_sum(self, weight):\n                return weight * self.conv.weight\n\n\n        n = Net()\n        example_weight = torch.rand(1, 1, 3, 3)\n        example_forward_input = torch.rand(1, 1, 3, 3)\n\n        # Trace a specific method and construct `ScriptModule` with\n        # a single `forward` method\n        module = torch.jit.trace(n.forward, example_forward_input)\n\n        # Trace a module (implicitly traces `forward`) and construct a\n        # `ScriptModule` with a single `forward` method\n        module = torch.jit.trace(n, example_forward_input)\n\n        # Trace specific methods on a module (specified in `inputs`), constructs\n        # a `ScriptModule` with `forward` and `weighted_kernel_sum` methods\n        inputs = {\"forward\": example_forward_input, \"weighted_kernel_sum\": example_weight}\n        module = torch.jit.trace_module(n, inputs)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.unused",
      "signature": "torch.jit.unused(fn)",
      "doc": "\n    This decorator indicates to the compiler that a function or method should\n    be ignored and replaced with the raising of an exception. This allows you\n    to leave code in your model that is not yet TorchScript compatible and still\n    export your model.\n\n        Example (using ``@torch.jit.unused`` on a method)::\n\n            import torch\n            import torch.nn as nn\n\n\n            class MyModule(nn.Module):\n                def __init__(self, use_memory_efficient):\n                    super().__init__()\n                    self.use_memory_efficient = use_memory_efficient\n\n                @torch.jit.unused\n                def memory_efficient(self, x):\n                    import pdb\n\n                    pdb.set_trace()\n                    return x + 10\n\n                def forward(self, x):\n                    # Use not-yet-scriptable memory efficient mode\n                    if self.use_memory_efficient:\n                        return self.memory_efficient(x)\n                    else:\n                        return x + 10\n\n\n            m = torch.jit.script(MyModule(use_memory_efficient=False))\n            m.save(\"m.pt\")\n\n            m = torch.jit.script(MyModule(use_memory_efficient=True))\n            # exception raised\n            m(torch.rand(100))\n    ",
      "arguments": [
        "fn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This decorator indicates to the compiler that a function or method should\n    be ignored and replaced with the raising of an exception. This allows you\n    to leave code in your model that is not yet TorchScript compatible and still\n    export your model.\n\n        Example (using ``@torch.jit.unused`` on a method)::\n\n            import torch\n            import torch.nn as nn\n\n\n            class MyModule(nn.Module):\n                def __init__(self, use_memory_efficient):\n                    super().__init__()\n                    self.use_memory_efficient = use_memory_efficient\n\n                @torch.jit.unused\n                def memory_efficient(self, x):\n                    import pdb\n\n                    pdb.set_trace()\n                    return x + 10\n\n                def forward(self, x):\n                    # Use not-yet-scriptable memory efficient mode\n                    if self.use_memory_efficient:\n                        return self.memory_efficient(x)\n                    else:\n                        return x + 10\n\n\n            m = torch.jit.script(MyModule(use_memory_efficient=False))\n            m.save(\"m.pt\")\n\n            m = torch.jit.script(MyModule(use_memory_efficient=True))\n            # exception raised\n            m(torch.rand(100))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.jit.wait",
      "signature": "torch.jit.wait(future)",
      "doc": "\n    Force completion of a `torch.jit.Future[T]` asynchronous task, returning the result of the task.\n\n    See :func:`~fork` for docs and examples.\n    Args:\n        future (torch.jit.Future[T]): an asynchronous task reference, created through `torch.jit.fork`\n    Returns:\n        `T`: the return value of the completed task\n    ",
      "arguments": [
        "future"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Force completion of a `torch.jit.Future[T]` asynchronous task, returning the result of the task.\n\n    See :func:`~fork` for docs and examples.\n    Args:\n        future (torch.jit.Future[T]): an asynchronous task reference, created through `torch.jit.fork`\n    Returns:\n        `T`: the return value of the completed task\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "MASKED_OPERATIONS": [
    {
      "function": "torch.masked.amax",
      "signature": "torch.masked.amax(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[tuple[int, ...]] = None, *, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "amax(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns maximum of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of maximum operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``-inf``, ``0``, and ``-2147483648``, respectively.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in maximum computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of maximum operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.amax(input, 1, mask=mask)\n    tensor([                  -1, -9223372036854775808])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "amax(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns maximum of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of maximum operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``-inf``, ``0``, and ``-2147483648``, respectively.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in maximum computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of maximum operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.amax(input, 1, mask=mask)\n    tensor([                  -1, -9223372036854775808])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.amin",
      "signature": "torch.masked.amin(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[tuple[int, ...]] = None, *, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "amin(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns minimum of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of minimum operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``inf``, ``255``, and ``2147483647``, respectively.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in minimum computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of minimum operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.amin(input, 1, mask=mask)\n    tensor([                 -3, 9223372036854775807])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "amin(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns minimum of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of minimum operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``inf``, ``255``, and ``2147483647``, respectively.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in minimum computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of minimum operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.amin(input, 1, mask=mask)\n    tensor([                 -3, 9223372036854775807])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.argmax",
      "signature": "torch.masked.argmax(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[int] = None, *, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "argmax(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns argmax of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nThe identity value of argmax operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``-inf``, ``0``, and ``-2147483648``, respectively.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in argmax computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of argmax operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which argmax is computed.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.argmax(input, 1, mask=mask)\n    tensor([2, 0])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "argmax(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns argmax of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nThe identity value of argmax operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``-inf``, ``0``, and ``-2147483648``, respectively.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in argmax computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of argmax operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which argmax is computed.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.argmax(input, 1, mask=mask)\n    tensor([2, 0])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.argmin",
      "signature": "torch.masked.argmin(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[int] = None, *, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "argmin(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns argmin of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nThe identity value of argmin operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``inf``, ``255``, and ``2147483647``, respectively.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in argmin computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of argmin operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which argmin is computed.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.argmin(input, 1, mask=mask)\n    tensor([0, 0])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "argmin(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns argmin of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nThe identity value of argmin operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``inf``, ``255``, and ``2147483647``, respectively.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in argmin computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of argmin operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which argmin is computed.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.argmin(input, 1, mask=mask)\n    tensor([0, 0])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.as_masked_tensor",
      "signature": "torch.masked.as_masked_tensor(data: object, mask: object) -> torch.masked.maskedtensor.core.MaskedTensor",
      "doc": "",
      "arguments": [
        "data",
        "mask"
      ],
      "return_type": "<class 'torch.masked.maskedtensor.core.MaskedTensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.cumprod",
      "signature": "torch.masked.cumprod(input: torch.Tensor, dim: int, *, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "cumprod(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns cumulative_prod of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Cumsum of i-th element in ``x`` is\ndefined as ``prod(x[:i])``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\ncumulative_prod computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the cumulative_prod output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which cumulative_prod is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.cumprod(input, 1, mask=mask)\n    tensor([[-3., -3.,  3.],\n            [ 1.,  1.,  1.]])\n",
      "arguments": [
        "input",
        "dim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "cumprod(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns cumulative_prod of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Cumsum of i-th element in ``x`` is\ndefined as ``prod(x[:i])``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\ncumulative_prod computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the cumulative_prod output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which cumulative_prod is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.cumprod(input, 1, mask=mask)\n    tensor([[-3., -3.,  3.],\n            [ 1.,  1.,  1.]])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.cumsum",
      "signature": "torch.masked.cumsum(input: torch.Tensor, dim: int, *, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "cumsum(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns cumulative_sum of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Cumsum of i-th element in ``x`` is\ndefined as ``sum(x[:i])``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\ncumulative_sum computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the cumulative_sum output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which cumulative_sum is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.cumsum(input, 1, mask=mask)\n    tensor([[-3., -3., -4.],\n            [ 0.,  0.,  0.]])\n",
      "arguments": [
        "input",
        "dim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "cumsum(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns cumulative_sum of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Cumsum of i-th element in ``x`` is\ndefined as ``sum(x[:i])``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\ncumulative_sum computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the cumulative_sum output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which cumulative_sum is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.cumsum(input, 1, mask=mask)\n    tensor([[-3., -3., -4.],\n            [ 0.,  0.,  0.]])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.is_masked_tensor",
      "signature": "torch.masked.is_masked_tensor(obj: Any, /) -> typing_extensions.TypeIs[ForwardRef('MaskedTensor')]",
      "doc": "Returns True if the input is a MaskedTensor, else False\n\n    Args:\n        a: any input\n\n    Examples:\n\n        >>> # xdoctest: +SKIP\n        >>> from torch.masked import MaskedTensor\n        >>> data = torch.arange(6).reshape(2,3)\n        >>> mask = torch.tensor([[True, False, False], [True, True, False]])\n        >>> mt = MaskedTensor(data, mask)\n        >>> is_masked_tensor(mt)\n        True\n    ",
      "arguments": [
        "obj"
      ],
      "return_type": "typing_extensions.TypeIs[ForwardRef('MaskedTensor')]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns True if the input is a MaskedTensor, else False\n\n    Args:\n        a: any input\n\n    Examples:\n\n        >>> # xdoctest: +SKIP\n        >>> from torch.masked import MaskedTensor\n        >>> data = torch.arange(6).reshape(2,3)\n        >>> mask = torch.tensor([[True, False, False], [True, True, False]])\n        >>> mt = MaskedTensor(data, mask)\n        >>> is_masked_tensor(mt)\n        True\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.log_softmax",
      "signature": "torch.masked.log_softmax(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: int, *, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "log_softmax(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns log_softmax of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. LogSoftmax of i-th element in ``x`` is\ndefined as ``log(exp(x[i])/sum(exp(x)))``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\nlog_softmax computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the log_softmax output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which log_softmax is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.log_softmax(input, 1, mask=mask)\n    tensor([[-2.1269,    -inf, -0.1269],\n            [    nan,     nan,     nan]])\n",
      "arguments": [
        "input",
        "dim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "log_softmax(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns log_softmax of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. LogSoftmax of i-th element in ``x`` is\ndefined as ``log(exp(x[i])/sum(exp(x)))``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\nlog_softmax computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the log_softmax output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which log_softmax is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.log_softmax(input, 1, mask=mask)\n    tensor([[-2.1269,    -inf, -0.1269],\n            [    nan,     nan,     nan]])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.logaddexp",
      "signature": "torch.masked.logaddexp(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], other: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], *, dtype: Optional[int] = None, input_mask: Optional[torch.Tensor] = None, other_mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "logaddexp(input, other, *, dtype=None, input_mask=None, other_mask=None) -> Tensor\n\n    Returns logaddexp of all the elements in the :attr:`input` and the :attr:`other`\n    tensor. The :attr:`input` elements are masked out according to the boolean tensor\n    :attr:`input_mask` and the attr:`other` elements are masked out according to the boolean tensor\n    :attr:`other_mask`.\n\n    The shapes of a mask tensor and the tensor to be masked\n    don't need to match, but they must be :ref:`broadcastable\n    <broadcasting-semantics>` and the dimensionality of the mask\n    tensor must not be greater than of the tensor to be masked.\n\n    Args:\n        input (Tensor): the input tensor\n        other (Tensor): the second input tensor\n\n    Keyword args:\n        dtype (:class:`torch.dtype`, optional): the desired data type\n          of returned tensor.  If specified, the output tensor is\n          casted to :attr:`dtype` after the operation is\n          performed. Default: None.\n        input_mask (:class:`torch.Tensor`, optional): the boolean tensor\n          containing the binary mask of validity of :attr:`input` tensor elements.\n          Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n        other_mask (:class:`torch.Tensor`, optional): the boolean tensor\n          containing the binary mask of validity of :attr:`other` tensor elements.\n          Default: None that is equivalent to ``torch.ones(other.shape, dtype=torch.bool)``.\n\n    Example::\n\n        >>> input = torch.tensor([-100.0, -200, -300])\n        >>> input\n        tensor([-100., -200., -300.])\n        >>> other = torch.tensor([-1.0, -2, -3])\n        >>> other\n        tensor([-1., -2., -3.])\n        >>> mask = torch.tensor([True, False, True])\n        >>> mask\n        tensor([ True, False,  True])\n        >>> torch.masked._ops.logaddexp(input, other, input_mask=mask, other_mask=mask)\n        tensor([-1., -inf, -3.])",
      "arguments": [
        "input",
        "other",
        "dtype",
        "input_mask",
        "other_mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "logaddexp(input, other, *, dtype=None, input_mask=None, other_mask=None) -> Tensor\n\n    Returns logaddexp of all the elements in the :attr:`input` and the :attr:`other`\n    tensor. The :attr:`input` elements are masked out according to the boolean tensor\n    :attr:`input_mask` and the attr:`other` elements are masked out according to the boolean tensor\n    :attr:`other_mask`.\n\n    The shapes of a mask tensor and the tensor to be masked\n    don't need to match, but they must be :ref:`broadcastable\n    <broadcasting-semantics>` and the dimensionality of the mask\n    tensor must not be greater than of the tensor to be masked.\n\n    Args:\n        input (Tensor): the input tensor\n        other (Tensor): the second input tensor\n\n    Keyword args:\n        dtype (:class:`torch.dtype`, optional): the desired data type\n          of returned tensor.  If specified, the output tensor is\n          casted to :attr:`dtype` after the operation is\n          performed. Default: None.\n        input_mask (:class:`torch.Tensor`, optional): the boolean tensor\n          containing the binary mask of validity of :attr:`input` tensor elements.\n          Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n        other_mask (:class:`torch.Tensor`, optional): the boolean tensor\n          containing the binary mask of validity of :attr:`other` tensor elements.\n          Default: None that is equivalent to ``torch.ones(other.shape, dtype=torch.bool)``.\n\n    Example::\n\n        >>> input = torch.tensor([-100.0, -200, -300])\n        >>> input\n        tensor([-100., -200., -300.])\n        >>> other = torch.tensor([-1.0, -2, -3])\n        >>> other\n        tensor([-1., -2., -3.])\n        >>> mask = torch.tensor([True, False, True])\n        >>> mask\n        tensor([ True, False,  True])\n        >>> torch.masked._ops.logaddexp(input, other, input_mask=mask, other_mask=mask)\n        tensor([-1., -inf, -3.])",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.logsumexp",
      "signature": "torch.masked.logsumexp(input: torch.Tensor, dim: Optional[tuple[int, ...]] = None, *, keepdim: bool = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "logsumexp(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns logsumexp of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of logsumexp operation, which is used to start the reduction, is ``-2147483648``.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in logsumexp computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of logsumexp operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.logsumexp(input, 1, mask=mask)\n    tensor([                   0, -9223372036854775808])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "logsumexp(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns logsumexp of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of logsumexp operation, which is used to start the reduction, is ``-2147483648``.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in logsumexp computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of logsumexp operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.logsumexp(input, 1, mask=mask)\n    tensor([                   0, -9223372036854775808])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.masked_tensor",
      "signature": "torch.masked.masked_tensor(data: object, mask: object, requires_grad: bool = False) -> torch.masked.maskedtensor.core.MaskedTensor",
      "doc": "",
      "arguments": [
        "data",
        "mask",
        "requires_grad"
      ],
      "return_type": "<class 'torch.masked.maskedtensor.core.MaskedTensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.mean",
      "signature": "torch.masked.mean(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[tuple[int, ...]] = None, *, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "mean(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns mean of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nBy definition, the identity value of a mean operation is the mean\nvalue of the tensor. If all elements of the input tensor along given\ndimension(s) :attr:`dim` are masked-out, the identity value of the\nmean is undefined.  Due to this ambiguity, the elements of output\ntensor with strided layout, that correspond to fully masked-out\nelements, have ``nan`` values.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in mean computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of mean operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.mean(input, 1, mask=mask)\n    tensor([-2., nan])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "mean(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns mean of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nBy definition, the identity value of a mean operation is the mean\nvalue of the tensor. If all elements of the input tensor along given\ndimension(s) :attr:`dim` are masked-out, the identity value of the\nmean is undefined.  Due to this ambiguity, the elements of output\ntensor with strided layout, that correspond to fully masked-out\nelements, have ``nan`` values.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in mean computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of mean operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.mean(input, 1, mask=mask)\n    tensor([-2., nan])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.median",
      "signature": "torch.masked.median(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: int = -1, *, keepdim: bool = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "median(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns median of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nBy definition, the identity value of a median operation is the median\nvalue of the tensor. If all elements of the input tensor along given\ndimension(s) :attr:`dim` are masked-out, the identity value of the\nmedian is undefined.  Due to this ambiguity, the elements of output\ntensor with strided layout, that correspond to fully masked-out\nelements, have ``nan`` values.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in median computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of median operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which median is computed.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.median(input, 1, mask=mask)\n    tensor([-3., nan])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "median(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns median of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nBy definition, the identity value of a median operation is the median\nvalue of the tensor. If all elements of the input tensor along given\ndimension(s) :attr:`dim` are masked-out, the identity value of the\nmedian is undefined.  Due to this ambiguity, the elements of output\ntensor with strided layout, that correspond to fully masked-out\nelements, have ``nan`` values.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in median computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of median operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which median is computed.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.median(input, 1, mask=mask)\n    tensor([-3., nan])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.norm",
      "signature": "torch.masked.norm(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], ord: Optional[float] = 2.0, dim: Optional[tuple[int, ...]] = None, *, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "norm(input, ord, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns norm of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of norm operation, which is used to start the\nreduction, is ``0.0``, except for ``ord=-inf`` it is\n``inf``.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in norm computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of norm operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    ord (int, float, optional): the order of vector norm. Default: 2.\n      See :func:`torch.linalg.vector_norm` for a list of supported norms.\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.norm(input, 2.0, 1, mask=mask)\n    tensor([3.1623, 0.0000])\n",
      "arguments": [
        "input",
        "ord",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "norm(input, ord, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns norm of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of norm operation, which is used to start the\nreduction, is ``0.0``, except for ``ord=-inf`` it is\n``inf``.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in norm computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of norm operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    ord (int, float, optional): the order of vector norm. Default: 2.\n      See :func:`torch.linalg.vector_norm` for a list of supported norms.\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.norm(input, 2.0, 1, mask=mask)\n    tensor([3.1623, 0.0000])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.normalize",
      "signature": "torch.masked.normalize(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], ord: float, dim: int, *, eps: float = 1e-12, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "normalize(input, ord, dim, *, eps=1e-12, dtype=None, mask=None) -> Tensor\n\nReturns normalize of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Normalize of i-th element in ``x`` is\ndefined as ``x[i]/max(norm(x, p), eps)``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\nnormalize computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the normalize output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    ord (int, float): the order of vector norm. Default: 2.\n      See :func:`torch.linalg.vector_norm` for a list of supported norms.\n    dim (int): the dimension along which normalize is computed.\n\nKeyword args:\n    eps (float, optional): small value to avoid division by zero. Default: 1e-12.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.normalize(input, 2.0, 1, mask=mask)\n    tensor([[-0.9487,  0.0000, -0.3162],\n            [ 0.0000,  0.0000,  0.0000]])\n",
      "arguments": [
        "input",
        "ord",
        "dim",
        "eps",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "normalize(input, ord, dim, *, eps=1e-12, dtype=None, mask=None) -> Tensor\n\nReturns normalize of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Normalize of i-th element in ``x`` is\ndefined as ``x[i]/max(norm(x, p), eps)``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\nnormalize computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the normalize output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    ord (int, float): the order of vector norm. Default: 2.\n      See :func:`torch.linalg.vector_norm` for a list of supported norms.\n    dim (int): the dimension along which normalize is computed.\n\nKeyword args:\n    eps (float, optional): small value to avoid division by zero. Default: 1e-12.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.normalize(input, 2.0, 1, mask=mask)\n    tensor([[-0.9487,  0.0000, -0.3162],\n            [ 0.0000,  0.0000,  0.0000]])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.prod",
      "signature": "torch.masked.prod(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[tuple[int, ...]] = None, *, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "prod(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns product of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of product operation, which is used to start the reduction, is ``1``.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in product computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of product operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.prod(input, 1, mask=mask)\n    tensor([3, 1])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "prod(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns product of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of product operation, which is used to start the reduction, is ``1``.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in product computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of product operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.prod(input, 1, mask=mask)\n    tensor([3, 1])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.softmax",
      "signature": "torch.masked.softmax(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: int, *, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "softmax(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns softmax of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Softmax of i-th element in ``x`` is\ndefined as ``exp(x[i])/sum(exp(x))``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\nsoftmax computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the softmax output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which softmax is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.softmax(input, 1, mask=mask)\n    tensor([[0.1192, 0.0000, 0.8808],\n            [   nan,    nan,    nan]])\n",
      "arguments": [
        "input",
        "dim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "softmax(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns softmax of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Softmax of i-th element in ``x`` is\ndefined as ``exp(x[i])/sum(exp(x))``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\nsoftmax computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the softmax output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which softmax is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.softmax(input, 1, mask=mask)\n    tensor([[0.1192, 0.0000, 0.8808],\n            [   nan,    nan,    nan]])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.softmin",
      "signature": "torch.masked.softmin(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: int, *, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "softmin(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns softmin of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Softmin of i-th element in ``x`` is\ndefined as ``exp(-x[i])/sum(exp(-x))``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\nsoftmin computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the softmin output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which softmin is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.softmin(input, 1, mask=mask)\n    tensor([[0.8808, 0.0000, 0.1192],\n            [   nan,    nan,    nan]])\n",
      "arguments": [
        "input",
        "dim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "softmin(input, dim, *, dtype=None, mask=None) -> Tensor\n\nReturns softmin of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\nLet ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Softmin of i-th element in ``x`` is\ndefined as ``exp(-x[i])/sum(exp(-x))``.\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\nsoftmin computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the softmin output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int): the dimension along which softmin is computed.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3., -2., -1.], [ 0., 1., 2.]])\n    >>> input\n    tensor([[-3., -2., -1.],\n            [ 0.,  1.,  2.]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.softmin(input, 1, mask=mask)\n    tensor([[0.8808, 0.0000, 0.1192],\n            [   nan,    nan,    nan]])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.std",
      "signature": "torch.masked.std(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[tuple[int, ...]] = None, unbiased: Optional[bool] = None, *, correction: Optional[int] = None, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "std(input, dim, unbiased, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns standard_deviation of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nThe identity value of sample standard deviation operation is undefined. The\nelements of output tensor with strided layout, that correspond to\nfully masked-out elements, have ``nan`` values.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in standard_deviation computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of standard_deviation operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n    unbiased (bool): when True, use Bessel's correction, otherwise, compute\n      the uncorrected sample variance.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.std(input, 1, False, mask=mask)\n    tensor([1., nan])\n",
      "arguments": [
        "input",
        "dim",
        "unbiased",
        "correction",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "std(input, dim, unbiased, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns standard_deviation of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nThe identity value of sample standard deviation operation is undefined. The\nelements of output tensor with strided layout, that correspond to\nfully masked-out elements, have ``nan`` values.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in standard_deviation computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of standard_deviation operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n    unbiased (bool): when True, use Bessel's correction, otherwise, compute\n      the uncorrected sample variance.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.std(input, 1, False, mask=mask)\n    tensor([1., nan])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.sum",
      "signature": "torch.masked.sum(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[tuple[int, ...]] = None, *, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "sum(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns sum of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of sum operation, which is used to start the reduction, is ``0``.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in sum computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of sum operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.sum(input, 1, mask=mask)\n    tensor([-4,  0])\n",
      "arguments": [
        "input",
        "dim",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "sum(input, dim, *, keepdim=False, dtype=None, mask=None) -> Tensor\n\nReturns sum of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\n\nThe identity value of sum operation, which is used to start the reduction, is ``0``.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in sum computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of sum operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\n\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.sum(input, 1, mask=mask)\n    tensor([-4,  0])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.masked.var",
      "signature": "torch.masked.var(input: Union[torch.Tensor, torch.masked.maskedtensor.core.MaskedTensor], dim: Optional[tuple[int, ...]] = None, unbiased: Optional[bool] = None, *, correction: Union[int, float, NoneType] = None, keepdim: Optional[bool] = False, dtype: Optional[int] = None, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
      "doc": "var(input, dim, unbiased, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns variance of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nThe identity value of sample variance operation is undefined. The\nelements of output tensor with strided layout, that correspond to\nfully masked-out elements, have ``nan`` values.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in variance computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of variance operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n    unbiased (bool): when True, use Bessel's correction, otherwise, compute\n      the uncorrected sample variance.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.var(input, 1, False, mask=mask)\n    tensor([1., nan])\n",
      "arguments": [
        "input",
        "dim",
        "unbiased",
        "correction",
        "keepdim",
        "dtype",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "var(input, dim, unbiased, *, keepdim=False, dtype=None, mask=None) -> Tensor\nReturns variance of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.\nThe identity value of sample variance operation is undefined. The\nelements of output tensor with strided layout, that correspond to\nfully masked-out elements, have ``nan`` values.\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the \"validity\" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in variance computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of variance operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon't need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n      Default: None that is equivalent to ``tuple(range(input.ndim))``.\n    unbiased (bool): when True, use Bessel's correction, otherwise, compute\n      the uncorrected sample variance.\n\nKeyword args:\n    keepdim (bool, optional): whether the output tensor has\n      :attr:`dim` retained or not. Default: False.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n      of returned tensor.  If specified, the input tensor is\n      casted to :attr:`dtype` before the operation is\n      performed. Default: None.\n    mask (:class:`torch.Tensor`, optional): the boolean tensor\n      containing the binary mask of validity of input tensor\n      elements.\n      Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.\nExample::\n\n    >>> input = tensor([[-3, -2, -1], [ 0, 1, 2]])\n    >>> input\n    tensor([[-3, -2, -1],\n            [ 0,  1,  2]])\n    >>> mask = tensor([[ True, False, True], [False, False, False]])\n    >>> mask\n    tensor([[ True, False,  True],\n            [False, False, False]])\n    >>> torch.masked._ops.var(input, 1, False, mask=mask)\n    tensor([1., nan])\n",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "MPS_OPERATIONS": [
    {
      "function": "torch.mps.compile_shader",
      "signature": "torch.mps.compile_shader(source: str)",
      "doc": "Compiles compute shader from source and allows one to invoke kernels\n    defined there from the comfort of Python runtime\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n        >>> lib = torch.mps.compile_shader(\n        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n        ...  )\n        >>> x = torch.zeros(16, device=\"mps\")\n        >>> lib.full(x, 3.14)\n    ",
      "arguments": [
        "source"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Compiles compute shader from source and allows one to invoke kernels\n    defined there from the comfort of Python runtime\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n        >>> lib = torch.mps.compile_shader(\n        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n        ...  )\n        >>> x = torch.zeros(16, device=\"mps\")\n        >>> lib.full(x, 3.14)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.current_allocated_memory",
      "signature": "torch.mps.current_allocated_memory() -> int",
      "doc": "Returns the current GPU memory occupied by tensors in bytes.\n\n    .. note::\n       The returned size does not include cached allocations in\n       memory pools of MPSAllocator.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the current GPU memory occupied by tensors in bytes.\n\n    .. note::\n       The returned size does not include cached allocations in\n       memory pools of MPSAllocator.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.device_count",
      "signature": "torch.mps.device_count() -> int",
      "doc": "Returns the number of available MPS devices.",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the number of available MPS devices.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.driver_allocated_memory",
      "signature": "torch.mps.driver_allocated_memory() -> int",
      "doc": "Returns total GPU memory allocated by Metal driver for the process in bytes.\n\n    .. note::\n       The returned size includes cached allocations in MPSAllocator pools\n       as well as allocations from MPS/MPSGraph frameworks.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns total GPU memory allocated by Metal driver for the process in bytes.\n\n    .. note::\n       The returned size includes cached allocations in MPSAllocator pools\n       as well as allocations from MPS/MPSGraph frameworks.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.empty_cache",
      "signature": "torch.mps.empty_cache() -> None",
      "doc": "Releases all unoccupied cached memory currently held by the caching\n    allocator so that those can be used in other GPU applications.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Releases all unoccupied cached memory currently held by the caching\n    allocator so that those can be used in other GPU applications.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.get_rng_state",
      "signature": "torch.mps.get_rng_state(device: Union[int, str, torch.device] = 'mps') -> torch.Tensor",
      "doc": "Returns the random number generator state as a ByteTensor.\n\n    Args:\n        device (torch.device or int, optional): The device to return the RNG state of.\n            Default: ``'mps'`` (i.e., ``torch.device('mps')``, the current MPS device).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the random number generator state as a ByteTensor.\n\n    Args:\n        device (torch.device or int, optional): The device to return the RNG state of.\n            Default: ``'mps'`` (i.e., ``torch.device('mps')``, the current MPS device).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.is_available",
      "signature": "torch.mps.is_available() -> bool",
      "doc": "",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.manual_seed",
      "signature": "torch.mps.manual_seed(seed: int) -> None",
      "doc": "Sets the seed for generating random numbers.\n\n    Args:\n        seed (int): The desired seed.\n    ",
      "arguments": [
        "seed"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the seed for generating random numbers.\n\n    Args:\n        seed (int): The desired seed.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.recommended_max_memory",
      "signature": "torch.mps.recommended_max_memory() -> int",
      "doc": "Returns recommended max Working set size for GPU memory in bytes.\n\n    .. note::\n       Recommended max working set size for Metal.\n       returned from device.recommendedMaxWorkingSetSize.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns recommended max Working set size for GPU memory in bytes.\n\n    .. note::\n       Recommended max working set size for Metal.\n       returned from device.recommendedMaxWorkingSetSize.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.seed",
      "signature": "torch.mps.seed() -> None",
      "doc": "Sets the seed for generating random numbers to a random number.",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the seed for generating random numbers to a random number.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.set_per_process_memory_fraction",
      "signature": "torch.mps.set_per_process_memory_fraction(fraction) -> None",
      "doc": "Set memory fraction for limiting process's memory allocation on MPS device.\n    The allowed value equals the fraction multiplied by recommended maximum device memory\n    (obtained from Metal API device.recommendedMaxWorkingSetSize).\n    If trying to allocate more than the allowed value in a process, it will raise an out of\n    memory error in allocator.\n\n    Args:\n        fraction(float): Range: 0~2. Allowed memory equals total_memory * fraction.\n\n    .. note::\n       Passing 0 to fraction means unlimited allocations\n       (may cause system failure if out of memory).\n       Passing fraction greater than 1.0 allows limits beyond the value\n       returned from device.recommendedMaxWorkingSetSize.\n    ",
      "arguments": [
        "fraction"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set memory fraction for limiting process's memory allocation on MPS device.\n    The allowed value equals the fraction multiplied by recommended maximum device memory\n    (obtained from Metal API device.recommendedMaxWorkingSetSize).\n    If trying to allocate more than the allowed value in a process, it will raise an out of\n    memory error in allocator.\n\n    Args:\n        fraction(float): Range: 0~2. Allowed memory equals total_memory * fraction.\n\n    .. note::\n       Passing 0 to fraction means unlimited allocations\n       (may cause system failure if out of memory).\n       Passing fraction greater than 1.0 allows limits beyond the value\n       returned from device.recommendedMaxWorkingSetSize.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.set_rng_state",
      "signature": "torch.mps.set_rng_state(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'mps') -> None",
      "doc": "Sets the random number generator state.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n        device (torch.device or int, optional): The device to set the RNG state.\n            Default: ``'mps'`` (i.e., ``torch.device('mps')``, the current MPS device).\n    ",
      "arguments": [
        "new_state",
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the random number generator state.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n        device (torch.device or int, optional): The device to set the RNG state.\n            Default: ``'mps'`` (i.e., ``torch.device('mps')``, the current MPS device).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mps.synchronize",
      "signature": "torch.mps.synchronize() -> None",
      "doc": "Waits for all kernels in all streams on a MPS device to complete.",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Waits for all kernels in all streams on a MPS device to complete.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "MTIA_OPERATIONS": [
    {
      "function": "torch.mtia.classproperty",
      "signature": "torch.mtia.classproperty(func)",
      "doc": "",
      "arguments": [
        "func"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.current_device",
      "signature": "torch.mtia.current_device() -> int",
      "doc": "Return the index of a currently selected device.",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the index of a currently selected device.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.current_stream",
      "signature": "torch.mtia.current_stream(device: Union[torch.device, str, int, NoneType] = None) -> torch.Stream",
      "doc": "Return the currently selected :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the currently selected :class:`Stream` for the current device, given\n            by :func:`~torch.mtia.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the currently selected :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the currently selected :class:`Stream` for the current device, given\n            by :func:`~torch.mtia.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.default_stream",
      "signature": "torch.mtia.default_stream(device: Union[torch.device, str, int, NoneType] = None) -> torch.Stream",
      "doc": "Return the default :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the default :class:`Stream` for the current device, given by\n            :func:`~torch.mtia.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the default :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the default :class:`Stream` for the current device, given by\n            :func:`~torch.mtia.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.device_count",
      "signature": "torch.mtia.device_count() -> int",
      "doc": "Return the number of MTIA devices available.",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the number of MTIA devices available.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.empty_cache",
      "signature": "torch.mtia.empty_cache() -> None",
      "doc": "Empty the MTIA device cache.",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Empty the MTIA device cache.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.get_device_capability",
      "signature": "torch.mtia.get_device_capability(device: Union[torch.device, str, int, NoneType] = None) -> tuple[int, int]",
      "doc": "Return capability of a given device as a tuple of (major version, minor version).\n\n    Args:\n        device (torch.device or int, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "tuple[int, int]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return capability of a given device as a tuple of (major version, minor version).\n\n    Args:\n        device (torch.device or int, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.get_rng_state",
      "signature": "torch.mtia.get_rng_state(device: Union[int, str, torch.device] = 'mtia') -> torch.Tensor",
      "doc": "Returns the random number generator state as a ByteTensor.\n\n    Args:\n        device (torch.device or int, optional): The device to return the RNG state of.\n            Default: ``'mtia'`` (i.e., ``torch.device('mtia')``, the current mtia device).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns the random number generator state as a ByteTensor.\n\n    Args:\n        device (torch.device or int, optional): The device to return the RNG state of.\n            Default: ``'mtia'`` (i.e., ``torch.device('mtia')``, the current mtia device).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.init",
      "signature": "torch.mtia.init()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.is_available",
      "signature": "torch.mtia.is_available() -> bool",
      "doc": "Return true if MTIA device is available",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return true if MTIA device is available",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.is_initialized",
      "signature": "torch.mtia.is_initialized()",
      "doc": "Return whether PyTorch's MTIA state has been initialized.",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return whether PyTorch's MTIA state has been initialized.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.max_memory_allocated",
      "signature": "torch.mtia.max_memory_allocated(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the maximum memory allocated in bytes for a given device.\n\n    Args:\n        device (torch.device, str, or int, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the maximum memory allocated in bytes for a given device.\n\n    Args:\n        device (torch.device, str, or int, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.memory_stats",
      "signature": "torch.mtia.memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> dict[str, typing.Any]",
      "doc": "Return a dictionary of MTIA memory allocator statistics for a given device.\n\n    Args:\n        device (torch.device, str, or int, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a dictionary of MTIA memory allocator statistics for a given device.\n\n    Args:\n        device (torch.device, str, or int, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.record_memory_history",
      "signature": "torch.mtia.record_memory_history(enabled: Optional[str] = 'all', stacks: str = 'python', max_entries: int = 0) -> None",
      "doc": "Enable/Disable the memory profiler on MTIA allocator\n\n    Args:\n        enabled (all or state, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n\n        stacks (\"python\" or \"cpp\", optional). Select the stack trace to record.\n\n        max_entries (int, optional). Maximum number of entries to record.\n    ",
      "arguments": [
        "enabled",
        "stacks",
        "max_entries"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Enable/Disable the memory profiler on MTIA allocator\n\n    Args:\n        enabled (all or state, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n\n        stacks (\"python\" or \"cpp\", optional). Select the stack trace to record.\n\n        max_entries (int, optional). Maximum number of entries to record.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.reset_peak_memory_stats",
      "signature": "torch.mtia.reset_peak_memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Reset the peak memory stats for a given device.\n\n\n    Args:\n        device (torch.device, str, or int, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the peak memory stats for a given device.\n\n\n    Args:\n        device (torch.device, str, or int, optional) selected device. Returns\n            statistics for the current device, given by current_device(),\n            if device is None (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.set_device",
      "signature": "torch.mtia.set_device(device: Union[torch.device, str, int]) -> None",
      "doc": "Set the current device.\n\n    Args:\n        device (torch.device or int): selected device. This function is a no-op\n            if this argument is negative.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current device.\n\n    Args:\n        device (torch.device or int): selected device. This function is a no-op\n            if this argument is negative.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.set_rng_state",
      "signature": "torch.mtia.set_rng_state(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'mtia') -> None",
      "doc": "Sets the random number generator state.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n        device (torch.device or int, optional): The device to set the RNG state.\n            Default: ``'mtia'`` (i.e., ``torch.device('mtia')``, the current mtia device).\n    ",
      "arguments": [
        "new_state",
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Sets the random number generator state.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n        device (torch.device or int, optional): The device to set the RNG state.\n            Default: ``'mtia'`` (i.e., ``torch.device('mtia')``, the current mtia device).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.set_stream",
      "signature": "torch.mtia.set_stream(stream: torch.Stream)",
      "doc": "Set the current stream.This is a wrapper API to set the stream.\n        Usage of this function is discouraged in favor of the ``stream``\n        context manager.\n\n    Args:\n        stream (Stream): selected stream. This function is a no-op\n            if this argument is ``None``.\n    ",
      "arguments": [
        "stream"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current stream.This is a wrapper API to set the stream.\n        Usage of this function is discouraged in favor of the ``stream``\n        context manager.\n\n    Args:\n        stream (Stream): selected stream. This function is a no-op\n            if this argument is ``None``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.snapshot",
      "signature": "torch.mtia.snapshot() -> dict[str, typing.Any]",
      "doc": "Return a dictionary of MTIA memory allocator history",
      "arguments": [],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a dictionary of MTIA memory allocator history",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.stream",
      "signature": "torch.mtia.stream(stream: Optional[ForwardRef('torch.mtia.Stream')]) -> torch.mtia.StreamContext",
      "doc": "Wrap around the Context-manager StreamContext that selects a given stream.\n\n    Arguments:\n        stream (Stream): selected stream. This manager is a no-op if it's\n            ``None``.\n    .. note:: In eager mode stream is of type Stream class while in JIT it doesn't support torch.mtia.stream\n    ",
      "arguments": [
        "stream"
      ],
      "return_type": "<class 'torch.mtia.StreamContext'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Wrap around the Context-manager StreamContext that selects a given stream.\n\n    Arguments:\n        stream (Stream): selected stream. This manager is a no-op if it's\n            ``None``.\n    .. note:: In eager mode stream is of type Stream class while in JIT it doesn't support torch.mtia.stream\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.mtia.synchronize",
      "signature": "torch.mtia.synchronize(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Waits for all jobs in all streams on a MTIA device to complete.",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Waits for all jobs in all streams on a MTIA device to complete.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "MULTIPROCESSING": [
    {
      "function": "torch.multiprocessing.active_children",
      "signature": "torch.multiprocessing.active_children()",
      "doc": "\n    Return list of process objects corresponding to live child processes\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return list of process objects corresponding to live child processes\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.multiprocessing.current_process",
      "signature": "torch.multiprocessing.current_process()",
      "doc": "\n    Return process object representing the current process\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return process object representing the current process\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.multiprocessing.get_all_sharing_strategies",
      "signature": "torch.multiprocessing.get_all_sharing_strategies()",
      "doc": "Return a set of sharing strategies supported on a current system.",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a set of sharing strategies supported on a current system.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.multiprocessing.get_sharing_strategy",
      "signature": "torch.multiprocessing.get_sharing_strategy()",
      "doc": "Return the current strategy for sharing CPU tensors.",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the current strategy for sharing CPU tensors.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.multiprocessing.init_reductions",
      "signature": "torch.multiprocessing.init_reductions()",
      "doc": "",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.multiprocessing.parent_process",
      "signature": "torch.multiprocessing.parent_process()",
      "doc": "\n    Return process object representing the parent process\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return process object representing the parent process\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.multiprocessing.set_sharing_strategy",
      "signature": "torch.multiprocessing.set_sharing_strategy(new_strategy)",
      "doc": "Set the strategy for sharing CPU tensors.\n\n    Args:\n        new_strategy (str): Name of the selected strategy. Should be one of\n            the values returned by :func:`get_all_sharing_strategies()`.\n    ",
      "arguments": [
        "new_strategy"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the strategy for sharing CPU tensors.\n\n    Args:\n        new_strategy (str): Name of the selected strategy. Should be one of\n            the values returned by :func:`get_all_sharing_strategies()`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.multiprocessing.spawn",
      "signature": "torch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn')",
      "doc": "Spawns ``nprocs`` processes that run ``fn`` with ``args``.\n\n    If one of the processes exits with a non-zero exit status, the\n    remaining processes are killed and an exception is raised with the\n    cause of termination. In the case an exception was caught in the\n    child process, it is forwarded and its traceback is included in\n    the exception raised in the parent process.\n\n    Args:\n        fn (function): Function is called as the entrypoint of the\n            spawned process. This function must be defined at the top\n            level of a module so it can be pickled and spawned. This\n            is a requirement imposed by multiprocessing.\n\n            The function is called as ``fn(i, *args)``, where ``i`` is\n            the process index and ``args`` is the passed through tuple\n            of arguments.\n\n        args (tuple): Arguments passed to ``fn``.\n        nprocs (int): Number of processes to spawn.\n        join (bool): Perform a blocking join on all processes.\n        daemon (bool): The spawned processes' daemon flag. If set to True,\n                       daemonic processes will be created.\n        start_method (str): (deprecated) this method will always use ``spawn``\n                               as the start method. To use a different start method\n                               use ``start_processes()``.\n\n    Returns:\n        None if ``join`` is ``True``,\n        :class:`~ProcessContext` if ``join`` is ``False``\n\n    ",
      "arguments": [
        "fn",
        "args",
        "nprocs",
        "join",
        "daemon",
        "start_method"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Spawns ``nprocs`` processes that run ``fn`` with ``args``.\n\n    If one of the processes exits with a non-zero exit status, the\n    remaining processes are killed and an exception is raised with the\n    cause of termination. In the case an exception was caught in the\n    child process, it is forwarded and its traceback is included in\n    the exception raised in the parent process.\n\n    Args:\n        fn (function): Function is called as the entrypoint of the\n            spawned process. This function must be defined at the top\n            level of a module so it can be pickled and spawned. This\n            is a requirement imposed by multiprocessing.\n\n            The function is called as ``fn(i, *args)``, where ``i`` is\n            the process index and ``args`` is the passed through tuple\n            of arguments.\n\n        args (tuple): Arguments passed to ``fn``.\n        nprocs (int): Number of processes to spawn.\n        join (bool): Perform a blocking join on all processes.\n        daemon (bool): The spawned processes' daemon flag. If set to True,\n                       daemonic processes will be created.\n        start_method (str): (deprecated) this method will always use ``spawn``\n                               as the start method. To use a different start method\n                               use ``start_processes()``.\n\n    Returns:\n        None if ``join`` is ``True``,\n        :class:`~ProcessContext` if ``join`` is ``False``\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.multiprocessing.start_processes",
      "signature": "torch.multiprocessing.start_processes(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn')",
      "doc": "",
      "arguments": [
        "fn",
        "args",
        "nprocs",
        "join",
        "daemon",
        "start_method"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "NESTED_TENSORS": [
    {
      "function": "torch.nested.as_nested_tensor",
      "signature": "torch.nested.as_nested_tensor(ts: Union[torch.Tensor, list[torch.Tensor], tuple[torch.Tensor, ...]], dtype: Optional[torch.dtype] = None, device: Optional[torch.device] = None, layout=None) -> torch.Tensor",
      "doc": "\n    Constructs a nested tensor preserving autograd history from a tensor or a list / tuple of\n    tensors.\n\n    If a nested tensor is passed, it will be returned directly unless the device / dtype / layout\n    differ. Note that converting device / dtype will result in a copy, while converting layout\n    is not currently supported by this function.\n\n    If a non-nested tensor is passed, it is treated as a batch of constituents of consistent size.\n    A copy will be incurred if the passed device / dtype differ from those of the input OR if\n    the input is non-contiguous. Otherwise, the input's storage will be used directly.\n\n    If a tensor list is provided, tensors in the list are always copied during construction of\n    the nested tensor.\n\n    Args:\n        ts (Tensor or List[Tensor] or Tuple[Tensor]): a tensor to treat as a nested tensor OR a\n            list / tuple of tensors with the same ndim\n\n    Keyword arguments:\n        dtype (:class:`torch.dtype`, optional): the desired type of returned nested tensor.\n            Default: if None, same :class:`torch.dtype` as leftmost tensor in the list.\n        device (:class:`torch.device`, optional): the desired device of returned nested tensor.\n            Default: if None, same :class:`torch.device` as leftmost tensor in the list\n        layout (:class:`torch.layout`, optional): the desired layout of returned nested tensor.\n            Only strided and jagged layouts are supported. Default: if None, the strided layout.\n\n    Example::\n\n        >>> a = torch.arange(3, dtype=torch.float, requires_grad=True)\n        >>> b = torch.arange(5, dtype=torch.float, requires_grad=True)\n        >>> nt = torch.nested.as_nested_tensor([a, b])\n        >>> nt.is_leaf\n        False\n        >>> fake_grad = torch.nested.nested_tensor([torch.ones_like(a), torch.zeros_like(b)])\n        >>> nt.backward(fake_grad)\n        >>> a.grad\n        tensor([1., 1., 1.])\n        >>> b.grad\n        tensor([0., 0., 0., 0., 0.])\n        >>> c = torch.randn(3, 5, requires_grad=True)\n        >>> nt2 = torch.nested.as_nested_tensor(c)\n    ",
      "arguments": [
        "ts",
        "dtype",
        "device",
        "layout"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Constructs a nested tensor preserving autograd history from a tensor or a list / tuple of\n    tensors.\n\n    If a nested tensor is passed, it will be returned directly unless the device / dtype / layout\n    differ. Note that converting device / dtype will result in a copy, while converting layout\n    is not currently supported by this function.\n\n    If a non-nested tensor is passed, it is treated as a batch of constituents of consistent size.\n    A copy will be incurred if the passed device / dtype differ from those of the input OR if\n    the input is non-contiguous. Otherwise, the input's storage will be used directly.\n\n    If a tensor list is provided, tensors in the list are always copied during construction of\n    the nested tensor.\n\n    Args:\n        ts (Tensor or List[Tensor] or Tuple[Tensor]): a tensor to treat as a nested tensor OR a\n            list / tuple of tensors with the same ndim\n\n    Keyword arguments:\n        dtype (:class:`torch.dtype`, optional): the desired type of returned nested tensor.\n            Default: if None, same :class:`torch.dtype` as leftmost tensor in the list.\n        device (:class:`torch.device`, optional): the desired device of returned nested tensor.\n            Default: if None, same :class:`torch.device` as leftmost tensor in the list\n        layout (:class:`torch.layout`, optional): the desired layout of returned nested tensor.\n            Only strided and jagged layouts are supported. Default: if None, the strided layout.\n\n    Example::\n\n        >>> a = torch.arange(3, dtype=torch.float, requires_grad=True)\n        >>> b = torch.arange(5, dtype=torch.float, requires_grad=True)\n        >>> nt = torch.nested.as_nested_tensor([a, b])\n        >>> nt.is_leaf\n        False\n        >>> fake_grad = torch.nested.nested_tensor([torch.ones_like(a), torch.zeros_like(b)])\n        >>> nt.backward(fake_grad)\n        >>> a.grad\n        tensor([1., 1., 1.])\n        >>> b.grad\n        tensor([0., 0., 0., 0., 0.])\n        >>> c = torch.randn(3, 5, requires_grad=True)\n        >>> nt2 = torch.nested.as_nested_tensor(c)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.nested.masked_select",
      "signature": "torch.nested.masked_select(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor",
      "doc": "\n    Constructs a nested tensor given a strided tensor input and a strided mask, the resulting jagged layout nested tensor\n    will have values retain values where the mask is equal to True. The dimensionality of the mask is preserved and is\n    represented with the offsets, this is unlike :func:`masked_select` where the output is collapsed to a 1D tensor.\n\n    Args:\n    tensor (:class:`torch.Tensor`): a strided tensor from which the jagged layout nested tensor is constructed from.\n    mask (:class:`torch.Tensor`): a strided mask tensor which is applied to the tensor input\n\n    Example::\n\n        >>> tensor = torch.randn(3, 3)\n        >>> mask = torch.tensor([[False, False, True], [True, False, True], [False, False, True]])\n        >>> nt = torch.nested.masked_select(tensor, mask)\n        >>> nt.shape\n        torch.Size([3, j4])\n        >>> # Length of each item in the batch:\n        >>> nt.offsets().diff()\n        tensor([1, 2, 1])\n\n        >>> tensor = torch.randn(6, 5)\n        >>> mask = torch.tensor([False])\n        >>> nt = torch.nested.masked_select(tensor, mask)\n        >>> nt.shape\n        torch.Size([6, j5])\n        >>> # Length of each item in the batch:\n        >>> nt.offsets().diff()\n        tensor([0, 0, 0, 0, 0, 0])\n    ",
      "arguments": [
        "tensor",
        "mask"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Constructs a nested tensor given a strided tensor input and a strided mask, the resulting jagged layout nested tensor\n    will have values retain values where the mask is equal to True. The dimensionality of the mask is preserved and is\n    represented with the offsets, this is unlike :func:`masked_select` where the output is collapsed to a 1D tensor.\n\n    Args:\n    tensor (:class:`torch.Tensor`): a strided tensor from which the jagged layout nested tensor is constructed from.\n    mask (:class:`torch.Tensor`): a strided mask tensor which is applied to the tensor input\n\n    Example::\n\n        >>> tensor = torch.randn(3, 3)\n        >>> mask = torch.tensor([[False, False, True], [True, False, True], [False, False, True]])\n        >>> nt = torch.nested.masked_select(tensor, mask)\n        >>> nt.shape\n        torch.Size([3, j4])\n        >>> # Length of each item in the batch:\n        >>> nt.offsets().diff()\n        tensor([1, 2, 1])\n\n        >>> tensor = torch.randn(6, 5)\n        >>> mask = torch.tensor([False])\n        >>> nt = torch.nested.masked_select(tensor, mask)\n        >>> nt.shape\n        torch.Size([6, j5])\n        >>> # Length of each item in the batch:\n        >>> nt.offsets().diff()\n        tensor([0, 0, 0, 0, 0, 0])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.nested.narrow",
      "signature": "torch.nested.narrow(tensor: torch.Tensor, dim: int, start: Union[int, torch.Tensor], length: Union[int, torch.Tensor], layout=torch.strided) -> torch.Tensor",
      "doc": "\n    Constructs a nested tensor (which might be a view) from :attr:`tensor`, a strided tensor. This follows\n    similar semantics to torch.Tensor.narrow, where in the :attr:`dim`-th dimension the new nested tensor\n    shows only the elements in the interval `[start, start+length)`. As nested representations\n    allow for a different `start` and `length` at each 'row' of that dimension, :attr:`start` and :attr:`length`\n    can also be tensors of shape `tensor.shape[0]`.\n\n    There's some differences depending on the layout you use for the nested tensor. If using strided layout,\n    torch.narrow will do a copy of the narrowed data into a contiguous NT with strided layout, while\n    jagged layout narrow() will create a non-contiguous view of your original strided tensor. This particular\n    representation is really useful for representing kv-caches in Transformer models, as specialized\n    SDPA kernels can deal with format easily, resulting in performance improvements.\n\n\n    Args:\n        tensor (:class:`torch.Tensor`): a strided tensor, which will be used as the underlying data\n            for the nested tensor if using the jagged layout or will be copied for the strided layout.\n        dim (int): the dimension where narrow will be applied. Only `dim=1` is supported for the\n            jagged layout, while strided supports all dim\n        start (Union[int, :class:`torch.Tensor`]): starting element for the narrow operation\n        length (Union[int, :class:`torch.Tensor`]): number of elements taken during the narrow op\n\n    Keyword arguments:\n        layout (:class:`torch.layout`, optional): the desired layout of returned nested tensor.\n            Only strided and jagged layouts are supported. Default: if None, the strided layout.\n\n    Example::\n\n        >>> starts = torch.tensor([0, 1, 2, 3, 4], dtype=torch.int64)\n        >>> lengths = torch.tensor([3, 2, 2, 1, 5], dtype=torch.int64)\n        >>> narrow_base = torch.randn(5, 10, 20)\n        >>> nt_narrowed = torch.nested.narrow(narrow_base, 1, starts, lengths, layout=torch.jagged)\n        >>> nt_narrowed.is_contiguous()\n        False\n    ",
      "arguments": [
        "tensor",
        "dim",
        "start",
        "length",
        "layout"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Constructs a nested tensor (which might be a view) from :attr:`tensor`, a strided tensor. This follows\n    similar semantics to torch.Tensor.narrow, where in the :attr:`dim`-th dimension the new nested tensor\n    shows only the elements in the interval `[start, start+length)`. As nested representations\n    allow for a different `start` and `length` at each 'row' of that dimension, :attr:`start` and :attr:`length`\n    can also be tensors of shape `tensor.shape[0]`.\n\n    There's some differences depending on the layout you use for the nested tensor. If using strided layout,\n    torch.narrow will do a copy of the narrowed data into a contiguous NT with strided layout, while\n    jagged layout narrow() will create a non-contiguous view of your original strided tensor. This particular\n    representation is really useful for representing kv-caches in Transformer models, as specialized\n    SDPA kernels can deal with format easily, resulting in performance improvements.\n\n\n    Args:\n        tensor (:class:`torch.Tensor`): a strided tensor, which will be used as the underlying data\n            for the nested tensor if using the jagged layout or will be copied for the strided layout.\n        dim (int): the dimension where narrow will be applied. Only `dim=1` is supported for the\n            jagged layout, while strided supports all dim\n        start (Union[int, :class:`torch.Tensor`]): starting element for the narrow operation\n        length (Union[int, :class:`torch.Tensor`]): number of elements taken during the narrow op\n\n    Keyword arguments:\n        layout (:class:`torch.layout`, optional): the desired layout of returned nested tensor.\n            Only strided and jagged layouts are supported. Default: if None, the strided layout.\n\n    Example::\n\n        >>> starts = torch.tensor([0, 1, 2, 3, 4], dtype=torch.int64)\n        >>> lengths = torch.tensor([3, 2, 2, 1, 5], dtype=torch.int64)\n        >>> narrow_base = torch.randn(5, 10, 20)\n        >>> nt_narrowed = torch.nested.narrow(narrow_base, 1, starts, lengths, layout=torch.jagged)\n        >>> nt_narrowed.is_contiguous()\n        False\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.nested.nested_tensor",
      "signature": "torch.nested.nested_tensor(tensor_list, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) -> torch.Tensor",
      "doc": "\n    Constructs a nested tensor with no autograd history (also known as a \"leaf tensor\", see\n    :ref:`Autograd mechanics <autograd-mechanics>`) from :attr:`tensor_list` a list of tensors.\n\n    Args:\n        tensor_list (List[array_like]): a list of tensors, or anything that can be passed to torch.tensor,\n        where each element of the list has the same dimensionality.\n\n    Keyword arguments:\n        dtype (:class:`torch.dtype`, optional): the desired type of returned nested tensor.\n            Default: if None, same :class:`torch.dtype` as leftmost tensor in the list.\n        layout (:class:`torch.layout`, optional): the desired layout of returned nested tensor.\n            Only strided and jagged layouts are supported. Default: if None, the strided layout.\n        device (:class:`torch.device`, optional): the desired device of returned nested tensor.\n            Default: if None, same :class:`torch.device` as leftmost tensor in the list\n        requires_grad (bool, optional): If autograd should record operations on the\n            returned nested tensor. Default: ``False``.\n        pin_memory (bool, optional): If set, returned nested tensor would be allocated in\n            the pinned memory. Works only for CPU tensors. Default: ``False``.\n\n    Example::\n\n        >>> a = torch.arange(3, dtype=torch.float, requires_grad=True)\n        >>> b = torch.arange(5, dtype=torch.float, requires_grad=True)\n        >>> nt = torch.nested.nested_tensor([a, b], requires_grad=True)\n        >>> nt.is_leaf\n        True\n    ",
      "arguments": [
        "tensor_list",
        "dtype",
        "layout",
        "device",
        "requires_grad",
        "pin_memory"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Constructs a nested tensor with no autograd history (also known as a \"leaf tensor\", see\n    :ref:`Autograd mechanics <autograd-mechanics>`) from :attr:`tensor_list` a list of tensors.\n\n    Args:\n        tensor_list (List[array_like]): a list of tensors, or anything that can be passed to torch.tensor,\n        where each element of the list has the same dimensionality.\n\n    Keyword arguments:\n        dtype (:class:`torch.dtype`, optional): the desired type of returned nested tensor.\n            Default: if None, same :class:`torch.dtype` as leftmost tensor in the list.\n        layout (:class:`torch.layout`, optional): the desired layout of returned nested tensor.\n            Only strided and jagged layouts are supported. Default: if None, the strided layout.\n        device (:class:`torch.device`, optional): the desired device of returned nested tensor.\n            Default: if None, same :class:`torch.device` as leftmost tensor in the list\n        requires_grad (bool, optional): If autograd should record operations on the\n            returned nested tensor. Default: ``False``.\n        pin_memory (bool, optional): If set, returned nested tensor would be allocated in\n            the pinned memory. Works only for CPU tensors. Default: ``False``.\n\n    Example::\n\n        >>> a = torch.arange(3, dtype=torch.float, requires_grad=True)\n        >>> b = torch.arange(5, dtype=torch.float, requires_grad=True)\n        >>> nt = torch.nested.nested_tensor([a, b], requires_grad=True)\n        >>> nt.is_leaf\n        True\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.nested.nested_tensor_from_jagged",
      "signature": "torch.nested.nested_tensor_from_jagged(values: torch.Tensor, offsets: Optional[torch.Tensor] = None, lengths: Optional[torch.Tensor] = None, jagged_dim: Optional[int] = None, min_seqlen: Optional[int] = None, max_seqlen: Optional[int] = None) -> torch.Tensor",
      "doc": "\n    Constructs a jagged layout nested tensor from the given jagged components. The jagged layout\n    consists of a required values buffer with the jagged dimension packed into a single dimension.\n    The offsets / lengths metadata determines how this dimension is split into batch elements\n    and are expected to be allocated on the same device as the values buffer.\n\n    Expected metadata formats:\n        * offsets: Indices within the packed dimension splitting it into heterogeneously-sized\n          batch elements. Example: [0, 2, 3, 6] indicates that a packed jagged dim of size 6\n          should be conceptually split into batch elements of length [2, 1, 3]. Note that both the\n          beginning and ending offsets are required for kernel convenience (i.e. shape batch_size + 1).\n        * lengths: Lengths of the individual batch elements; shape == batch_size. Example: [2, 1, 3]\n          indicates that a packed jagged dim of size 6 should be conceptually split into batch\n          elements of length [2, 1, 3].\n\n    Note that it can be useful to provide both offsets and lengths. This describes a nested tensor\n    with \"holes\", where the offsets indicate the start position of each batch item and the length\n    specifies the total number of elements (see example below).\n\n    The returned jagged layout nested tensor will be a view of the input values tensor.\n\n    Args:\n        values (:class:`torch.Tensor`): The underlying buffer in the shape of\n            (sum_B(*), D_1, ..., D_N). The jagged dimension is packed into a single dimension,\n            with the offsets / lengths metadata used to distinguish batch elements.\n        offsets (optional :class:`torch.Tensor`): Offsets into the jagged dimension of shape B + 1.\n        lengths (optional :class:`torch.Tensor`): Lengths of the batch elements of shape B.\n        jagged_dim (optional int): Indicates which dimension in values is the packed jagged\n            dimension. If None, this is set to dim=1 (i.e. the dimension immediately following\n            the batch dimension). Default: None\n        min_seqlen (optional int): If set, uses the specified value as the cached minimum sequence\n            length for the returned nested tensor. This can be a useful alternative to computing\n            this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\n        max_seqlen (optional int): If set, uses the specified value as the cached maximum sequence\n            length for the returned nested tensor. This can be a useful alternative to computing\n            this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\n\n    Example::\n\n        >>> values = torch.randn(12, 5)\n        >>> offsets = torch.tensor([0, 3, 5, 6, 10, 12])\n        >>> nt = nested_tensor_from_jagged(values, offsets)\n        >>> # 3D shape with the middle dimension jagged\n        >>> nt.shape\n        torch.Size([5, j2, 5])\n        >>> # Length of each item in the batch:\n        >>> offsets.diff()\n        tensor([3, 2, 1, 4, 2])\n\n        >>> values = torch.randn(6, 5)\n        >>> offsets = torch.tensor([0, 2, 3, 6])\n        >>> lengths = torch.tensor([1, 1, 2])\n        >>> # NT with holes\n        >>> nt = nested_tensor_from_jagged(values, offsets, lengths)\n        >>> a, b, c = nt.unbind()\n        >>> # Batch item 1 consists of indices [0, 1)\n        >>> torch.equal(a, values[0:1, :])\n        True\n        >>> # Batch item 2 consists of indices [2, 3)\n        >>> torch.equal(b, values[2:3, :])\n        True\n        >>> # Batch item 3 consists of indices [3, 5)\n        >>> torch.equal(c, values[3:5, :])\n        True\n    ",
      "arguments": [
        "values",
        "offsets",
        "lengths",
        "jagged_dim",
        "min_seqlen",
        "max_seqlen"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Constructs a jagged layout nested tensor from the given jagged components. The jagged layout\n    consists of a required values buffer with the jagged dimension packed into a single dimension.\n    The offsets / lengths metadata determines how this dimension is split into batch elements\n    and are expected to be allocated on the same device as the values buffer.\n\n    Expected metadata formats:\n        * offsets: Indices within the packed dimension splitting it into heterogeneously-sized\n          batch elements. Example: [0, 2, 3, 6] indicates that a packed jagged dim of size 6\n          should be conceptually split into batch elements of length [2, 1, 3]. Note that both the\n          beginning and ending offsets are required for kernel convenience (i.e. shape batch_size + 1).\n        * lengths: Lengths of the individual batch elements; shape == batch_size. Example: [2, 1, 3]\n          indicates that a packed jagged dim of size 6 should be conceptually split into batch\n          elements of length [2, 1, 3].\n\n    Note that it can be useful to provide both offsets and lengths. This describes a nested tensor\n    with \"holes\", where the offsets indicate the start position of each batch item and the length\n    specifies the total number of elements (see example below).\n\n    The returned jagged layout nested tensor will be a view of the input values tensor.\n\n    Args:\n        values (:class:`torch.Tensor`): The underlying buffer in the shape of\n            (sum_B(*), D_1, ..., D_N). The jagged dimension is packed into a single dimension,\n            with the offsets / lengths metadata used to distinguish batch elements.\n        offsets (optional :class:`torch.Tensor`): Offsets into the jagged dimension of shape B + 1.\n        lengths (optional :class:`torch.Tensor`): Lengths of the batch elements of shape B.\n        jagged_dim (optional int): Indicates which dimension in values is the packed jagged\n            dimension. If None, this is set to dim=1 (i.e. the dimension immediately following\n            the batch dimension). Default: None\n        min_seqlen (optional int): If set, uses the specified value as the cached minimum sequence\n            length for the returned nested tensor. This can be a useful alternative to computing\n            this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\n        max_seqlen (optional int): If set, uses the specified value as the cached maximum sequence\n            length for the returned nested tensor. This can be a useful alternative to computing\n            this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\n\n    Example::\n\n        >>> values = torch.randn(12, 5)\n        >>> offsets = torch.tensor([0, 3, 5, 6, 10, 12])\n        >>> nt = nested_tensor_from_jagged(values, offsets)\n        >>> # 3D shape with the middle dimension jagged\n        >>> nt.shape\n        torch.Size([5, j2, 5])\n        >>> # Length of each item in the batch:\n        >>> offsets.diff()\n        tensor([3, 2, 1, 4, 2])\n\n        >>> values = torch.randn(6, 5)\n        >>> offsets = torch.tensor([0, 2, 3, 6])\n        >>> lengths = torch.tensor([1, 1, 2])\n        >>> # NT with holes\n        >>> nt = nested_tensor_from_jagged(values, offsets, lengths)\n        >>> a, b, c = nt.unbind()\n        >>> # Batch item 1 consists of indices [0, 1)\n        >>> torch.equal(a, values[0:1, :])\n        True\n        >>> # Batch item 2 consists of indices [2, 3)\n        >>> torch.equal(b, values[2:3, :])\n        True\n        >>> # Batch item 3 consists of indices [3, 5)\n        >>> torch.equal(c, values[3:5, :])\n        True\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "NEURAL_NETWORK": [
    {
      "function": "torch.nn.factory_kwargs",
      "signature": "torch.nn.factory_kwargs(kwargs)",
      "doc": "Return a canonicalized dict of factory kwargs.\n\n    Given kwargs, returns a canonicalized dict of factory kwargs that can be directly passed\n    to factory functions like torch.empty, or errors if unrecognized kwargs are present.\n\n    This function makes it simple to write code like this::\n\n        class MyModule(nn.Module):\n            def __init__(self, **kwargs):\n                factory_kwargs = torch.nn.factory_kwargs(kwargs)\n                self.weight = Parameter(torch.empty(10, **factory_kwargs))\n\n    Why should you use this function instead of just passing `kwargs` along directly?\n\n    1. This function does error validation, so if there are unexpected kwargs we will\n    immediately report an error, instead of deferring it to the factory call\n    2. This function supports a special `factory_kwargs` argument, which can be used to\n    explicitly specify a kwarg to be used for factory functions, in the event one of the\n    factory kwargs conflicts with an already existing argument in the signature (e.g.\n    in the signature ``def f(dtype, **kwargs)``, you can specify ``dtype`` for factory\n    functions, as distinct from the dtype argument, by saying\n    ``f(dtype1, factory_kwargs={\"dtype\": dtype2})``)\n    ",
      "arguments": [
        "kwargs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a canonicalized dict of factory kwargs.\n\n    Given kwargs, returns a canonicalized dict of factory kwargs that can be directly passed\n    to factory functions like torch.empty, or errors if unrecognized kwargs are present.\n\n    This function makes it simple to write code like this::\n\n        class MyModule(nn.Module):\n            def __init__(self, **kwargs):\n                factory_kwargs = torch.nn.factory_kwargs(kwargs)\n                self.weight = Parameter(torch.empty(10, **factory_kwargs))\n\n    Why should you use this function instead of just passing `kwargs` along directly?\n\n    1. This function does error validation, so if there are unexpected kwargs we will\n    immediately report an error, instead of deferring it to the factory call\n    2. This function supports a special `factory_kwargs` argument, which can be used to\n    explicitly specify a kwarg to be used for factory functions, in the event one of the\n    factory kwargs conflicts with an already existing argument in the signature (e.g.\n    in the signature ``def f(dtype, **kwargs)``, you can specify ``dtype`` for factory\n    functions, as distinct from the dtype argument, by saying\n    ``f(dtype1, factory_kwargs={\"dtype\": dtype2})``)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "ONNX_EXPORT": [
    {
      "function": "torch.onnx.dynamo_export",
      "signature": "torch.onnx.dynamo_export(model: 'torch.nn.Module | Callable | torch.export.ExportedProgram', /, *model_args, export_options: 'ExportOptions | None' = None, **model_kwargs) -> 'ONNXProgram'",
      "doc": "Export a torch.nn.Module to an ONNX graph.\n\n    .. deprecated:: 2.7\n        Please use ``torch.onnx.export(..., dynamo=True)`` instead.\n\n    Args:\n        model: The PyTorch model to be exported to ONNX.\n        model_args: Positional inputs to ``model``.\n        model_kwargs: Keyword inputs to ``model``.\n        export_options: Options to influence the export to ONNX.\n\n    Returns:\n        An in-memory representation of the exported ONNX model.\n    ",
      "arguments": [
        "model",
        "model_args",
        "export_options",
        "model_kwargs"
      ],
      "return_type": "ONNXProgram",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Export a torch.nn.Module to an ONNX graph.\n\n    .. deprecated:: 2.7\n        Please use ``torch.onnx.export(..., dynamo=True)`` instead.\n\n    Args:\n        model: The PyTorch model to be exported to ONNX.\n        model_args: Positional inputs to ``model``.\n        model_kwargs: Keyword inputs to ``model``.\n        export_options: Options to influence the export to ONNX.\n\n    Returns:\n        An in-memory representation of the exported ONNX model.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.onnx.enable_fake_mode",
      "signature": "torch.onnx.enable_fake_mode()",
      "doc": "Enable fake mode for the duration of the context.\n\n    Internally it instantiates a :class:`torch._subclasses.fake_tensor.FakeTensorMode` context manager\n    that converts user input and model parameters into :class:`torch._subclasses.fake_tensor.FakeTensor`.\n\n    A :class:`torch._subclasses.fake_tensor.FakeTensor`\n    is a :class:`torch.Tensor` with the ability to run PyTorch code without having to\n    actually do computation through tensors allocated on a ``meta`` device. Because\n    there is no actual data being allocated on the device, this API allows for\n    initializing and exporting large models without the actual memory footprint needed for executing it.\n\n    It is highly recommended to initialize the model in fake mode when exporting models that\n    are too large to fit into memory.\n\n    .. note::\n        This function does not support torch.onnx.export(..., dynamo=True, optimize=True).\n        Please call ONNXProgram.optimize() outside of the function after the model is exported.\n\n    Example::\n\n        # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX)\n        >>> import torch\n        >>> class MyModel(torch.nn.Module):  # Model with a parameter\n        ...     def __init__(self) -> None:\n        ...         super().__init__()\n        ...         self.weight = torch.nn.Parameter(torch.tensor(42.0))\n        ...     def forward(self, x):\n        ...         return self.weight + x\n        >>> with torch.onnx.enable_fake_mode():\n        ...     # When initialized in fake mode, the model's parameters are fake tensors\n        ...     # They do not take up memory so we can initialize large models\n        ...     my_nn_module = MyModel()\n        ...     arg1 = torch.randn(2, 2, 2)\n        >>> onnx_program = torch.onnx.export(my_nn_module, (arg1,), dynamo=True, optimize=False)\n        >>> # Saving model WITHOUT initializers (only the architecture)\n        >>> onnx_program.save(\n        ...     \"my_model_without_initializers.onnx\",\n        ...     include_initializers=False,\n        ...     keep_initializers_as_inputs=True,\n        ... )\n        >>> # Saving model WITH initializers after applying concrete weights\n        >>> onnx_program.apply_weights({\"weight\": torch.tensor(42.0)})\n        >>> onnx_program.save(\"my_model_with_initializers.onnx\")\n\n    .. warning::\n        This API is experimental and is *NOT* backward-compatible.\n\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Enable fake mode for the duration of the context.\n\n    Internally it instantiates a :class:`torch._subclasses.fake_tensor.FakeTensorMode` context manager\n    that converts user input and model parameters into :class:`torch._subclasses.fake_tensor.FakeTensor`.\n\n    A :class:`torch._subclasses.fake_tensor.FakeTensor`\n    is a :class:`torch.Tensor` with the ability to run PyTorch code without having to\n    actually do computation through tensors allocated on a ``meta`` device. Because\n    there is no actual data being allocated on the device, this API allows for\n    initializing and exporting large models without the actual memory footprint needed for executing it.\n\n    It is highly recommended to initialize the model in fake mode when exporting models that\n    are too large to fit into memory.\n\n    .. note::\n        This function does not support torch.onnx.export(..., dynamo=True, optimize=True).\n        Please call ONNXProgram.optimize() outside of the function after the model is exported.\n\n    Example::\n\n        # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX)\n        >>> import torch\n        >>> class MyModel(torch.nn.Module):  # Model with a parameter\n        ...     def __init__(self) -> None:\n        ...         super().__init__()\n        ...         self.weight = torch.nn.Parameter(torch.tensor(42.0))\n        ...     def forward(self, x):\n        ...         return self.weight + x\n        >>> with torch.onnx.enable_fake_mode():\n        ...     # When initialized in fake mode, the model's parameters are fake tensors\n        ...     # They do not take up memory so we can initialize large models\n        ...     my_nn_module = MyModel()\n        ...     arg1 = torch.randn(2, 2, 2)\n        >>> onnx_program = torch.onnx.export(my_nn_module, (arg1,), dynamo=True, optimize=False)\n        >>> # Saving model WITHOUT initializers (only the architecture)\n        >>> onnx_program.save(\n        ...     \"my_model_without_initializers.onnx\",\n        ...     include_initializers=False,\n        ...     keep_initializers_as_inputs=True,\n        ... )\n        >>> # Saving model WITH initializers after applying concrete weights\n        >>> onnx_program.apply_weights({\"weight\": torch.tensor(42.0)})\n        >>> onnx_program.save(\"my_model_with_initializers.onnx\")\n\n    .. warning::\n        This API is experimental and is *NOT* backward-compatible.\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.onnx.export",
      "signature": "torch.onnx.export(model: 'torch.nn.Module | torch.export.ExportedProgram | torch.jit.ScriptModule | torch.jit.ScriptFunction', args: 'tuple[Any, ...]' = (), f: 'str | os.PathLike | None' = None, *, kwargs: 'dict[str, Any] | None' = None, export_params: 'bool' = True, verbose: 'bool | None' = None, input_names: 'Sequence[str] | None' = None, output_names: 'Sequence[str] | None' = None, opset_version: 'int | None' = None, dynamic_axes: 'Mapping[str, Mapping[int, str]] | Mapping[str, Sequence[int]] | None' = None, keep_initializers_as_inputs: 'bool' = False, dynamo: 'bool' = False, external_data: 'bool' = True, dynamic_shapes: 'dict[str, Any] | tuple[Any, ...] | list[Any] | None' = None, custom_translation_table: 'dict[Callable, Callable | Sequence[Callable]] | None' = None, report: 'bool' = False, optimize: 'bool' = True, verify: 'bool' = False, profile: 'bool' = False, dump_exported_program: 'bool' = False, artifacts_dir: 'str | os.PathLike' = '.', fallback: 'bool' = False, training: '_C_onnx.TrainingMode' = <TrainingMode.EVAL: 0>, operator_export_type: '_C_onnx.OperatorExportTypes' = <OperatorExportTypes.ONNX: 0>, do_constant_folding: 'bool' = True, custom_opsets: 'Mapping[str, int] | None' = None, export_modules_as_functions: 'bool | Collection[type[torch.nn.Module]]' = False, autograd_inlining: 'bool' = True) -> 'ONNXProgram | None'",
      "doc": "Exports a model into ONNX format.\n\n    Setting ``dynamo=True`` enables the new ONNX export logic\n    which is based on :class:`torch.export.ExportedProgram` and a more modern\n    set of translation logic. This is the recommended way to export models\n    to ONNX.\n\n    When ``dynamo=True``:\n\n    The exporter tries the following strategies to get an ExportedProgram for conversion to ONNX.\n\n    #. If the model is already an ExportedProgram, it will be used as-is.\n    #. Use :func:`torch.export.export` and set ``strict=False``.\n    #. Use :func:`torch.export.export` and set ``strict=True``.\n    #. Use ``draft_export`` which removes some soundness guarantees in data-dependent\n       operations to allow export to proceed. You will get a warning if the exporter\n       encounters any unsound data-dependent operation.\n    #. Use :func:`torch.jit.trace` to trace the model then convert to ExportedProgram.\n       This is the most unsound strategy but may be useful for converting TorchScript\n       models to ONNX.\n\n    Args:\n        model: The model to be exported.\n        args: Example positional inputs. Any non-Tensor arguments will be hard-coded into the\n            exported model; any Tensor arguments will become inputs of the exported model,\n            in the order they occur in the tuple.\n        f: Path to the output ONNX model file. E.g. \"model.onnx\".\n        kwargs: Optional example keyword inputs.\n        export_params: If false, parameters (weights) will not be exported.\n        verbose: Whether to enable verbose logging.\n        input_names: names to assign to the input nodes of the graph, in order.\n        output_names: names to assign to the output nodes of the graph, in order.\n        opset_version: The version of the\n            `default (ai.onnx) opset <https://github.com/onnx/onnx/blob/master/docs/Operators.md>`_\n            to target. Must be >= 7.\n        dynamic_axes:\n\n            By default the exported model will have the shapes of all input and output tensors\n            set to exactly match those given in ``args``. To specify axes of tensors as\n            dynamic (i.e. known only at run-time), set ``dynamic_axes`` to a dict with schema:\n\n            * KEY (str): an input or output name. Each name must also be provided in ``input_names`` or\n                ``output_names``.\n            * VALUE (dict or list): If a dict, keys are axis indices and values are axis names. If a\n                list, each element is an axis index.\n\n            For example::\n\n                class SumModule(torch.nn.Module):\n                    def forward(self, x):\n                        return torch.sum(x, dim=1)\n\n\n                torch.onnx.export(\n                    SumModule(),\n                    (torch.ones(2, 2),),\n                    \"onnx.pb\",\n                    input_names=[\"x\"],\n                    output_names=[\"sum\"],\n                )\n\n            Produces::\n\n                input {\n                  name: \"x\"\n                  ...\n                      shape {\n                        dim {\n                          dim_value: 2  # axis 0\n                        }\n                        dim {\n                          dim_value: 2  # axis 1\n                ...\n                output {\n                  name: \"sum\"\n                  ...\n                      shape {\n                        dim {\n                          dim_value: 2  # axis 0\n                ...\n\n            While::\n\n                torch.onnx.export(\n                    SumModule(),\n                    (torch.ones(2, 2),),\n                    \"onnx.pb\",\n                    input_names=[\"x\"],\n                    output_names=[\"sum\"],\n                    dynamic_axes={\n                        # dict value: manually named axes\n                        \"x\": {0: \"my_custom_axis_name\"},\n                        # list value: automatic names\n                        \"sum\": [0],\n                    },\n                )\n\n            Produces::\n\n                input {\n                  name: \"x\"\n                  ...\n                      shape {\n                        dim {\n                          dim_param: \"my_custom_axis_name\"  # axis 0\n                        }\n                        dim {\n                          dim_value: 2  # axis 1\n                ...\n                output {\n                  name: \"sum\"\n                  ...\n                      shape {\n                        dim {\n                          dim_param: \"sum_dynamic_axes_1\"  # axis 0\n                ...\n\n        keep_initializers_as_inputs: If True, all the\n            initializers (typically corresponding to model weights) in the\n            exported graph will also be added as inputs to the graph. If False,\n            then initializers are not added as inputs to the graph, and only\n            the user inputs are added as inputs.\n\n            Set this to True if you intend to supply model weights at runtime.\n            Set it to False if the weights are static to allow for better optimizations\n            (e.g. constant folding) by backends/runtimes.\n\n        dynamo: Whether to export the model with ``torch.export`` ExportedProgram instead of TorchScript.\n        external_data: Whether to save the model weights as an external data file.\n            This is required for models with large weights that exceed the ONNX file size limit (2GB).\n            When False, the weights are saved in the ONNX file with the model architecture.\n        dynamic_shapes: A dictionary or a tuple of dynamic shapes for the model inputs. Refer to\n            :func:`torch.export.export` for more details. This is only used (and preferred) when dynamo is True.\n            Note that dynamic_shapes is designed to be used when the model is exported with dynamo=True, while\n            dynamic_axes is used when dynamo=False.\n        custom_translation_table: A dictionary of custom decompositions for operators in the model.\n            The dictionary should have the callable target in the fx Node as the key (e.g. ``torch.ops.aten.stft.default``),\n            and the value should be a function that builds that graph using ONNX Script. This option\n            is only valid when dynamo is True.\n        report: Whether to generate a markdown report for the export process. This option\n            is only valid when dynamo is True.\n        optimize: Whether to optimize the exported model. This option\n            is only valid when dynamo is True. Default is True.\n        verify: Whether to verify the exported model using ONNX Runtime. This option\n            is only valid when dynamo is True.\n        profile: Whether to profile the export process. This option\n            is only valid when dynamo is True.\n        dump_exported_program: Whether to dump the :class:`torch.export.ExportedProgram` to a file.\n            This is useful for debugging the exporter. This option is only valid when dynamo is True.\n        artifacts_dir: The directory to save the debugging artifacts like the report and the serialized\n            exported program. This option is only valid when dynamo is True.\n        fallback: Whether to fallback to the TorchScript exporter if the dynamo exporter fails.\n            This option is only valid when dynamo is True. When fallback is enabled, It is\n            recommended to set dynamic_axes even when dynamic_shapes is provided.\n\n        training: Deprecated option. Instead, set the training mode of the model before exporting.\n        operator_export_type: Deprecated option. Only ONNX is supported.\n        do_constant_folding: Deprecated option.\n        custom_opsets: Deprecated.\n            A dictionary:\n\n            * KEY (str): opset domain name\n            * VALUE (int): opset version\n\n            If a custom opset is referenced by ``model`` but not mentioned in this dictionary,\n            the opset version is set to 1. Only custom opset domain name and version should be\n            indicated through this argument.\n        export_modules_as_functions: Deprecated option.\n\n            Flag to enable\n            exporting all ``nn.Module`` forward calls as local functions in ONNX. Or a set to indicate the\n            particular types of modules to export as local functions in ONNX.\n            This feature requires ``opset_version`` >= 15, otherwise the export will fail. This is because\n            ``opset_version`` < 15 implies IR version < 8, which means no local function support.\n            Module variables will be exported as function attributes. There are two categories of function\n            attributes.\n\n            1. Annotated attributes: class variables that have type annotations via\n            `PEP 526-style <https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations>`_\n            will be exported as attributes.\n            Annotated attributes are not used inside the subgraph of ONNX local function because\n            they are not created by PyTorch JIT tracing, but they may be used by consumers\n            to determine whether or not to replace the function with a particular fused kernel.\n\n            2. Inferred attributes: variables that are used by operators inside the module. Attribute names\n            will have prefix \"inferred::\". This is to differentiate from predefined attributes retrieved from\n            python module annotations. Inferred attributes are used inside the subgraph of ONNX local function.\n\n            * ``False`` (default): export ``nn.Module`` forward calls as fine grained nodes.\n            * ``True``: export all ``nn.Module`` forward calls as local function nodes.\n            * Set of type of nn.Module: export ``nn.Module`` forward calls as local function nodes,\n                only if the type of the ``nn.Module`` is found in the set.\n        autograd_inlining: Deprecated.\n            Flag used to control whether to inline autograd functions.\n            Refer to https://github.com/pytorch/pytorch/pull/74765 for more details.\n\n    Returns:\n        :class:`torch.onnx.ONNXProgram` if dynamo is True, otherwise None.\n\n    .. versionchanged:: 2.6\n        *training* is now deprecated. Instead, set the training mode of the model before exporting.\n        *operator_export_type* is now deprecated. Only ONNX is supported.\n        *do_constant_folding* is now deprecated. It is always enabled.\n        *export_modules_as_functions* is now deprecated.\n        *autograd_inlining* is now deprecated.\n    .. versionchanged:: 2.7\n        *optimize* is now True by default.\n    ",
      "arguments": [
        "model",
        "args",
        "f",
        "kwargs",
        "export_params",
        "verbose",
        "input_names",
        "output_names",
        "opset_version",
        "dynamic_axes",
        "keep_initializers_as_inputs",
        "dynamo",
        "external_data",
        "dynamic_shapes",
        "custom_translation_table",
        "report",
        "optimize",
        "verify",
        "profile",
        "dump_exported_program",
        "artifacts_dir",
        "fallback",
        "training",
        "operator_export_type",
        "do_constant_folding",
        "custom_opsets",
        "export_modules_as_functions",
        "autograd_inlining"
      ],
      "return_type": "ONNXProgram | None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Exports a model into ONNX format.\n\n    Setting ``dynamo=True`` enables the new ONNX export logic\n    which is based on :class:`torch.export.ExportedProgram` and a more modern\n    set of translation logic. This is the recommended way to export models\n    to ONNX.\n\n    When ``dynamo=True``:\n\n    The exporter tries the following strategies to get an ExportedProgram for conversion to ONNX.\n\n    #. If the model is already an ExportedProgram, it will be used as-is.\n    #. Use :func:`torch.export.export` and set ``strict=False``.\n    #. Use :func:`torch.export.export` and set ``strict=True``.\n    #. Use ``draft_export`` which removes some soundness guarantees in data-dependent\n       operations to allow export to proceed. You will get a warning if the exporter\n       encounters any unsound data-dependent operation.\n    #. Use :func:`torch.jit.trace` to trace the model then convert to ExportedProgram.\n       This is the most unsound strategy but may be useful for converting TorchScript\n       models to ONNX.\n\n    Args:\n        model: The model to be exported.\n        args: Example positional inputs. Any non-Tensor arguments will be hard-coded into the\n            exported model; any Tensor arguments will become inputs of the exported model,\n            in the order they occur in the tuple.\n        f: Path to the output ONNX model file. E.g. \"model.onnx\".\n        kwargs: Optional example keyword inputs.\n        export_params: If false, parameters (weights) will not be exported.\n        verbose: Whether to enable verbose logging.\n        input_names: names to assign to the input nodes of the graph, in order.\n        output_names: names to assign to the output nodes of the graph, in order.\n        opset_version: The version of the\n            `default (ai.onnx) opset <https://github.com/onnx/onnx/blob/master/docs/Operators.md>`_\n            to target. Must be >= 7.\n        dynamic_axes:\n\n            By default the exported model will have the shapes of all input and output tensors\n            set to exactly match those given in ``args``. To specify axes of tensors as\n            dynamic (i.e. known only at run-time), set ``dynamic_axes`` to a dict with schema:\n\n            * KEY (str): an input or output name. Each name must also be provided in ``input_names`` or\n                ``output_names``.\n            * VALUE (dict or list): If a dict, keys are axis indices and values are axis names. If a\n                list, each element is an axis index.\n\n            For example::\n\n                class SumModule(torch.nn.Module):\n                    def forward(self, x):\n                        return torch.sum(x, dim=1)\n\n\n                torch.onnx.export(\n                    SumModule(),\n                    (torch.ones(2, 2),),\n                    \"onnx.pb\",\n                    input_names=[\"x\"],\n                    output_names=[\"sum\"],\n                )\n\n            Produces::\n\n                input {\n                  name: \"x\"\n                  ...\n                      shape {\n                        dim {\n                          dim_value: 2  # axis 0\n                        }\n                        dim {\n                          dim_value: 2  # axis 1\n                ...\n                output {\n                  name: \"sum\"\n                  ...\n                      shape {\n                        dim {\n                          dim_value: 2  # axis 0\n                ...\n\n            While::\n\n                torch.onnx.export(\n                    SumModule(),\n                    (torch.ones(2, 2),),\n                    \"onnx.pb\",\n                    input_names=[\"x\"],\n                    output_names=[\"sum\"],\n                    dynamic_axes={\n                        # dict value: manually named axes\n                        \"x\": {0: \"my_custom_axis_name\"},\n                        # list value: automatic names\n                        \"sum\": [0],\n                    },\n                )\n\n            Produces::\n\n                input {\n                  name: \"x\"\n                  ...\n                      shape {\n                        dim {\n                          dim_param: \"my_custom_axis_name\"  # axis 0\n                        }\n                        dim {\n                          dim_value: 2  # axis 1\n                ...\n                output {\n                  name: \"sum\"\n                  ...\n                      shape {\n                        dim {\n                          dim_param: \"sum_dynamic_axes_1\"  # axis 0\n                ...\n\n        keep_initializers_as_inputs: If True, all the\n            initializers (typically corresponding to model weights) in the\n            exported graph will also be added as inputs to the graph. If False,\n            then initializers are not added as inputs to the graph, and only\n            the user inputs are added as inputs.\n\n            Set this to True if you intend to supply model weights at runtime.\n            Set it to False if the weights are static to allow for better optimizations\n            (e.g. constant folding) by backends/runtimes.\n\n        dynamo: Whether to export the model with ``torch.export`` ExportedProgram instead of TorchScript.\n        external_data: Whether to save the model weights as an external data file.\n            This is required for models with large weights that exceed the ONNX file size limit (2GB).\n            When False, the weights are saved in the ONNX file with the model architecture.\n        dynamic_shapes: A dictionary or a tuple of dynamic shapes for the model inputs. Refer to\n            :func:`torch.export.export` for more details. This is only used (and preferred) when dynamo is True.\n            Note that dynamic_shapes is designed to be used when the model is exported with dynamo=True, while\n            dynamic_axes is used when dynamo=False.\n        custom_translation_table: A dictionary of custom decompositions for operators in the model.\n            The dictionary should have the callable target in the fx Node as the key (e.g. ``torch.ops.aten.stft.default``),\n            and the value should be a function that builds that graph using ONNX Script. This option\n            is only valid when dynamo is True.\n        report: Whether to generate a markdown report for the export process. This option\n            is only valid when dynamo is True.\n        optimize: Whether to optimize the exported model. This option\n            is only valid when dynamo is True. Default is True.\n        verify: Whether to verify the exported model using ONNX Runtime. This option\n            is only valid when dynamo is True.\n        profile: Whether to profile the export process. This option\n            is only valid when dynamo is True.\n        dump_exported_program: Whether to dump the :class:`torch.export.ExportedProgram` to a file.\n            This is useful for debugging the exporter. This option is only valid when dynamo is True.\n        artifacts_dir: The directory to save the debugging artifacts like the report and the serialized\n            exported program. This option is only valid when dynamo is True.\n        fallback: Whether to fallback to the TorchScript exporter if the dynamo exporter fails.\n            This option is only valid when dynamo is True. When fallback is enabled, It is\n            recommended to set dynamic_axes even when dynamic_shapes is provided.\n\n        training: Deprecated option. Instead, set the training mode of the model before exporting.\n        operator_export_type: Deprecated option. Only ONNX is supported.\n        do_constant_folding: Deprecated option.\n        custom_opsets: Deprecated.\n            A dictionary:\n\n            * KEY (str): opset domain name\n            * VALUE (int): opset version\n\n            If a custom opset is referenced by ``model`` but not mentioned in this dictionary,\n            the opset version is set to 1. Only custom opset domain name and version should be\n            indicated through this argument.\n        export_modules_as_functions: Deprecated option.\n\n            Flag to enable\n            exporting all ``nn.Module`` forward calls as local functions in ONNX. Or a set to indicate the\n            particular types of modules to export as local functions in ONNX.\n            This feature requires ``opset_version`` >= 15, otherwise the export will fail. This is because\n            ``opset_version`` < 15 implies IR version < 8, which means no local function support.\n            Module variables will be exported as function attributes. There are two categories of function\n            attributes.\n\n            1. Annotated attributes: class variables that have type annotations via\n            `PEP 526-style <https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations>`_\n            will be exported as attributes.\n            Annotated attributes are not used inside the subgraph of ONNX local function because\n            they are not created by PyTorch JIT tracing, but they may be used by consumers\n            to determine whether or not to replace the function with a particular fused kernel.\n\n            2. Inferred attributes: variables that are used by operators inside the module. Attribute names\n            will have prefix \"inferred::\". This is to differentiate from predefined attributes retrieved from\n            python module annotations. Inferred attributes are used inside the subgraph of ONNX local function.\n\n            * ``False`` (default): export ``nn.Module`` forward calls as fine grained nodes.\n            * ``True``: export all ``nn.Module`` forward calls as local function nodes.\n            * Set of type of nn.Module: export ``nn.Module`` forward calls as local function nodes,\n                only if the type of the ``nn.Module`` is found in the set.\n        autograd_inlining: Deprecated.\n            Flag used to control whether to inline autograd functions.\n            Refer to https://github.com/pytorch/pytorch/pull/74765 for more details.\n\n    Returns:\n        :class:`torch.onnx.ONNXProgram` if dynamo is True, otherwise None.\n\n    .. versionchanged:: 2.6\n        *training* is now deprecated. Instead, set the training mode of the model before exporting.\n        *operator_export_type* is now deprecated. Only ONNX is supported.\n        *do_constant_folding* is now deprecated. It is always enabled.\n        *export_modules_as_functions* is now deprecated.\n        *autograd_inlining* is now deprecated.\n    .. versionchanged:: 2.7\n        *optimize* is now True by default.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.onnx.is_in_onnx_export",
      "signature": "torch.onnx.is_in_onnx_export() -> 'bool'",
      "doc": "Returns whether it is in the middle of ONNX export.",
      "arguments": [],
      "return_type": "bool",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns whether it is in the middle of ONNX export.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.onnx.is_onnxrt_backend_supported",
      "signature": "torch.onnx.is_onnxrt_backend_supported() -> bool",
      "doc": "Returns ``True`` if ONNX Runtime dependencies are installed and usable\n    to support TorchDynamo backend integration; ``False`` otherwise.\n\n    Example::\n\n        # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX)\n        >>> import torch\n        >>> if torch.onnx.is_onnxrt_backend_supported():\n        ...     @torch.compile(backend=\"onnxrt\")\n        ...     def f(x):\n        ...             return x * x\n        ...     print(f(torch.randn(10)))\n        ... else:\n        ...     print(\"pip install onnx onnxscript onnxruntime\")\n        ...\n    ",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Returns ``True`` if ONNX Runtime dependencies are installed and usable\n    to support TorchDynamo backend integration; ``False`` otherwise.\n\n    Example::\n\n        # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX)\n        >>> import torch\n        >>> if torch.onnx.is_onnxrt_backend_supported():\n        ...     @torch.compile(backend=\"onnxrt\")\n        ...     def f(x):\n        ...             return x * x\n        ...     print(f(torch.randn(10)))\n        ... else:\n        ...     print(\"pip install onnx onnxscript onnxruntime\")\n        ...\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.onnx.register_custom_op_symbolic",
      "signature": "torch.onnx.register_custom_op_symbolic(symbolic_name: 'str', symbolic_fn: 'Callable', opset_version: 'int')",
      "doc": "Registers a symbolic function for a custom operator.\n\n    When the user registers symbolic for custom/contrib ops,\n    it is highly recommended to add shape inference for that operator via setType API,\n    otherwise the exported graph may have incorrect shape inference in some extreme cases.\n    An example of setType is `test_aten_embedding_2` in `test_operators.py`.\n\n    See \"Custom Operators\" in the module documentation for an example usage.\n\n    Args:\n        symbolic_name (str): The name of the custom operator in \"<domain>::<op>\"\n            format.\n        symbolic_fn (Callable): A function that takes in the ONNX graph and\n            the input arguments to the current operator, and returns new\n            operator nodes to add to the graph.\n        opset_version (int): The ONNX opset version in which to register.\n    ",
      "arguments": [
        "symbolic_name",
        "symbolic_fn",
        "opset_version"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Registers a symbolic function for a custom operator.\n\n    When the user registers symbolic for custom/contrib ops,\n    it is highly recommended to add shape inference for that operator via setType API,\n    otherwise the exported graph may have incorrect shape inference in some extreme cases.\n    An example of setType is `test_aten_embedding_2` in `test_operators.py`.\n\n    See \"Custom Operators\" in the module documentation for an example usage.\n\n    Args:\n        symbolic_name (str): The name of the custom operator in \"<domain>::<op>\"\n            format.\n        symbolic_fn (Callable): A function that takes in the ONNX graph and\n            the input arguments to the current operator, and returns new\n            operator nodes to add to the graph.\n        opset_version (int): The ONNX opset version in which to register.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.onnx.select_model_mode_for_export",
      "signature": "torch.onnx.select_model_mode_for_export(model, mode: '_C_onnx.TrainingMode')",
      "doc": "A context manager to temporarily set the training mode of ``model``\n    to ``mode``, resetting it when we exit the with-block.\n\n    .. deprecated:: 2.7\n        Please set training mode before exporting the model.\n\n    Args:\n        model: Same type and meaning as ``model`` arg to :func:`export`.\n        mode: Same type and meaning as ``training`` arg to :func:`export`.\n    ",
      "arguments": [
        "model",
        "mode"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "A context manager to temporarily set the training mode of ``model``\n    to ``mode``, resetting it when we exit the with-block.\n\n    .. deprecated:: 2.7\n        Please set training mode before exporting the model.\n\n    Args:\n        model: Same type and meaning as ``model`` arg to :func:`export`.\n        mode: Same type and meaning as ``training`` arg to :func:`export`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.onnx.unregister_custom_op_symbolic",
      "signature": "torch.onnx.unregister_custom_op_symbolic(symbolic_name: 'str', opset_version: 'int')",
      "doc": "Unregisters ``symbolic_name``.\n\n    See \"Custom Operators\" in the module documentation for an example usage.\n\n    Args:\n        symbolic_name (str): The name of the custom operator in \"<domain>::<op>\"\n            format.\n        opset_version (int): The ONNX opset version in which to unregister.\n    ",
      "arguments": [
        "symbolic_name",
        "opset_version"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Unregisters ``symbolic_name``.\n\n    See \"Custom Operators\" in the module documentation for an example usage.\n\n    Args:\n        symbolic_name (str): The name of the custom operator in \"<domain>::<op>\"\n            format.\n        opset_version (int): The ONNX opset version in which to unregister.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "PACKAGING": [
    {
      "function": "torch.package.is_from_package",
      "signature": "torch.package.is_from_package(obj: Any) -> bool",
      "doc": "\n    Return whether an object was loaded from a package.\n\n    Note: packaged objects from externed modules will return ``False``.\n    ",
      "arguments": [
        "obj"
      ],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return whether an object was loaded from a package.\n\n    Note: packaged objects from externed modules will return ``False``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "PROFILING": [
    {
      "function": "torch.profiler.is_fbcode",
      "signature": "torch.profiler.is_fbcode() -> bool",
      "doc": "",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.profiler.register_optimizer_step_post_hook",
      "signature": "torch.profiler.register_optimizer_step_post_hook(hook: Callable[[ForwardRef('Optimizer'), tuple[Any, ...], dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle",
      "doc": "Register a post hook common to all optimizers.\n\n    The hook should have the following signature::\n\n        hook(optimizer, args, kwargs) -> None\n\n    Args:\n        hook (Callable): A user defined hook which is registered on all optimizers.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    ",
      "arguments": [
        "hook"
      ],
      "return_type": "<class 'torch.utils.hooks.RemovableHandle'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Register a post hook common to all optimizers.\n\n    The hook should have the following signature::\n\n        hook(optimizer, args, kwargs) -> None\n\n    Args:\n        hook (Callable): A user defined hook which is registered on all optimizers.\n\n    Returns:\n        :class:`torch.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.profiler.schedule",
      "signature": "torch.profiler.schedule(*, wait: int, warmup: int, active: int, repeat: int = 0, skip_first: int = 0, skip_first_wait: int = 0) -> Callable",
      "doc": "\n    Returns a callable that can be used as profiler ``schedule`` argument. The profiler will skip\n    the first ``skip_first`` steps, then wait for ``wait`` steps, then do the warmup for the next ``warmup`` steps,\n    then do the active recording for the next ``active`` steps and then repeat the cycle starting with ``wait`` steps.\n    The optional number of cycles is specified with the ``repeat`` parameter, the zero value means that\n    the cycles will continue until the profiling is finished.\n\n    The ``skip_first_wait`` parameter controls whether the first ``wait`` stage should be skipped.\n    This can be useful if a user wants to wait longer than ``skip_first`` between cycles, but not\n    for the first profile. For example, if ``skip_first`` is 10 and ``wait`` is 20, the first cycle will\n    wait 10 + 20 = 30 steps before warmup if ``skip_first_wait`` is zero, but will wait only 10\n    steps if ``skip_first_wait`` is non-zero. All subsequent cycles will then wait 20 steps between the\n    last active and warmup.\n    ",
      "arguments": [
        "wait",
        "warmup",
        "active",
        "repeat",
        "skip_first",
        "skip_first_wait"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns a callable that can be used as profiler ``schedule`` argument. The profiler will skip\n    the first ``skip_first`` steps, then wait for ``wait`` steps, then do the warmup for the next ``warmup`` steps,\n    then do the active recording for the next ``active`` steps and then repeat the cycle starting with ``wait`` steps.\n    The optional number of cycles is specified with the ``repeat`` parameter, the zero value means that\n    the cycles will continue until the profiling is finished.\n\n    The ``skip_first_wait`` parameter controls whether the first ``wait`` stage should be skipped.\n    This can be useful if a user wants to wait longer than ``skip_first`` between cycles, but not\n    for the first profile. For example, if ``skip_first`` is 10 and ``wait`` is 20, the first cycle will\n    wait 10 + 20 = 30 steps before warmup if ``skip_first_wait`` is zero, but will wait only 10\n    steps if ``skip_first_wait`` is non-zero. All subsequent cycles will then wait 20 steps between the\n    last active and warmup.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.profiler.supported_activities",
      "signature": "torch.profiler.supported_activities()",
      "doc": "\n    Returns a set of supported profiler tracing activities.\n\n    Note: profiler uses CUPTI library to trace on-device CUDA kernels.\n    In case when CUDA is enabled but CUPTI is not available, passing\n    ``ProfilerActivity.CUDA`` to profiler results in using the legacy CUDA\n    profiling code (same as in the legacy ``torch.autograd.profiler``).\n    This, in turn, results in including CUDA time in the profiler table output,\n    but not in the JSON trace.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns a set of supported profiler tracing activities.\n\n    Note: profiler uses CUPTI library to trace on-device CUDA kernels.\n    In case when CUDA is enabled but CUPTI is not available, passing\n    ``ProfilerActivity.CUDA`` to profiler results in using the legacy CUDA\n    profiling code (same as in the legacy ``torch.autograd.profiler``).\n    This, in turn, results in including CUDA time in the profiler table output,\n    but not in the JSON trace.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.profiler.tensorboard_trace_handler",
      "signature": "torch.profiler.tensorboard_trace_handler(dir_name: str, worker_name: Optional[str] = None, use_gzip: bool = False)",
      "doc": "\n    Outputs tracing files to directory of ``dir_name``, then that directory can be\n    directly delivered to tensorboard as logdir.\n    ``worker_name`` should be unique for each worker in distributed scenario,\n    it will be set to '[hostname]_[pid]' by default.\n    ",
      "arguments": [
        "dir_name",
        "worker_name",
        "use_gzip"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Outputs tracing files to directory of ``dir_name``, then that directory can be\n    directly delivered to tensorboard as logdir.\n    ``worker_name`` should be unique for each worker in distributed scenario,\n    it will be set to '[hostname]_[pid]' by default.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "QUANTIZATION": [
    {
      "function": "torch.quantization.add_quant_dequant",
      "signature": "torch.quantization.add_quant_dequant(module)",
      "doc": "Wrap the leaf child module in QuantWrapper if it has a valid qconfig\n    Note that this function will modify the children of module inplace and it\n    can return a new module which wraps the input module as well.\n\n    Args:\n        module: input module with qconfig attributes for all the leaf modules\n        that we want to quantize\n\n    Return:\n        Either the inplace modified module with submodules wrapped in\n        `QuantWrapper` based on qconfig or a new `QuantWrapper` module which\n        wraps the input module, the latter case only happens when the input\n        module is a leaf module and we want to quantize it.\n    ",
      "arguments": [
        "module"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Wrap the leaf child module in QuantWrapper if it has a valid qconfig\n    Note that this function will modify the children of module inplace and it\n    can return a new module which wraps the input module as well.\n\n    Args:\n        module: input module with qconfig attributes for all the leaf modules\n        that we want to quantize\n\n    Return:\n        Either the inplace modified module with submodules wrapped in\n        `QuantWrapper` based on qconfig or a new `QuantWrapper` module which\n        wraps the input module, the latter case only happens when the input\n        module is a leaf module and we want to quantize it.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.convert",
      "signature": "torch.quantization.convert(module, mapping=None, inplace=False, remove_qconfig=True, is_reference=False, convert_custom_config_dict=None, use_precomputed_fake_quant=False)",
      "doc": "Converts submodules in input module to a different module according to `mapping`\n    by calling `from_float` method on the target module class. And remove qconfig at the\n    end if remove_qconfig is set to True.\n\n    Args:\n        `module`: prepared and calibrated module\n        `mapping`: a dictionary that maps from source module type to target\n                   module type, can be overwritten to allow swapping user defined\n                   Modules\n        `inplace`: carry out model transformations in-place, the original module\n                   is mutated\n        `convert_custom_config_dict`: custom configuration dictionary for convert function\n        `use_precomputed_fake_quant`: a flag to enable use of precomputed fake quant\n\n    .. code-block:: python\n\n       # Example of convert_custom_config_dict:\n       convert_custom_config_dict = {\n           # user will manually define the corresponding quantized\n           # module class which has a from_observed class method that converts\n           # observed custom module to quantized custom module\n           \"observed_to_quantized_custom_module_class\": {\n               ObservedCustomModule: QuantizedCustomModule\n           }\n       }\n\n    ",
      "arguments": [
        "module",
        "mapping",
        "inplace",
        "remove_qconfig",
        "is_reference",
        "convert_custom_config_dict",
        "use_precomputed_fake_quant"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Converts submodules in input module to a different module according to `mapping`\n    by calling `from_float` method on the target module class. And remove qconfig at the\n    end if remove_qconfig is set to True.\n\n    Args:\n        `module`: prepared and calibrated module\n        `mapping`: a dictionary that maps from source module type to target\n                   module type, can be overwritten to allow swapping user defined\n                   Modules\n        `inplace`: carry out model transformations in-place, the original module\n                   is mutated\n        `convert_custom_config_dict`: custom configuration dictionary for convert function\n        `use_precomputed_fake_quant`: a flag to enable use of precomputed fake quant\n\n    .. code-block:: python\n\n       # Example of convert_custom_config_dict:\n       convert_custom_config_dict = {\n           # user will manually define the corresponding quantized\n           # module class which has a from_observed class method that converts\n           # observed custom module to quantized custom module\n           \"observed_to_quantized_custom_module_class\": {\n               ObservedCustomModule: QuantizedCustomModule\n           }\n       }\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.convert_dynamic_jit",
      "signature": "torch.quantization.convert_dynamic_jit(model, inplace=False, debug=False, preserved_attrs=None)",
      "doc": "",
      "arguments": [
        "model",
        "inplace",
        "debug",
        "preserved_attrs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.convert_jit",
      "signature": "torch.quantization.convert_jit(model, inplace=False, debug=False, preserved_attrs=None)",
      "doc": "",
      "arguments": [
        "model",
        "inplace",
        "debug",
        "preserved_attrs"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.default_eval_fn",
      "signature": "torch.quantization.default_eval_fn(model, calib_data)",
      "doc": "\n    Default evaluation function takes a torch.utils.data.Dataset or a list of\n    input Tensors and run the model on the dataset\n    ",
      "arguments": [
        "model",
        "calib_data"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Default evaluation function takes a torch.utils.data.Dataset or a list of\n    input Tensors and run the model on the dataset\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.disable_fake_quant",
      "signature": "torch.quantization.disable_fake_quant(mod)",
      "doc": "Disable fake quantization for the module.\n\n    Disable fake quantization for this module, if applicable. Example usage::\n\n      # model is any PyTorch model\n      model.apply(torch.ao.quantization.disable_fake_quant)\n\n    ",
      "arguments": [
        "mod"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Disable fake quantization for the module.\n\n    Disable fake quantization for this module, if applicable. Example usage::\n\n      # model is any PyTorch model\n      model.apply(torch.ao.quantization.disable_fake_quant)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.disable_observer",
      "signature": "torch.quantization.disable_observer(mod)",
      "doc": "Disable observation for this module.\n\n    Disable observation for this module, if applicable. Example usage::\n\n      # model is any PyTorch model\n      model.apply(torch.ao.quantization.disable_observer)\n\n    ",
      "arguments": [
        "mod"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Disable observation for this module.\n\n    Disable observation for this module, if applicable. Example usage::\n\n      # model is any PyTorch model\n      model.apply(torch.ao.quantization.disable_observer)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.enable_fake_quant",
      "signature": "torch.quantization.enable_fake_quant(mod)",
      "doc": "Enable fake quantization for the module.\n\n    Enable fake quantization for this module, if applicable. Example usage::\n\n      # model is any PyTorch model\n      model.apply(torch.ao.quantization.enable_fake_quant)\n\n    ",
      "arguments": [
        "mod"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Enable fake quantization for the module.\n\n    Enable fake quantization for this module, if applicable. Example usage::\n\n      # model is any PyTorch model\n      model.apply(torch.ao.quantization.enable_fake_quant)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.enable_observer",
      "signature": "torch.quantization.enable_observer(mod)",
      "doc": "Enable observation for this module.\n\n    Enable observation for this module, if applicable. Example usage::\n\n      # model is any PyTorch model\n      model.apply(torch.ao.quantization.enable_observer)\n\n    ",
      "arguments": [
        "mod"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Enable observation for this module.\n\n    Enable observation for this module, if applicable. Example usage::\n\n      # model is any PyTorch model\n      model.apply(torch.ao.quantization.enable_observer)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.fuse_conv_bn",
      "signature": "torch.quantization.fuse_conv_bn(is_qat, conv, bn)",
      "doc": "Return the fused the conv and bn modules.\n    Given the conv and bn modules, fuses them and returns the fused module\n\n    Args:\n        is_qat: a flag for whether we are using quantization aware training fusion\n        or post training quantization fusion\n        conv: Module instance of type conv2d/conv3d\n        bn: Spatial BN instance that needs to be fused with the conv\n\n    Examples::\n\n        >>> m1 = nn.Conv2d(10, 20, 3)\n        >>> b1 = nn.BatchNorm2d(20)\n        >>> # xdoctest: +SKIP\n        >>> m2 = fuse_conv_bn(m1, b1)\n    ",
      "arguments": [
        "is_qat",
        "conv",
        "bn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the fused the conv and bn modules.\n    Given the conv and bn modules, fuses them and returns the fused module\n\n    Args:\n        is_qat: a flag for whether we are using quantization aware training fusion\n        or post training quantization fusion\n        conv: Module instance of type conv2d/conv3d\n        bn: Spatial BN instance that needs to be fused with the conv\n\n    Examples::\n\n        >>> m1 = nn.Conv2d(10, 20, 3)\n        >>> b1 = nn.BatchNorm2d(20)\n        >>> # xdoctest: +SKIP\n        >>> m2 = fuse_conv_bn(m1, b1)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.fuse_conv_bn_jit",
      "signature": "torch.quantization.fuse_conv_bn_jit(model, inplace=False)",
      "doc": "Fuse conv - bn module\n    Works for eval model only.\n\n    Args:\n        model: TorchScript model from scripting or tracing\n    ",
      "arguments": [
        "model",
        "inplace"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Fuse conv - bn module\n    Works for eval model only.\n\n    Args:\n        model: TorchScript model from scripting or tracing\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.fuse_conv_bn_relu",
      "signature": "torch.quantization.fuse_conv_bn_relu(is_qat, conv, bn, relu)",
      "doc": "Return the fused conv and bv modules.\n\n    Given the conv and bn modules, fuses them and returns the fused module\n\n    Args:\n        is_qat: a flag for whether we are using quantization aware training fusion\n        or post training quantization fusion\n        conv: Module instance of type conv2d/conv3d\n        bn: Spatial BN instance that needs to be fused with the conv\n\n    Examples::\n\n        >>> m1 = nn.Conv2d(10, 20, 3)\n        >>> b1 = nn.BatchNorm2d(20)\n        >>> r1 = nn.ReLU(inplace=False)\n        >>> # xdoctest: +SKIP\n        >>> m2 = fuse_conv_bn_relu(m1, b1, r1)\n    ",
      "arguments": [
        "is_qat",
        "conv",
        "bn",
        "relu"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the fused conv and bv modules.\n\n    Given the conv and bn modules, fuses them and returns the fused module\n\n    Args:\n        is_qat: a flag for whether we are using quantization aware training fusion\n        or post training quantization fusion\n        conv: Module instance of type conv2d/conv3d\n        bn: Spatial BN instance that needs to be fused with the conv\n\n    Examples::\n\n        >>> m1 = nn.Conv2d(10, 20, 3)\n        >>> b1 = nn.BatchNorm2d(20)\n        >>> r1 = nn.ReLU(inplace=False)\n        >>> # xdoctest: +SKIP\n        >>> m2 = fuse_conv_bn_relu(m1, b1, r1)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.fuse_linear_bn",
      "signature": "torch.quantization.fuse_linear_bn(is_qat, linear, bn)",
      "doc": "Return the fused linear and bn modules.\n    Given the linear and bn modules, fuses them and returns the fused module\n\n    Args:\n        is_qat: a flag for whether we are using quantization aware training fusion\n        or post training quantization fusion\n        linear: Module instance of type Linear\n        bn: BatchNorm1d instance that needs to be fused with the linear layer\n\n    Examples::\n\n        >>> m1 = nn.Linear(20, 10)\n        >>> b1 = nn.BatchNorm1d(10)\n        >>> # xdoctest: +SKIP\n        >>> m2 = fuse_linear_bn(m1, b1)\n    ",
      "arguments": [
        "is_qat",
        "linear",
        "bn"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the fused linear and bn modules.\n    Given the linear and bn modules, fuses them and returns the fused module\n\n    Args:\n        is_qat: a flag for whether we are using quantization aware training fusion\n        or post training quantization fusion\n        linear: Module instance of type Linear\n        bn: BatchNorm1d instance that needs to be fused with the linear layer\n\n    Examples::\n\n        >>> m1 = nn.Linear(20, 10)\n        >>> b1 = nn.BatchNorm1d(10)\n        >>> # xdoctest: +SKIP\n        >>> m2 = fuse_linear_bn(m1, b1)\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.fuse_modules",
      "signature": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules at 0x112e337e0>, fuse_custom_config_dict=None)",
      "doc": "Fuse a list of modules into a single module.\n\n    Fuses only the following sequence of modules:\n    conv, bn\n    conv, bn, relu\n    conv, relu\n    linear, relu\n    bn, relu\n    All other sequences are left unchanged.\n    For these sequences, replaces the first item in the list\n    with the fused module, replacing the rest of the modules\n    with identity.\n\n    Args:\n        model: Model containing the modules to be fused\n        modules_to_fuse: list of list of module names to fuse. Can also be a list\n                         of strings if there is only a single list of modules to fuse.\n        inplace: bool specifying if fusion happens in place on the model, by default\n                 a new model is returned\n        fuser_func: Function that takes in a list of modules and outputs a list of fused modules\n                    of the same length. For example,\n                    fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()]\n                    Defaults to torch.ao.quantization.fuse_known_modules\n        `fuse_custom_config_dict`: custom configuration for fusion\n\n    .. code-block:: python\n\n       # Example of fuse_custom_config_dict\n       fuse_custom_config_dict = {\n           # Additional fuser_method mapping\n           \"additional_fuser_method_mapping\": {\n               (torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn\n           },\n       }\n\n    Returns:\n        model with fused modules. A new copy is created if inplace=True.\n\n    Examples::\n\n            >>> # xdoctest: +SKIP\n            >>> m = M().eval()\n            >>> # m is a module containing the sub-modules below\n            >>> modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]\n            >>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)\n            >>> output = fused_m(input)\n\n            >>> m = M().eval()\n            >>> # Alternately provide a single list of modules to fuse\n            >>> modules_to_fuse = ['conv1', 'bn1', 'relu1']\n            >>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)\n            >>> output = fused_m(input)\n\n    ",
      "arguments": [
        "model",
        "modules_to_fuse",
        "inplace",
        "fuser_func",
        "fuse_custom_config_dict"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Fuse a list of modules into a single module.\n\n    Fuses only the following sequence of modules:\n    conv, bn\n    conv, bn, relu\n    conv, relu\n    linear, relu\n    bn, relu\n    All other sequences are left unchanged.\n    For these sequences, replaces the first item in the list\n    with the fused module, replacing the rest of the modules\n    with identity.\n\n    Args:\n        model: Model containing the modules to be fused\n        modules_to_fuse: list of list of module names to fuse. Can also be a list\n                         of strings if there is only a single list of modules to fuse.\n        inplace: bool specifying if fusion happens in place on the model, by default\n                 a new model is returned\n        fuser_func: Function that takes in a list of modules and outputs a list of fused modules\n                    of the same length. For example,\n                    fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()]\n                    Defaults to torch.ao.quantization.fuse_known_modules\n        `fuse_custom_config_dict`: custom configuration for fusion\n\n    .. code-block:: python\n\n       # Example of fuse_custom_config_dict\n       fuse_custom_config_dict = {\n           # Additional fuser_method mapping\n           \"additional_fuser_method_mapping\": {\n               (torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn\n           },\n       }\n\n    Returns:\n        model with fused modules. A new copy is created if inplace=True.\n\n    Examples::\n\n            >>> # xdoctest: +SKIP\n            >>> m = M().eval()\n            >>> # m is a module containing the sub-modules below\n            >>> modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]\n            >>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)\n            >>> output = fused_m(input)\n\n            >>> m = M().eval()\n            >>> # Alternately provide a single list of modules to fuse\n            >>> modules_to_fuse = ['conv1', 'bn1', 'relu1']\n            >>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)\n            >>> output = fused_m(input)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_default_compare_output_module_list",
      "signature": "torch.quantization.get_default_compare_output_module_list() -> set[typing.Callable]",
      "doc": "Get list of module class types that we will record output\n    in numeric suite\n    ",
      "arguments": [],
      "return_type": "set[typing.Callable]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get list of module class types that we will record output\n    in numeric suite\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_default_dynamic_quant_module_mappings",
      "signature": "torch.quantization.get_default_dynamic_quant_module_mappings() -> dict[typing.Callable, typing.Any]",
      "doc": "Get module mapping for post training dynamic quantization",
      "arguments": [],
      "return_type": "dict[typing.Callable, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get module mapping for post training dynamic quantization",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_default_float_to_quantized_operator_mappings",
      "signature": "torch.quantization.get_default_float_to_quantized_operator_mappings() -> dict[typing.Union[typing.Callable, str], typing.Callable]",
      "doc": "",
      "arguments": [],
      "return_type": "dict[typing.Union[typing.Callable, str], typing.Callable]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_default_qat_module_mappings",
      "signature": "torch.quantization.get_default_qat_module_mappings() -> dict[typing.Callable, typing.Any]",
      "doc": "Get default module mapping for quantization aware training",
      "arguments": [],
      "return_type": "dict[typing.Callable, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get default module mapping for quantization aware training",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_default_qat_qconfig",
      "signature": "torch.quantization.get_default_qat_qconfig(backend='x86', version=1)",
      "doc": "\n    Returns the default QAT qconfig for the specified backend.\n\n    Args:\n      * `backend` (str): a string representing the target backend. Currently supports\n        `x86` (default), `fbgemm`, `qnnpack` and `onednn`.\n      * `version`: version, for backwards compatibility. Can be `None` or `1`.\n\n    Return:\n        qconfig\n    ",
      "arguments": [
        "backend",
        "version"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns the default QAT qconfig for the specified backend.\n\n    Args:\n      * `backend` (str): a string representing the target backend. Currently supports\n        `x86` (default), `fbgemm`, `qnnpack` and `onednn`.\n      * `version`: version, for backwards compatibility. Can be `None` or `1`.\n\n    Return:\n        qconfig\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_default_qconfig",
      "signature": "torch.quantization.get_default_qconfig(backend='x86', version=0)",
      "doc": "\n    Returns the default PTQ qconfig for the specified backend.\n\n    Args:\n      * `backend` (str): a string representing the target backend. Currently supports\n        `x86` (default), `fbgemm`, `qnnpack` and `onednn`.\n\n    Return:\n        qconfig\n    ",
      "arguments": [
        "backend",
        "version"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns the default PTQ qconfig for the specified backend.\n\n    Args:\n      * `backend` (str): a string representing the target backend. Currently supports\n        `x86` (default), `fbgemm`, `qnnpack` and `onednn`.\n\n    Return:\n        qconfig\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_default_qconfig_propagation_list",
      "signature": "torch.quantization.get_default_qconfig_propagation_list() -> set[typing.Callable]",
      "doc": "Get the default list of module types that we'll attach qconfig\n    attribute to in prepare\n    ",
      "arguments": [],
      "return_type": "set[typing.Callable]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get the default list of module types that we'll attach qconfig\n    attribute to in prepare\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_default_static_quant_module_mappings",
      "signature": "torch.quantization.get_default_static_quant_module_mappings() -> dict[typing.Callable, typing.Any]",
      "doc": "Get module mapping for post training static quantization",
      "arguments": [],
      "return_type": "dict[typing.Callable, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get module mapping for post training static quantization",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_dynamic_quant_module_class",
      "signature": "torch.quantization.get_dynamic_quant_module_class(float_module_class: Callable, additional_dynamic_quant_mapping: Optional[dict[Callable, Any]] = None) -> Any",
      "doc": "n Get the dynamically quantized module class corresponding to\n    the floating point module class\n    ",
      "arguments": [
        "float_module_class",
        "additional_dynamic_quant_mapping"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "n Get the dynamically quantized module class corresponding to\n    the floating point module class\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_fuser_method",
      "signature": "torch.quantization.get_fuser_method(op_list, additional_fuser_method_mapping=None)",
      "doc": "Get fuser method for the given list of module types.\n\n    Get fuser method for the given list of module types,\n    return None if fuser method does not exist\n    ",
      "arguments": [
        "op_list",
        "additional_fuser_method_mapping"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get fuser method for the given list of module types.\n\n    Get fuser method for the given list of module types,\n    return None if fuser method does not exist\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_observer_state_dict",
      "signature": "torch.quantization.get_observer_state_dict(mod)",
      "doc": "\n    Returns the state dict corresponding to the observer stats.\n    Traverse the model state_dict and extract out the stats.\n    ",
      "arguments": [
        "mod"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns the state dict corresponding to the observer stats.\n    Traverse the model state_dict and extract out the stats.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_quantized_operator",
      "signature": "torch.quantization.get_quantized_operator(float_op: Union[Callable, str]) -> Callable",
      "doc": "Get the quantized operator corresponding to the float operator",
      "arguments": [
        "float_op"
      ],
      "return_type": "typing.Callable",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get the quantized operator corresponding to the float operator",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.get_static_quant_module_class",
      "signature": "torch.quantization.get_static_quant_module_class(float_module_class: Callable, additional_static_quant_mapping: Optional[dict[Callable, Any]] = None, is_reference: bool = False) -> Any",
      "doc": "n Get the statically quantized module class corresponding to\n    the floating point module class\n    ",
      "arguments": [
        "float_module_class",
        "additional_static_quant_mapping",
        "is_reference"
      ],
      "return_type": "typing.Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "n Get the statically quantized module class corresponding to\n    the floating point module class\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.load_observer_state_dict",
      "signature": "torch.quantization.load_observer_state_dict(mod, obs_dict)",
      "doc": "\n    Given input model and a state_dict containing model observer stats,\n    load the stats back into the model. The observer state_dict can be saved\n    using torch.ao.quantization.get_observer_state_dict\n    ",
      "arguments": [
        "mod",
        "obs_dict"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Given input model and a state_dict containing model observer stats,\n    load the stats back into the model. The observer state_dict can be saved\n    using torch.ao.quantization.get_observer_state_dict\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.no_observer_set",
      "signature": "torch.quantization.no_observer_set() -> set[typing.Any]",
      "doc": "These modules cannot have observers inserted by default.",
      "arguments": [],
      "return_type": "set[typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "These modules cannot have observers inserted by default.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.prepare",
      "signature": "torch.quantization.prepare(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None)",
      "doc": "Prepares a copy of the model for quantization calibration or quantization-aware training.\n\n    Quantization configuration should be assigned preemptively\n    to individual submodules in `.qconfig` attribute.\n\n    The model will be attached with observer or fake quant modules, and qconfig\n    will be propagated.\n\n    Args:\n        `model`: input model to be modified in-place\n        `inplace`: carry out model transformations in-place, the original module is mutated\n        `allow_list`: list of quantizable modules\n        `observer_non_leaf_module_list`: list of non-leaf modules we want to add observer\n        `prepare_custom_config_dict`: customization configuration dictionary for prepare function\n\n    .. code-block:: python\n\n       # Example of prepare_custom_config_dict:\n       prepare_custom_config_dict = {\n           # user will manually define the corresponding observed\n           # module class which has a from_float class method that converts\n           # float custom module to observed custom module\n           \"float_to_observed_custom_module_class\": {\n               CustomModule: ObservedCustomModule\n           }\n        }\n\n    ",
      "arguments": [
        "model",
        "inplace",
        "allow_list",
        "observer_non_leaf_module_list",
        "prepare_custom_config_dict"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Prepares a copy of the model for quantization calibration or quantization-aware training.\n\n    Quantization configuration should be assigned preemptively\n    to individual submodules in `.qconfig` attribute.\n\n    The model will be attached with observer or fake quant modules, and qconfig\n    will be propagated.\n\n    Args:\n        `model`: input model to be modified in-place\n        `inplace`: carry out model transformations in-place, the original module is mutated\n        `allow_list`: list of quantizable modules\n        `observer_non_leaf_module_list`: list of non-leaf modules we want to add observer\n        `prepare_custom_config_dict`: customization configuration dictionary for prepare function\n\n    .. code-block:: python\n\n       # Example of prepare_custom_config_dict:\n       prepare_custom_config_dict = {\n           # user will manually define the corresponding observed\n           # module class which has a from_float class method that converts\n           # float custom module to observed custom module\n           \"float_to_observed_custom_module_class\": {\n               CustomModule: ObservedCustomModule\n           }\n        }\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.prepare_dynamic_jit",
      "signature": "torch.quantization.prepare_dynamic_jit(model, qconfig_dict, inplace=False)",
      "doc": "",
      "arguments": [
        "model",
        "qconfig_dict",
        "inplace"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.prepare_jit",
      "signature": "torch.quantization.prepare_jit(model, qconfig_dict, inplace=False)",
      "doc": "",
      "arguments": [
        "model",
        "qconfig_dict",
        "inplace"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.prepare_qat",
      "signature": "torch.quantization.prepare_qat(model, mapping=None, inplace=False)",
      "doc": "\n    Prepares a copy of the model for quantization calibration or\n    quantization-aware training and converts it to quantized version.\n\n    Quantization configuration should be assigned preemptively\n    to individual submodules in `.qconfig` attribute.\n\n    Args:\n        model: input model to be modified in-place\n        mapping: dictionary that maps float modules to quantized modules to be\n                 replaced.\n        inplace: carry out model transformations in-place, the original module\n                 is mutated\n    ",
      "arguments": [
        "model",
        "mapping",
        "inplace"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Prepares a copy of the model for quantization calibration or\n    quantization-aware training and converts it to quantized version.\n\n    Quantization configuration should be assigned preemptively\n    to individual submodules in `.qconfig` attribute.\n\n    Args:\n        model: input model to be modified in-place\n        mapping: dictionary that maps float modules to quantized modules to be\n                 replaced.\n        inplace: carry out model transformations in-place, the original module\n                 is mutated\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.propagate_qconfig_",
      "signature": "torch.quantization.propagate_qconfig_(module, qconfig_dict=None, prepare_custom_config_dict=None)",
      "doc": "Propagate qconfig through the module hierarchy and assign `qconfig`\n    attribute on each leaf module\n\n    Args:\n        module: input module\n        qconfig_dict: dictionary that maps from name or type of submodule to\n            quantization configuration, qconfig applies to all submodules of a\n            given module unless qconfig for the submodules are specified (when\n            the submodule already has qconfig attribute)\n        prepare_custom_config_dict: dictionary for custom handling of modules\n            see docs for :func:`~torch.ao.quantization.prepare_fx`\n\n    Return:\n        None, module is modified inplace with qconfig attached\n    ",
      "arguments": [
        "module",
        "qconfig_dict",
        "prepare_custom_config_dict"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Propagate qconfig through the module hierarchy and assign `qconfig`\n    attribute on each leaf module\n\n    Args:\n        module: input module\n        qconfig_dict: dictionary that maps from name or type of submodule to\n            quantization configuration, qconfig applies to all submodules of a\n            given module unless qconfig for the submodules are specified (when\n            the submodule already has qconfig attribute)\n        prepare_custom_config_dict: dictionary for custom handling of modules\n            see docs for :func:`~torch.ao.quantization.prepare_fx`\n\n    Return:\n        None, module is modified inplace with qconfig attached\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.qconfig_equals",
      "signature": "torch.quantization.qconfig_equals(q1: typing.Optional[torch.ao.quantization.qconfig.QConfig], q2: typing.Optional[torch.ao.quantization.qconfig.QConfig])",
      "doc": "\n    Returns `True` if `q1` equals `q2`, and `False` otherwise.\n    ",
      "arguments": [
        "q1",
        "q2"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Returns `True` if `q1` equals `q2`, and `False` otherwise.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.quantize",
      "signature": "torch.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False)",
      "doc": "Quantize the input float model with post training static quantization.\n\n    First it will prepare the model for calibration, then it calls\n    `run_fn` which will run the calibration step, after that we will\n    convert the model to a quantized model.\n\n    Args:\n        model: input float model\n        run_fn: a calibration function for calibrating the prepared model\n        run_args: positional arguments for `run_fn`\n        inplace: carry out model transformations in-place, the original module is mutated\n        mapping: correspondence between original module types and quantized counterparts\n\n    Return:\n        Quantized model.\n    ",
      "arguments": [
        "model",
        "run_fn",
        "run_args",
        "mapping",
        "inplace"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Quantize the input float model with post training static quantization.\n\n    First it will prepare the model for calibration, then it calls\n    `run_fn` which will run the calibration step, after that we will\n    convert the model to a quantized model.\n\n    Args:\n        model: input float model\n        run_fn: a calibration function for calibrating the prepared model\n        run_args: positional arguments for `run_fn`\n        inplace: carry out model transformations in-place, the original module is mutated\n        mapping: correspondence between original module types and quantized counterparts\n\n    Return:\n        Quantized model.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.quantize_dynamic",
      "signature": "torch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)",
      "doc": "Converts a float model to dynamic (i.e. weights-only) quantized model.\n\n    Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.\n\n    For simplest usage provide `dtype` argument that can be float16 or qint8. Weight-only quantization\n    by default is performed for layers with large weights size - i.e. Linear and RNN variants.\n\n    Fine grained control is possible with `qconfig` and `mapping` that act similarly to `quantize()`.\n    If `qconfig` is provided, the `dtype` argument is ignored.\n\n    Args:\n        model: input model\n        qconfig_spec: Either:\n\n            - A dictionary that maps from name or type of submodule to quantization\n              configuration, qconfig applies to all submodules of a given\n              module unless qconfig for the submodules are specified (when the\n              submodule already has qconfig attribute). Entries in the dictionary\n              need to be QConfig instances.\n\n            - A set of types and/or submodule names to apply dynamic quantization to,\n              in which case the `dtype` argument is used to specify the bit-width\n\n        inplace: carry out model transformations in-place, the original module is mutated\n        mapping: maps type of a submodule to a type of corresponding dynamically quantized version\n            with which the submodule needs to be replaced\n\n    ",
      "arguments": [
        "model",
        "qconfig_spec",
        "dtype",
        "mapping",
        "inplace"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Converts a float model to dynamic (i.e. weights-only) quantized model.\n\n    Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.\n\n    For simplest usage provide `dtype` argument that can be float16 or qint8. Weight-only quantization\n    by default is performed for layers with large weights size - i.e. Linear and RNN variants.\n\n    Fine grained control is possible with `qconfig` and `mapping` that act similarly to `quantize()`.\n    If `qconfig` is provided, the `dtype` argument is ignored.\n\n    Args:\n        model: input model\n        qconfig_spec: Either:\n\n            - A dictionary that maps from name or type of submodule to quantization\n              configuration, qconfig applies to all submodules of a given\n              module unless qconfig for the submodules are specified (when the\n              submodule already has qconfig attribute). Entries in the dictionary\n              need to be QConfig instances.\n\n            - A set of types and/or submodule names to apply dynamic quantization to,\n              in which case the `dtype` argument is used to specify the bit-width\n\n        inplace: carry out model transformations in-place, the original module is mutated\n        mapping: maps type of a submodule to a type of corresponding dynamically quantized version\n            with which the submodule needs to be replaced\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.quantize_dynamic_jit",
      "signature": "torch.quantization.quantize_dynamic_jit(model, qconfig_dict, inplace=False, debug=False)",
      "doc": "Quantize the input float TorchScript model with\n    post training dynamic quantization.\n    Currently only qint8 quantization of torch.nn.Linear is supported.\n\n    Args:\n        `model`: input float TorchScript model\n        `qconfig_dict`: qconfig_dict is a dictionary with names of sub modules as key and\n        qconfig for that module as value, please see detailed\n        descriptions in :func:`~torch.ao.quantization.quantize_jit`\n        `inplace`: carry out model transformations in-place, the original module is\n        mutated\n        `debug`: flag for producing a debug friendly model (preserve weight attribute)\n\n    Return:\n        Quantized TorchSciprt model.\n\n    Example:\n    ```python\n    import torch\n    from torch.ao.quantization import per_channel_dynamic_qconfig\n    from torch.ao.quantization import quantize_dynamic_jit\n\n    ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)\n    qconfig = get_default_qconfig('fbgemm')\n    def calibrate(model, data_loader):\n        model.eval()\n        with torch.no_grad():\n            for image, target in data_loader:\n                model(image)\n\n    quantized_model = quantize_dynamic_jit(\n        ts_model,\n        {'': qconfig},\n        calibrate,\n        [data_loader_test])\n    ```\n    ",
      "arguments": [
        "model",
        "qconfig_dict",
        "inplace",
        "debug"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Quantize the input float TorchScript model with\n    post training dynamic quantization.\n    Currently only qint8 quantization of torch.nn.Linear is supported.\n\n    Args:\n        `model`: input float TorchScript model\n        `qconfig_dict`: qconfig_dict is a dictionary with names of sub modules as key and\n        qconfig for that module as value, please see detailed\n        descriptions in :func:`~torch.ao.quantization.quantize_jit`\n        `inplace`: carry out model transformations in-place, the original module is\n        mutated\n        `debug`: flag for producing a debug friendly model (preserve weight attribute)\n\n    Return:\n        Quantized TorchSciprt model.\n\n    Example:\n    ```python\n    import torch\n    from torch.ao.quantization import per_channel_dynamic_qconfig\n    from torch.ao.quantization import quantize_dynamic_jit\n\n    ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)\n    qconfig = get_default_qconfig('fbgemm')\n    def calibrate(model, data_loader):\n        model.eval()\n        with torch.no_grad():\n            for image, target in data_loader:\n                model(image)\n\n    quantized_model = quantize_dynamic_jit(\n        ts_model,\n        {'': qconfig},\n        calibrate,\n        [data_loader_test])\n    ```\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.quantize_jit",
      "signature": "torch.quantization.quantize_jit(model, qconfig_dict, run_fn, run_args, inplace=False, debug=False)",
      "doc": "Quantize the input float TorchScript model with\n    post training static quantization.\n\n    First it will prepare the model for calibration, then it calls\n    `run_fn` which will run the calibration step, after that we will\n    convert the model to a quantized model.\n\n    Args:\n        `model`: input float TorchScript model\n        `qconfig_dict`: qconfig_dict is a dictionary with names of sub modules as key and\n        qconfig for that module as value, empty key means the qconfig will be applied\n        to whole model unless it's overwritten by more specific configurations, the\n        qconfig for each module is either found in the dictionary or fallback to\n         the qconfig of parent module.\n\n        Right now qconfig_dict is the only way to configure how the model is quantized,\n        and it is done in the granularity of module, that is, we only support one type\n        of qconfig for each torch.nn.Module, and the qconfig for sub module will\n        override the qconfig for parent module, empty string means global configuration.\n        `run_fn`: a calibration function for calibrating the prepared model\n        `run_args`: positional arguments for `run_fn`\n        `inplace`: carry out model transformations in-place, the original module is\n        mutated\n        `debug`: flag for producing a debug friendly model (preserve weight attribute)\n\n    Return:\n        Quantized TorchSciprt model.\n\n    Example:\n    ```python\n    import torch\n    from torch.ao.quantization import get_default_qconfig\n    from torch.ao.quantization import quantize_jit\n\n    ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)\n    qconfig = get_default_qconfig('fbgemm')\n    def calibrate(model, data_loader):\n        model.eval()\n        with torch.no_grad():\n            for image, target in data_loader:\n                model(image)\n\n    quantized_model = quantize_jit(\n        ts_model,\n        {'': qconfig},\n        calibrate,\n        [data_loader_test])\n    ```\n    ",
      "arguments": [
        "model",
        "qconfig_dict",
        "run_fn",
        "run_args",
        "inplace",
        "debug"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Quantize the input float TorchScript model with\n    post training static quantization.\n\n    First it will prepare the model for calibration, then it calls\n    `run_fn` which will run the calibration step, after that we will\n    convert the model to a quantized model.\n\n    Args:\n        `model`: input float TorchScript model\n        `qconfig_dict`: qconfig_dict is a dictionary with names of sub modules as key and\n        qconfig for that module as value, empty key means the qconfig will be applied\n        to whole model unless it's overwritten by more specific configurations, the\n        qconfig for each module is either found in the dictionary or fallback to\n         the qconfig of parent module.\n\n        Right now qconfig_dict is the only way to configure how the model is quantized,\n        and it is done in the granularity of module, that is, we only support one type\n        of qconfig for each torch.nn.Module, and the qconfig for sub module will\n        override the qconfig for parent module, empty string means global configuration.\n        `run_fn`: a calibration function for calibrating the prepared model\n        `run_args`: positional arguments for `run_fn`\n        `inplace`: carry out model transformations in-place, the original module is\n        mutated\n        `debug`: flag for producing a debug friendly model (preserve weight attribute)\n\n    Return:\n        Quantized TorchSciprt model.\n\n    Example:\n    ```python\n    import torch\n    from torch.ao.quantization import get_default_qconfig\n    from torch.ao.quantization import quantize_jit\n\n    ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)\n    qconfig = get_default_qconfig('fbgemm')\n    def calibrate(model, data_loader):\n        model.eval()\n        with torch.no_grad():\n            for image, target in data_loader:\n                model(image)\n\n    quantized_model = quantize_jit(\n        ts_model,\n        {'': qconfig},\n        calibrate,\n        [data_loader_test])\n    ```\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.quantize_qat",
      "signature": "torch.quantization.quantize_qat(model, run_fn, run_args, inplace=False)",
      "doc": "Do quantization aware training and output a quantized model\n\n    Args:\n        model: input model\n        run_fn: a function for evaluating the prepared model, can be a\n                function that simply runs the prepared model or a training\n                loop\n        run_args: positional arguments for `run_fn`\n\n    Return:\n        Quantized model.\n    ",
      "arguments": [
        "model",
        "run_fn",
        "run_args",
        "inplace"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Do quantization aware training and output a quantized model\n\n    Args:\n        model: input model\n        run_fn: a function for evaluating the prepared model, can be a\n                function that simply runs the prepared model or a training\n                loop\n        run_args: positional arguments for `run_fn`\n\n    Return:\n        Quantized model.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.script_qconfig",
      "signature": "torch.quantization.script_qconfig(qconfig)",
      "doc": "Instantiate the activation and weight observer modules and script\n    them, these observer module instances will be deepcopied during\n    prepare_jit step.\n    ",
      "arguments": [
        "qconfig"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Instantiate the activation and weight observer modules and script\n    them, these observer module instances will be deepcopied during\n    prepare_jit step.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.script_qconfig_dict",
      "signature": "torch.quantization.script_qconfig_dict(qconfig_dict)",
      "doc": "Helper function used by `prepare_jit`.\n    Apply `script_qconfig` for all entries in `qconfig_dict` that is\n    not None.\n    ",
      "arguments": [
        "qconfig_dict"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Helper function used by `prepare_jit`.\n    Apply `script_qconfig` for all entries in `qconfig_dict` that is\n    not None.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.quantization.swap_module",
      "signature": "torch.quantization.swap_module(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant=False)",
      "doc": "Swaps the module if it has a quantized counterpart and it has an\n    `observer` attached.\n\n    Args:\n        mod: input module\n        mapping: a dictionary that maps from nn module to nnq module\n\n    Return:\n        The corresponding quantized module of `mod`\n    ",
      "arguments": [
        "mod",
        "mapping",
        "custom_module_class_mapping",
        "use_precomputed_fake_quant"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Swaps the module if it has a quantized counterpart and it has an\n    `observer` attached.\n\n    Args:\n        mod: input module\n        mapping: a dictionary that maps from nn module to nnq module\n\n    Return:\n        The corresponding quantized module of `mod`\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "SPARSE_OPERATIONS": [
    {
      "function": "torch.sparse.as_sparse_gradcheck",
      "signature": "torch.sparse.as_sparse_gradcheck(gradcheck)",
      "doc": "Decorate function, to extend gradcheck for sparse tensors.\n\n    Decorator for torch.autograd.gradcheck or its functools.partial\n    variants that extends the gradcheck function with support to input\n    functions that operate on or/and return sparse tensors.\n\n    The specified gradcheck function itself is guaranteed to operate\n    on strided tensors only.\n\n    For example:\n\n    >>> gradcheck = torch.sparse.as_sparse_gradcheck(torch.autograd.gradcheck)\n    >>> x = torch.tensor([[0, 1], [2, 3]], dtype=torch.float64).to_sparse_coo().requires_grad_(True)\n    >>> gradcheck(lambda x: x.to_sparse_csr(), x)\n    True\n    ",
      "arguments": [
        "gradcheck"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Decorate function, to extend gradcheck for sparse tensors.\n\n    Decorator for torch.autograd.gradcheck or its functools.partial\n    variants that extends the gradcheck function with support to input\n    functions that operate on or/and return sparse tensors.\n\n    The specified gradcheck function itself is guaranteed to operate\n    on strided tensors only.\n\n    For example:\n\n    >>> gradcheck = torch.sparse.as_sparse_gradcheck(torch.autograd.gradcheck)\n    >>> x = torch.tensor([[0, 1], [2, 3]], dtype=torch.float64).to_sparse_coo().requires_grad_(True)\n    >>> gradcheck(lambda x: x.to_sparse_csr(), x)\n    True\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sparse.sum",
      "signature": "torch.sparse.sum(input: torch.Tensor, dim: Optional[tuple[int]] = None, dtype: Optional[int] = None) -> torch.Tensor",
      "doc": "Return the sum of each row of the given sparse tensor.\n\n    Returns the sum of each row of the sparse tensor :attr:`input` in the given\n    dimensions :attr:`dim`. If :attr:`dim` is a list of dimensions,\n    reduce over all of them. When sum over all ``sparse_dim``, this method\n    returns a dense tensor instead of a sparse tensor.\n\n    All summed :attr:`dim` are squeezed (see :func:`torch.squeeze`), resulting an output\n    tensor having :attr:`dim` fewer dimensions than :attr:`input`.\n\n    During backward, only gradients at ``nnz`` locations of :attr:`input`\n    will propagate back. Note that the gradients of :attr:`input` is coalesced.\n\n    Args:\n        input (Tensor): the input sparse tensor\n        dim (int or tuple of ints): a dimension or a list of dimensions to reduce. Default: reduce\n            over all dims.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n            Default: dtype of :attr:`input`.\n\n    Example::\n\n        >>> nnz = 3\n        >>> dims = [5, 5, 2, 3]\n        >>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n                           torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n        >>> V = torch.randn(nnz, dims[2], dims[3])\n        >>> size = torch.Size(dims)\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> S = torch.sparse_coo_tensor(I, V, size)\n        >>> S\n        tensor(indices=tensor([[2, 0, 3],\n                               [2, 4, 1]]),\n               values=tensor([[[-0.6438, -1.6467,  1.4004],\n                               [ 0.3411,  0.0918, -0.2312]],\n\n                              [[ 0.5348,  0.0634, -2.0494],\n                               [-0.7125, -1.0646,  2.1844]],\n\n                              [[ 0.1276,  0.1874, -0.6334],\n                               [-1.9682, -0.5340,  0.7483]]]),\n               size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)\n\n        # when sum over only part of sparse_dims, return a sparse tensor\n        >>> torch.sparse.sum(S, [1, 3])\n        tensor(indices=tensor([[0, 2, 3]]),\n               values=tensor([[-1.4512,  0.4073],\n                              [-0.8901,  0.2017],\n                              [-0.3183, -1.7539]]),\n               size=(5, 2), nnz=3, layout=torch.sparse_coo)\n\n        # when sum over all sparse dim, return a dense tensor\n        # with summed dims squeezed\n        >>> torch.sparse.sum(S, [0, 1, 3])\n        tensor([-2.6596, -1.1450])\n    ",
      "arguments": [
        "input",
        "dim",
        "dtype"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the sum of each row of the given sparse tensor.\n\n    Returns the sum of each row of the sparse tensor :attr:`input` in the given\n    dimensions :attr:`dim`. If :attr:`dim` is a list of dimensions,\n    reduce over all of them. When sum over all ``sparse_dim``, this method\n    returns a dense tensor instead of a sparse tensor.\n\n    All summed :attr:`dim` are squeezed (see :func:`torch.squeeze`), resulting an output\n    tensor having :attr:`dim` fewer dimensions than :attr:`input`.\n\n    During backward, only gradients at ``nnz`` locations of :attr:`input`\n    will propagate back. Note that the gradients of :attr:`input` is coalesced.\n\n    Args:\n        input (Tensor): the input sparse tensor\n        dim (int or tuple of ints): a dimension or a list of dimensions to reduce. Default: reduce\n            over all dims.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n            Default: dtype of :attr:`input`.\n\n    Example::\n\n        >>> nnz = 3\n        >>> dims = [5, 5, 2, 3]\n        >>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n                           torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n        >>> V = torch.randn(nnz, dims[2], dims[3])\n        >>> size = torch.Size(dims)\n        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        >>> S = torch.sparse_coo_tensor(I, V, size)\n        >>> S\n        tensor(indices=tensor([[2, 0, 3],\n                               [2, 4, 1]]),\n               values=tensor([[[-0.6438, -1.6467,  1.4004],\n                               [ 0.3411,  0.0918, -0.2312]],\n\n                              [[ 0.5348,  0.0634, -2.0494],\n                               [-0.7125, -1.0646,  2.1844]],\n\n                              [[ 0.1276,  0.1874, -0.6334],\n                               [-1.9682, -0.5340,  0.7483]]]),\n               size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)\n\n        # when sum over only part of sparse_dims, return a sparse tensor\n        >>> torch.sparse.sum(S, [1, 3])\n        tensor(indices=tensor([[0, 2, 3]]),\n               values=tensor([[-1.4512,  0.4073],\n                              [-0.8901,  0.2017],\n                              [-0.3183, -1.7539]]),\n               size=(5, 2), nnz=3, layout=torch.sparse_coo)\n\n        # when sum over all sparse dim, return a dense tensor\n        # with summed dims squeezed\n        >>> torch.sparse.sum(S, [0, 1, 3])\n        tensor([-2.6596, -1.1450])\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.sparse.to_sparse_semi_structured",
      "signature": "torch.sparse.to_sparse_semi_structured(original_tensor: torch.Tensor, transposed: bool = False) -> torch.sparse.semi_structured.SparseSemiStructuredTensor",
      "doc": "\n    This function converts a dense tensor into a sparse semi-structured tensor.\n    It will return a SparseSemiStructuredTensor, a subclass of torch.Tensor.\n\n    This function will check to ensure the dense tensor has the right dtype, size, dims, and device.\n    We currently only support semi-structured sparse tensors for 2d CUDA tensors.\n    Additionally, your tensor must be a positive multiple of the mininum sparse block size, given in\n    `_DTYPE_TO_SHAPE_CONSTRAINTS` for each dtype (float32, float16, bfloat16, int8).\n\n    Args:\n        original_tensor (Tensor): the dense tensor to convert\n        transposed (bool, optional): deprecated arg to be removed in another release. Do not use.\n    Returns:\n        SparseSemiStructuredTensor: A sparse semi-structured tensor created from the given original_tensor\n    Raises:\n        None\n    Example:\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda()\n        tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n                [0., 0., 1.,  ..., 0., 1., 1.],\n                [0., 0., 1.,  ..., 0., 1., 1.],\n                ...,\n                [0., 0., 1.,  ..., 0., 1., 1.],\n                [0., 0., 1.,  ..., 0., 1., 1.],\n                [0., 0., 1.,  ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)\n        >>> A_sparse = to_sparse_semi_structured(A)\n        SparseSemiStructuredTensor(shape=torch.Size([128, 128]))\n        >>> A_sparse.values()\n        tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n                [1., 1., 1.,  ..., 1., 1., 1.],\n                [1., 1., 1.,  ..., 1., 1., 1.],\n                ...,\n                [1., 1., 1.,  ..., 1., 1., 1.],\n                [1., 1., 1.,  ..., 1., 1., 1.],\n                [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16),\n        >>> A_sparse.indices()\n        tensor([[-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                ...,\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370]], device='cuda:0', dtype=torch.int16))\n    ",
      "arguments": [
        "original_tensor",
        "transposed"
      ],
      "return_type": "<class 'torch.sparse.semi_structured.SparseSemiStructuredTensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This function converts a dense tensor into a sparse semi-structured tensor.\n    It will return a SparseSemiStructuredTensor, a subclass of torch.Tensor.\n\n    This function will check to ensure the dense tensor has the right dtype, size, dims, and device.\n    We currently only support semi-structured sparse tensors for 2d CUDA tensors.\n    Additionally, your tensor must be a positive multiple of the mininum sparse block size, given in\n    `_DTYPE_TO_SHAPE_CONSTRAINTS` for each dtype (float32, float16, bfloat16, int8).\n\n    Args:\n        original_tensor (Tensor): the dense tensor to convert\n        transposed (bool, optional): deprecated arg to be removed in another release. Do not use.\n    Returns:\n        SparseSemiStructuredTensor: A sparse semi-structured tensor created from the given original_tensor\n    Raises:\n        None\n    Example:\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda()\n        tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n                [0., 0., 1.,  ..., 0., 1., 1.],\n                [0., 0., 1.,  ..., 0., 1., 1.],\n                ...,\n                [0., 0., 1.,  ..., 0., 1., 1.],\n                [0., 0., 1.,  ..., 0., 1., 1.],\n                [0., 0., 1.,  ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)\n        >>> A_sparse = to_sparse_semi_structured(A)\n        SparseSemiStructuredTensor(shape=torch.Size([128, 128]))\n        >>> A_sparse.values()\n        tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n                [1., 1., 1.,  ..., 1., 1., 1.],\n                [1., 1., 1.,  ..., 1., 1., 1.],\n                ...,\n                [1., 1., 1.,  ..., 1., 1., 1.],\n                [1., 1., 1.,  ..., 1., 1., 1.],\n                [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16),\n        >>> A_sparse.indices()\n        tensor([[-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                ...,\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n                [-4370, -4370, -4370,  ..., -4370, -4370, -4370]], device='cuda:0', dtype=torch.int16))\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "TESTING_UTILITIES": [
    {
      "function": "torch.testing.assert_allclose",
      "signature": "torch.testing.assert_allclose(actual: Any, expected: Any, rtol: Optional[float] = None, atol: Optional[float] = None, equal_nan: bool = True, msg: str = '') -> None",
      "doc": "\n    .. warning::\n\n       :func:`torch.testing.assert_allclose` is deprecated since ``1.12`` and will be removed in a future release.\n       Please use :func:`torch.testing.assert_close` instead. You can find detailed upgrade instructions\n       `here <https://github.com/pytorch/pytorch/issues/61844>`_.\n    ",
      "arguments": [
        "actual",
        "expected",
        "rtol",
        "atol",
        "equal_nan",
        "msg"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    .. warning::\n\n       :func:`torch.testing.assert_allclose` is deprecated since ``1.12`` and will be removed in a future release.\n       Please use :func:`torch.testing.assert_close` instead. You can find detailed upgrade instructions\n       `here <https://github.com/pytorch/pytorch/issues/61844>`_.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.testing.assert_close",
      "signature": "torch.testing.assert_close(actual: Any, expected: Any, *, allow_subclasses: bool = True, rtol: Optional[float] = None, atol: Optional[float] = None, equal_nan: bool = False, check_device: bool = True, check_dtype: bool = True, check_layout: bool = True, check_stride: bool = False, msg: Union[str, Callable[[str], str], NoneType] = None)",
      "doc": "Asserts that ``actual`` and ``expected`` are close.\n\n    If ``actual`` and ``expected`` are strided, non-quantized, real-valued, and finite, they are considered close if\n\n    .. math::\n\n        \\lvert \\text{actual} - \\text{expected} \\rvert \\le \\texttt{atol} + \\texttt{rtol} \\cdot \\lvert \\text{expected} \\rvert\n\n    Non-finite values (``-inf`` and ``inf``) are only considered close if and only if they are equal. ``NaN``'s are\n    only considered equal to each other if ``equal_nan`` is ``True``.\n\n    In addition, they are only considered close if they have the same\n\n    - :attr:`~torch.Tensor.device` (if ``check_device`` is ``True``),\n    - ``dtype`` (if ``check_dtype`` is ``True``),\n    - ``layout`` (if ``check_layout`` is ``True``), and\n    - stride (if ``check_stride`` is ``True``).\n\n    If either ``actual`` or ``expected`` is a meta tensor, only the attribute checks will be performed.\n\n    If ``actual`` and ``expected`` are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are\n    checked individually. Indices, namely ``indices`` for COO, ``crow_indices`` and ``col_indices`` for CSR and BSR,\n    or ``ccol_indices``  and ``row_indices`` for CSC and BSC layouts, respectively,\n    are always checked for equality whereas the values are checked for closeness according to the definition above.\n\n    If ``actual`` and ``expected`` are quantized, they are considered close if they have the same\n    :meth:`~torch.Tensor.qscheme` and the result of :meth:`~torch.Tensor.dequantize` is close according to the\n    definition above.\n\n    ``actual`` and ``expected`` can be :class:`~torch.Tensor`'s or any tensor-or-scalar-likes from which\n    :class:`torch.Tensor`'s can be constructed with :func:`torch.as_tensor`. Except for Python scalars the input types\n    have to be directly related. In addition, ``actual`` and ``expected`` can be :class:`~collections.abc.Sequence`'s\n    or :class:`~collections.abc.Mapping`'s in which case they are considered close if their structure matches and all\n    their elements are considered close according to the above definition.\n\n    .. note::\n\n        Python scalars are an exception to the type relation requirement, because their :func:`type`, i.e.\n        :class:`int`, :class:`float`, and :class:`complex`, is equivalent to the ``dtype`` of a tensor-like. Thus,\n        Python scalars of different types can be checked, but require ``check_dtype=False``.\n\n    Args:\n        actual (Any): Actual input.\n        expected (Any): Expected input.\n        allow_subclasses (bool): If ``True`` (default) and except for Python scalars, inputs of directly related types\n            are allowed. Otherwise type equality is required.\n        rtol (Optional[float]): Relative tolerance. If specified ``atol`` must also be specified. If omitted, default\n            values based on the :attr:`~torch.Tensor.dtype` are selected with the below table.\n        atol (Optional[float]): Absolute tolerance. If specified ``rtol`` must also be specified. If omitted, default\n            values based on the :attr:`~torch.Tensor.dtype` are selected with the below table.\n        equal_nan (Union[bool, str]): If ``True``, two ``NaN`` values will be considered equal.\n        check_device (bool): If ``True`` (default), asserts that corresponding tensors are on the same\n            :attr:`~torch.Tensor.device`. If this check is disabled, tensors on different\n            :attr:`~torch.Tensor.device`'s are moved to the CPU before being compared.\n        check_dtype (bool): If ``True`` (default), asserts that corresponding tensors have the same ``dtype``. If this\n            check is disabled, tensors with different ``dtype``'s are promoted  to a common ``dtype`` (according to\n            :func:`torch.promote_types`) before being compared.\n        check_layout (bool): If ``True`` (default), asserts that corresponding tensors have the same ``layout``. If this\n            check is disabled, tensors with different ``layout``'s are converted to strided tensors before being\n            compared.\n        check_stride (bool): If ``True`` and corresponding tensors are strided, asserts that they have the same stride.\n        msg (Optional[Union[str, Callable[[str], str]]]): Optional error message to use in case a failure occurs during\n            the comparison. Can also passed as callable in which case it will be called with the generated message and\n            should return the new message.\n\n    Raises:\n        ValueError: If no :class:`torch.Tensor` can be constructed from an input.\n        ValueError: If only ``rtol`` or ``atol`` is specified.\n        AssertionError: If corresponding inputs are not Python scalars and are not directly related.\n        AssertionError: If ``allow_subclasses`` is ``False``, but corresponding inputs are not Python scalars and have\n            different types.\n        AssertionError: If the inputs are :class:`~collections.abc.Sequence`'s, but their length does not match.\n        AssertionError: If the inputs are :class:`~collections.abc.Mapping`'s, but their set of keys do not match.\n        AssertionError: If corresponding tensors do not have the same :attr:`~torch.Tensor.shape`.\n        AssertionError: If ``check_layout`` is ``True``, but corresponding tensors do not have the same\n            :attr:`~torch.Tensor.layout`.\n        AssertionError: If only one of corresponding tensors is quantized.\n        AssertionError: If corresponding tensors are quantized, but have different :meth:`~torch.Tensor.qscheme`'s.\n        AssertionError: If ``check_device`` is ``True``, but corresponding tensors are not on the same\n            :attr:`~torch.Tensor.device`.\n        AssertionError: If ``check_dtype`` is ``True``, but corresponding tensors do not have the same ``dtype``.\n        AssertionError: If ``check_stride`` is ``True``, but corresponding strided tensors do not have the same stride.\n        AssertionError: If the values of corresponding tensors are not close according to the definition above.\n\n    The following table displays the default ``rtol`` and ``atol`` for different ``dtype``'s. In case of mismatching\n    ``dtype``'s, the maximum of both tolerances is used.\n\n    +---------------------------+------------+----------+\n    | ``dtype``                 | ``rtol``   | ``atol`` |\n    +===========================+============+==========+\n    | :attr:`~torch.float16`    | ``1e-3``   | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.bfloat16`   | ``1.6e-2`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.float32`    | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.float64`    | ``1e-7``   | ``1e-7`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.complex32`  | ``1e-3``   | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.complex64`  | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.complex128` | ``1e-7``   | ``1e-7`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.quint8`     | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.quint2x4`   | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.quint4x2`   | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.qint8`      | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.qint32`     | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | other                     | ``0.0``    | ``0.0``  |\n    +---------------------------+------------+----------+\n\n    .. note::\n\n        :func:`~torch.testing.assert_close` is highly configurable with strict default settings. Users are encouraged\n        to :func:`~functools.partial` it to fit their use case. For example, if an equality check is needed, one might\n        define an ``assert_equal`` that uses zero tolerances for every ``dtype`` by default:\n\n        >>> import functools\n        >>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n        >>> assert_equal(1e-9, 1e-10)\n        Traceback (most recent call last):\n        ...\n        AssertionError: Scalars are not equal!\n        <BLANKLINE>\n        Expected 1e-10 but got 1e-09.\n        Absolute difference: 9.000000000000001e-10\n        Relative difference: 9.0\n\n    Examples:\n        >>> # tensor to tensor comparison\n        >>> expected = torch.tensor([1e0, 1e-1, 1e-2])\n        >>> actual = torch.acos(torch.cos(expected))\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> # scalar to scalar comparison\n        >>> import math\n        >>> expected = math.sqrt(2.0)\n        >>> actual = 2.0 / math.sqrt(2.0)\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> # numpy array to numpy array comparison\n        >>> import numpy as np\n        >>> expected = np.array([1e0, 1e-1, 1e-2])\n        >>> actual = np.arccos(np.cos(expected))\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> # sequence to sequence comparison\n        >>> import numpy as np\n        >>> # The types of the sequences do not have to match. They only have to have the same\n        >>> # length and their elements have to match.\n        >>> expected = [torch.tensor([1.0]), 2.0, np.array(3.0)]\n        >>> actual = tuple(expected)\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> # mapping to mapping comparison\n        >>> from collections import OrderedDict\n        >>> import numpy as np\n        >>> foo = torch.tensor(1.0)\n        >>> bar = 2.0\n        >>> baz = np.array(3.0)\n        >>> # The types and a possible ordering of mappings do not have to match. They only\n        >>> # have to have the same set of keys and their elements have to match.\n        >>> expected = OrderedDict([(\"foo\", foo), (\"bar\", bar), (\"baz\", baz)])\n        >>> actual = {\"baz\": baz, \"bar\": bar, \"foo\": foo}\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> expected = torch.tensor([1.0, 2.0, 3.0])\n        >>> actual = expected.clone()\n        >>> # By default, directly related instances can be compared\n        >>> torch.testing.assert_close(torch.nn.Parameter(actual), expected)\n        >>> # This check can be made more strict with allow_subclasses=False\n        >>> torch.testing.assert_close(\n        ...     torch.nn.Parameter(actual), expected, allow_subclasses=False\n        ... )\n        Traceback (most recent call last):\n        ...\n        TypeError: No comparison pair was able to handle inputs of type\n        <class 'torch.nn.parameter.Parameter'> and <class 'torch.Tensor'>.\n        >>> # If the inputs are not directly related, they are never considered close\n        >>> torch.testing.assert_close(actual.numpy(), expected)\n        Traceback (most recent call last):\n        ...\n        TypeError: No comparison pair was able to handle inputs of type <class 'numpy.ndarray'>\n        and <class 'torch.Tensor'>.\n        >>> # Exceptions to these rules are Python scalars. They can be checked regardless of\n        >>> # their type if check_dtype=False.\n        >>> torch.testing.assert_close(1.0, 1, check_dtype=False)\n\n        >>> # NaN != NaN by default.\n        >>> expected = torch.tensor(float(\"Nan\"))\n        >>> actual = expected.clone()\n        >>> torch.testing.assert_close(actual, expected)\n        Traceback (most recent call last):\n        ...\n        AssertionError: Scalars are not close!\n        <BLANKLINE>\n        Expected nan but got nan.\n        Absolute difference: nan (up to 1e-05 allowed)\n        Relative difference: nan (up to 1.3e-06 allowed)\n        >>> torch.testing.assert_close(actual, expected, equal_nan=True)\n\n        >>> expected = torch.tensor([1.0, 2.0, 3.0])\n        >>> actual = torch.tensor([1.0, 4.0, 5.0])\n        >>> # The default error message can be overwritten.\n        >>> torch.testing.assert_close(actual, expected, msg=\"Argh, the tensors are not close!\")\n        Traceback (most recent call last):\n        ...\n        AssertionError: Argh, the tensors are not close!\n        >>> # If msg is a callable, it can be used to augment the generated message with\n        >>> # extra information\n        >>> torch.testing.assert_close(\n        ...     actual, expected, msg=lambda msg: f\"Header\\n\\n{msg}\\n\\nFooter\"\n        ... )\n        Traceback (most recent call last):\n        ...\n        AssertionError: Header\n        <BLANKLINE>\n        Tensor-likes are not close!\n        <BLANKLINE>\n        Mismatched elements: 2 / 3 (66.7%)\n        Greatest absolute difference: 2.0 at index (1,) (up to 1e-05 allowed)\n        Greatest relative difference: 1.0 at index (1,) (up to 1.3e-06 allowed)\n        <BLANKLINE>\n        Footer\n    ",
      "arguments": [
        "actual",
        "expected",
        "allow_subclasses",
        "rtol",
        "atol",
        "equal_nan",
        "check_device",
        "check_dtype",
        "check_layout",
        "check_stride",
        "msg"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Asserts that ``actual`` and ``expected`` are close.\n\n    If ``actual`` and ``expected`` are strided, non-quantized, real-valued, and finite, they are considered close if\n\n    .. math::\n\n        \\lvert \\text{actual} - \\text{expected} \\rvert \\le \\texttt{atol} + \\texttt{rtol} \\cdot \\lvert \\text{expected} \\rvert\n\n    Non-finite values (``-inf`` and ``inf``) are only considered close if and only if they are equal. ``NaN``'s are\n    only considered equal to each other if ``equal_nan`` is ``True``.\n\n    In addition, they are only considered close if they have the same\n\n    - :attr:`~torch.Tensor.device` (if ``check_device`` is ``True``),\n    - ``dtype`` (if ``check_dtype`` is ``True``),\n    - ``layout`` (if ``check_layout`` is ``True``), and\n    - stride (if ``check_stride`` is ``True``).\n\n    If either ``actual`` or ``expected`` is a meta tensor, only the attribute checks will be performed.\n\n    If ``actual`` and ``expected`` are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are\n    checked individually. Indices, namely ``indices`` for COO, ``crow_indices`` and ``col_indices`` for CSR and BSR,\n    or ``ccol_indices``  and ``row_indices`` for CSC and BSC layouts, respectively,\n    are always checked for equality whereas the values are checked for closeness according to the definition above.\n\n    If ``actual`` and ``expected`` are quantized, they are considered close if they have the same\n    :meth:`~torch.Tensor.qscheme` and the result of :meth:`~torch.Tensor.dequantize` is close according to the\n    definition above.\n\n    ``actual`` and ``expected`` can be :class:`~torch.Tensor`'s or any tensor-or-scalar-likes from which\n    :class:`torch.Tensor`'s can be constructed with :func:`torch.as_tensor`. Except for Python scalars the input types\n    have to be directly related. In addition, ``actual`` and ``expected`` can be :class:`~collections.abc.Sequence`'s\n    or :class:`~collections.abc.Mapping`'s in which case they are considered close if their structure matches and all\n    their elements are considered close according to the above definition.\n\n    .. note::\n\n        Python scalars are an exception to the type relation requirement, because their :func:`type`, i.e.\n        :class:`int`, :class:`float`, and :class:`complex`, is equivalent to the ``dtype`` of a tensor-like. Thus,\n        Python scalars of different types can be checked, but require ``check_dtype=False``.\n\n    Args:\n        actual (Any): Actual input.\n        expected (Any): Expected input.\n        allow_subclasses (bool): If ``True`` (default) and except for Python scalars, inputs of directly related types\n            are allowed. Otherwise type equality is required.\n        rtol (Optional[float]): Relative tolerance. If specified ``atol`` must also be specified. If omitted, default\n            values based on the :attr:`~torch.Tensor.dtype` are selected with the below table.\n        atol (Optional[float]): Absolute tolerance. If specified ``rtol`` must also be specified. If omitted, default\n            values based on the :attr:`~torch.Tensor.dtype` are selected with the below table.\n        equal_nan (Union[bool, str]): If ``True``, two ``NaN`` values will be considered equal.\n        check_device (bool): If ``True`` (default), asserts that corresponding tensors are on the same\n            :attr:`~torch.Tensor.device`. If this check is disabled, tensors on different\n            :attr:`~torch.Tensor.device`'s are moved to the CPU before being compared.\n        check_dtype (bool): If ``True`` (default), asserts that corresponding tensors have the same ``dtype``. If this\n            check is disabled, tensors with different ``dtype``'s are promoted  to a common ``dtype`` (according to\n            :func:`torch.promote_types`) before being compared.\n        check_layout (bool): If ``True`` (default), asserts that corresponding tensors have the same ``layout``. If this\n            check is disabled, tensors with different ``layout``'s are converted to strided tensors before being\n            compared.\n        check_stride (bool): If ``True`` and corresponding tensors are strided, asserts that they have the same stride.\n        msg (Optional[Union[str, Callable[[str], str]]]): Optional error message to use in case a failure occurs during\n            the comparison. Can also passed as callable in which case it will be called with the generated message and\n            should return the new message.\n\n    Raises:\n        ValueError: If no :class:`torch.Tensor` can be constructed from an input.\n        ValueError: If only ``rtol`` or ``atol`` is specified.\n        AssertionError: If corresponding inputs are not Python scalars and are not directly related.\n        AssertionError: If ``allow_subclasses`` is ``False``, but corresponding inputs are not Python scalars and have\n            different types.\n        AssertionError: If the inputs are :class:`~collections.abc.Sequence`'s, but their length does not match.\n        AssertionError: If the inputs are :class:`~collections.abc.Mapping`'s, but their set of keys do not match.\n        AssertionError: If corresponding tensors do not have the same :attr:`~torch.Tensor.shape`.\n        AssertionError: If ``check_layout`` is ``True``, but corresponding tensors do not have the same\n            :attr:`~torch.Tensor.layout`.\n        AssertionError: If only one of corresponding tensors is quantized.\n        AssertionError: If corresponding tensors are quantized, but have different :meth:`~torch.Tensor.qscheme`'s.\n        AssertionError: If ``check_device`` is ``True``, but corresponding tensors are not on the same\n            :attr:`~torch.Tensor.device`.\n        AssertionError: If ``check_dtype`` is ``True``, but corresponding tensors do not have the same ``dtype``.\n        AssertionError: If ``check_stride`` is ``True``, but corresponding strided tensors do not have the same stride.\n        AssertionError: If the values of corresponding tensors are not close according to the definition above.\n\n    The following table displays the default ``rtol`` and ``atol`` for different ``dtype``'s. In case of mismatching\n    ``dtype``'s, the maximum of both tolerances is used.\n\n    +---------------------------+------------+----------+\n    | ``dtype``                 | ``rtol``   | ``atol`` |\n    +===========================+============+==========+\n    | :attr:`~torch.float16`    | ``1e-3``   | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.bfloat16`   | ``1.6e-2`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.float32`    | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.float64`    | ``1e-7``   | ``1e-7`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.complex32`  | ``1e-3``   | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.complex64`  | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.complex128` | ``1e-7``   | ``1e-7`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.quint8`     | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.quint2x4`   | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.quint4x2`   | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.qint8`      | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | :attr:`~torch.qint32`     | ``1.3e-6`` | ``1e-5`` |\n    +---------------------------+------------+----------+\n    | other                     | ``0.0``    | ``0.0``  |\n    +---------------------------+------------+----------+\n\n    .. note::\n\n        :func:`~torch.testing.assert_close` is highly configurable with strict default settings. Users are encouraged\n        to :func:`~functools.partial` it to fit their use case. For example, if an equality check is needed, one might\n        define an ``assert_equal`` that uses zero tolerances for every ``dtype`` by default:\n\n        >>> import functools\n        >>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n        >>> assert_equal(1e-9, 1e-10)\n        Traceback (most recent call last):\n        ...\n        AssertionError: Scalars are not equal!\n        <BLANKLINE>\n        Expected 1e-10 but got 1e-09.\n        Absolute difference: 9.000000000000001e-10\n        Relative difference: 9.0\n\n    Examples:\n        >>> # tensor to tensor comparison\n        >>> expected = torch.tensor([1e0, 1e-1, 1e-2])\n        >>> actual = torch.acos(torch.cos(expected))\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> # scalar to scalar comparison\n        >>> import math\n        >>> expected = math.sqrt(2.0)\n        >>> actual = 2.0 / math.sqrt(2.0)\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> # numpy array to numpy array comparison\n        >>> import numpy as np\n        >>> expected = np.array([1e0, 1e-1, 1e-2])\n        >>> actual = np.arccos(np.cos(expected))\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> # sequence to sequence comparison\n        >>> import numpy as np\n        >>> # The types of the sequences do not have to match. They only have to have the same\n        >>> # length and their elements have to match.\n        >>> expected = [torch.tensor([1.0]), 2.0, np.array(3.0)]\n        >>> actual = tuple(expected)\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> # mapping to mapping comparison\n        >>> from collections import OrderedDict\n        >>> import numpy as np\n        >>> foo = torch.tensor(1.0)\n        >>> bar = 2.0\n        >>> baz = np.array(3.0)\n        >>> # The types and a possible ordering of mappings do not have to match. They only\n        >>> # have to have the same set of keys and their elements have to match.\n        >>> expected = OrderedDict([(\"foo\", foo), (\"bar\", bar), (\"baz\", baz)])\n        >>> actual = {\"baz\": baz, \"bar\": bar, \"foo\": foo}\n        >>> torch.testing.assert_close(actual, expected)\n\n        >>> expected = torch.tensor([1.0, 2.0, 3.0])\n        >>> actual = expected.clone()\n        >>> # By default, directly related instances can be compared\n        >>> torch.testing.assert_close(torch.nn.Parameter(actual), expected)\n        >>> # This check can be made more strict with allow_subclasses=False\n        >>> torch.testing.assert_close(\n        ...     torch.nn.Parameter(actual), expected, allow_subclasses=False\n        ... )\n        Traceback (most recent call last):\n        ...\n        TypeError: No comparison pair was able to handle inputs of type\n        <class 'torch.nn.parameter.Parameter'> and <class 'torch.Tensor'>.\n        >>> # If the inputs are not directly related, they are never considered close\n        >>> torch.testing.assert_close(actual.numpy(), expected)\n        Traceback (most recent call last):\n        ...\n        TypeError: No comparison pair was able to handle inputs of type <class 'numpy.ndarray'>\n        and <class 'torch.Tensor'>.\n        >>> # Exceptions to these rules are Python scalars. They can be checked regardless of\n        >>> # their type if check_dtype=False.\n        >>> torch.testing.assert_close(1.0, 1, check_dtype=False)\n\n        >>> # NaN != NaN by default.\n        >>> expected = torch.tensor(float(\"Nan\"))\n        >>> actual = expected.clone()\n        >>> torch.testing.assert_close(actual, expected)\n        Traceback (most recent call last):\n        ...\n        AssertionError: Scalars are not close!\n        <BLANKLINE>\n        Expected nan but got nan.\n        Absolute difference: nan (up to 1e-05 allowed)\n        Relative difference: nan (up to 1.3e-06 allowed)\n        >>> torch.testing.assert_close(actual, expected, equal_nan=True)\n\n        >>> expected = torch.tensor([1.0, 2.0, 3.0])\n        >>> actual = torch.tensor([1.0, 4.0, 5.0])\n        >>> # The default error message can be overwritten.\n        >>> torch.testing.assert_close(actual, expected, msg=\"Argh, the tensors are not close!\")\n        Traceback (most recent call last):\n        ...\n        AssertionError: Argh, the tensors are not close!\n        >>> # If msg is a callable, it can be used to augment the generated message with\n        >>> # extra information\n        >>> torch.testing.assert_close(\n        ...     actual, expected, msg=lambda msg: f\"Header\\n\\n{msg}\\n\\nFooter\"\n        ... )\n        Traceback (most recent call last):\n        ...\n        AssertionError: Header\n        <BLANKLINE>\n        Tensor-likes are not close!\n        <BLANKLINE>\n        Mismatched elements: 2 / 3 (66.7%)\n        Greatest absolute difference: 2.0 at index (1,) (up to 1e-05 allowed)\n        Greatest relative difference: 1.0 at index (1,) (up to 1.3e-06 allowed)\n        <BLANKLINE>\n        Footer\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.testing.make_tensor",
      "signature": "torch.testing.make_tensor(*shape: Union[int, torch.Size, list[int], tuple[int, ...]], dtype: torch.dtype, device: Union[str, torch.device], low: Optional[float] = None, high: Optional[float] = None, requires_grad: bool = False, noncontiguous: bool = False, exclude_zero: bool = False, memory_format: Optional[torch.memory_format] = None) -> torch.Tensor",
      "doc": "Creates a tensor with the given :attr:`shape`, :attr:`device`, and :attr:`dtype`, and filled with\n    values uniformly drawn from ``[low, high)``.\n\n    If :attr:`low` or :attr:`high` are specified and are outside the range of the :attr:`dtype`'s representable\n    finite values then they are clamped to the lowest or highest representable finite value, respectively.\n    If ``None``, then the following table describes the default values for :attr:`low` and :attr:`high`,\n    which depend on :attr:`dtype`.\n\n    +---------------------------+------------+----------+\n    | ``dtype``                 | ``low``    | ``high`` |\n    +===========================+============+==========+\n    | boolean type              | ``0``      | ``2``    |\n    +---------------------------+------------+----------+\n    | unsigned integral type    | ``0``      | ``10``   |\n    +---------------------------+------------+----------+\n    | signed integral types     | ``-9``     | ``10``   |\n    +---------------------------+------------+----------+\n    | floating types            | ``-9``     | ``9``    |\n    +---------------------------+------------+----------+\n    | complex types             | ``-9``     | ``9``    |\n    +---------------------------+------------+----------+\n\n    Args:\n        shape (Tuple[int, ...]): Single integer or a sequence of integers defining the shape of the output tensor.\n        dtype (:class:`torch.dtype`): The data type of the returned tensor.\n        device (Union[str, torch.device]): The device of the returned tensor.\n        low (Optional[Number]): Sets the lower limit (inclusive) of the given range. If a number is provided it is\n            clamped to the least representable finite value of the given dtype. When ``None`` (default),\n            this value is determined based on the :attr:`dtype` (see the table above). Default: ``None``.\n        high (Optional[Number]): Sets the upper limit (exclusive) of the given range. If a number is provided it is\n            clamped to the greatest representable finite value of the given dtype. When ``None`` (default) this value\n            is determined based on the :attr:`dtype` (see the table above). Default: ``None``.\n\n            .. deprecated:: 2.1\n\n                Passing ``low==high`` to :func:`~torch.testing.make_tensor` for floating or complex types is deprecated\n                since 2.1 and will be removed in 2.3. Use :func:`torch.full` instead.\n\n        requires_grad (Optional[bool]): If autograd should record operations on the returned tensor. Default: ``False``.\n        noncontiguous (Optional[bool]): If `True`, the returned tensor will be noncontiguous. This argument is\n            ignored if the constructed tensor has fewer than two elements. Mutually exclusive with ``memory_format``.\n        exclude_zero (Optional[bool]): If ``True`` then zeros are replaced with the dtype's small positive value\n            depending on the :attr:`dtype`. For bool and integer types zero is replaced with one. For floating\n            point types it is replaced with the dtype's smallest positive normal number (the \"tiny\" value of the\n            :attr:`dtype`'s :func:`~torch.finfo` object), and for complex types it is replaced with a complex number\n            whose real and imaginary parts are both the smallest positive normal number representable by the complex\n            type. Default ``False``.\n        memory_format (Optional[torch.memory_format]): The memory format of the returned tensor. Mutually exclusive\n            with ``noncontiguous``.\n\n    Raises:\n        ValueError: If ``requires_grad=True`` is passed for integral `dtype`\n        ValueError: If ``low >= high``.\n        ValueError: If either :attr:`low` or :attr:`high` is ``nan``.\n        ValueError: If both :attr:`noncontiguous` and :attr:`memory_format` are passed.\n        TypeError: If :attr:`dtype` isn't supported by this function.\n\n    Examples:\n        >>> # xdoctest: +SKIP\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> from torch.testing import make_tensor\n        >>> # Creates a float tensor with values in [-1, 1)\n        >>> make_tensor((3,), device='cpu', dtype=torch.float32, low=-1, high=1)\n        >>> # xdoctest: +SKIP\n        tensor([ 0.1205, 0.2282, -0.6380])\n        >>> # Creates a bool tensor on CUDA\n        >>> make_tensor((2, 2), device='cuda', dtype=torch.bool)\n        tensor([[False, False],\n                [False, True]], device='cuda:0')\n    ",
      "arguments": [
        "shape",
        "dtype",
        "device",
        "low",
        "high",
        "requires_grad",
        "noncontiguous",
        "exclude_zero",
        "memory_format"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Creates a tensor with the given :attr:`shape`, :attr:`device`, and :attr:`dtype`, and filled with\n    values uniformly drawn from ``[low, high)``.\n\n    If :attr:`low` or :attr:`high` are specified and are outside the range of the :attr:`dtype`'s representable\n    finite values then they are clamped to the lowest or highest representable finite value, respectively.\n    If ``None``, then the following table describes the default values for :attr:`low` and :attr:`high`,\n    which depend on :attr:`dtype`.\n\n    +---------------------------+------------+----------+\n    | ``dtype``                 | ``low``    | ``high`` |\n    +===========================+============+==========+\n    | boolean type              | ``0``      | ``2``    |\n    +---------------------------+------------+----------+\n    | unsigned integral type    | ``0``      | ``10``   |\n    +---------------------------+------------+----------+\n    | signed integral types     | ``-9``     | ``10``   |\n    +---------------------------+------------+----------+\n    | floating types            | ``-9``     | ``9``    |\n    +---------------------------+------------+----------+\n    | complex types             | ``-9``     | ``9``    |\n    +---------------------------+------------+----------+\n\n    Args:\n        shape (Tuple[int, ...]): Single integer or a sequence of integers defining the shape of the output tensor.\n        dtype (:class:`torch.dtype`): The data type of the returned tensor.\n        device (Union[str, torch.device]): The device of the returned tensor.\n        low (Optional[Number]): Sets the lower limit (inclusive) of the given range. If a number is provided it is\n            clamped to the least representable finite value of the given dtype. When ``None`` (default),\n            this value is determined based on the :attr:`dtype` (see the table above). Default: ``None``.\n        high (Optional[Number]): Sets the upper limit (exclusive) of the given range. If a number is provided it is\n            clamped to the greatest representable finite value of the given dtype. When ``None`` (default) this value\n            is determined based on the :attr:`dtype` (see the table above). Default: ``None``.\n\n            .. deprecated:: 2.1\n\n                Passing ``low==high`` to :func:`~torch.testing.make_tensor` for floating or complex types is deprecated\n                since 2.1 and will be removed in 2.3. Use :func:`torch.full` instead.\n\n        requires_grad (Optional[bool]): If autograd should record operations on the returned tensor. Default: ``False``.\n        noncontiguous (Optional[bool]): If `True`, the returned tensor will be noncontiguous. This argument is\n            ignored if the constructed tensor has fewer than two elements. Mutually exclusive with ``memory_format``.\n        exclude_zero (Optional[bool]): If ``True`` then zeros are replaced with the dtype's small positive value\n            depending on the :attr:`dtype`. For bool and integer types zero is replaced with one. For floating\n            point types it is replaced with the dtype's smallest positive normal number (the \"tiny\" value of the\n            :attr:`dtype`'s :func:`~torch.finfo` object), and for complex types it is replaced with a complex number\n            whose real and imaginary parts are both the smallest positive normal number representable by the complex\n            type. Default ``False``.\n        memory_format (Optional[torch.memory_format]): The memory format of the returned tensor. Mutually exclusive\n            with ``noncontiguous``.\n\n    Raises:\n        ValueError: If ``requires_grad=True`` is passed for integral `dtype`\n        ValueError: If ``low >= high``.\n        ValueError: If either :attr:`low` or :attr:`high` is ``nan``.\n        ValueError: If both :attr:`noncontiguous` and :attr:`memory_format` are passed.\n        TypeError: If :attr:`dtype` isn't supported by this function.\n\n    Examples:\n        >>> # xdoctest: +SKIP\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> from torch.testing import make_tensor\n        >>> # Creates a float tensor with values in [-1, 1)\n        >>> make_tensor((3,), device='cpu', dtype=torch.float32, low=-1, high=1)\n        >>> # xdoctest: +SKIP\n        tensor([ 0.1205, 0.2282, -0.6380])\n        >>> # Creates a bool tensor on CUDA\n        >>> make_tensor((2, 2), device='cuda', dtype=torch.bool)\n        tensor([[False, False],\n                [False, True]], device='cuda:0')\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "UTILITIES": [
    {
      "function": "torch.utils.generate_methods_for_privateuse1_backend",
      "signature": "torch.utils.generate_methods_for_privateuse1_backend(for_tensor: bool = True, for_module: bool = True, for_packed_sequence: bool = True, for_storage: bool = False, unsupported_dtype: Optional[list[torch.dtype]] = None) -> None",
      "doc": "\n    Automatically generate attributes and methods for the custom backend after rename privateuse1 backend.\n\n    In the default scenario, storage-related methods will not be generated automatically.\n\n    When you implement kernels for various torch operations, and register them to the PrivateUse1 dispatch key.\n    And call the function torch.rename_privateuse1_backend(\"foo\") to rename your backend name.\n    At this point, you can easily register specific methods and attributes by calling this function.\n    Just like torch.Tensor.foo(), torch.Tensor.is_foo, torch.Storage.foo(), torch.Storage.is_foo.\n\n    Note: We recommend you use generic functions (check devices are equal or to(device=)).\n    We provide these methods for convenience only and they will be \"monkey patched\" onto the objects\n    and so will not be properly typed. For Storage methods generate, if you need to support sparse data storage,\n    you need to extend the implementation yourself.\n\n    Args:\n        for_tensor (bool): whether register related methods for torch.Tensor class.\n        for_module (bool): whether register related methods for torch.nn.Module class.\n        for_storage (bool): whether register related methods for torch.Storage class.\n        unsupported_dtype (List[torch.dtype]): takes effect only when the storage method needs to be generated,\n            indicating that the storage does not support the torch.dtype type.\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"failing\")\n        >>> torch.utils.rename_privateuse1_backend(\"foo\")\n        >>> torch.utils.generate_methods_for_privateuse1_backend()\n        # Then automatically generate backend-related attributes and methods.\n        >>> a = torch.tensor(2).foo()\n        >>> a.is_foo\n        >>> hasattr(torch.nn.Module, 'foo')\n    ",
      "arguments": [
        "for_tensor",
        "for_module",
        "for_packed_sequence",
        "for_storage",
        "unsupported_dtype"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Automatically generate attributes and methods for the custom backend after rename privateuse1 backend.\n\n    In the default scenario, storage-related methods will not be generated automatically.\n\n    When you implement kernels for various torch operations, and register them to the PrivateUse1 dispatch key.\n    And call the function torch.rename_privateuse1_backend(\"foo\") to rename your backend name.\n    At this point, you can easily register specific methods and attributes by calling this function.\n    Just like torch.Tensor.foo(), torch.Tensor.is_foo, torch.Storage.foo(), torch.Storage.is_foo.\n\n    Note: We recommend you use generic functions (check devices are equal or to(device=)).\n    We provide these methods for convenience only and they will be \"monkey patched\" onto the objects\n    and so will not be properly typed. For Storage methods generate, if you need to support sparse data storage,\n    you need to extend the implementation yourself.\n\n    Args:\n        for_tensor (bool): whether register related methods for torch.Tensor class.\n        for_module (bool): whether register related methods for torch.nn.Module class.\n        for_storage (bool): whether register related methods for torch.Storage class.\n        unsupported_dtype (List[torch.dtype]): takes effect only when the storage method needs to be generated,\n            indicating that the storage does not support the torch.dtype type.\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"failing\")\n        >>> torch.utils.rename_privateuse1_backend(\"foo\")\n        >>> torch.utils.generate_methods_for_privateuse1_backend()\n        # Then automatically generate backend-related attributes and methods.\n        >>> a = torch.tensor(2).foo()\n        >>> a.is_foo\n        >>> hasattr(torch.nn.Module, 'foo')\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.utils.get_cpp_backtrace",
      "signature": "torch.utils.get_cpp_backtrace(frames_to_skip=0, maximum_number_of_frames=64) -> str",
      "doc": "\n    Return a string containing the C++ stack trace of the current thread.\n\n    Args:\n        frames_to_skip (int): the number of frames to skip from the top of the stack\n        maximum_number_of_frames (int): the maximum number of frames to return\n    ",
      "arguments": [
        "frames_to_skip",
        "maximum_number_of_frames"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Return a string containing the C++ stack trace of the current thread.\n\n    Args:\n        frames_to_skip (int): the number of frames to skip from the top of the stack\n        maximum_number_of_frames (int): the maximum number of frames to return\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.utils.rename_privateuse1_backend",
      "signature": "torch.utils.rename_privateuse1_backend(backend_name: str) -> None",
      "doc": "\n    Rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.\n\n    The steps are:\n\n    (1) (In C++) implement kernels for various torch operations, and register them\n        to the PrivateUse1 dispatch key.\n    (2) (In python) call torch.utils.rename_privateuse1_backend(\"foo\")\n\n    You can now use \"foo\" as an ordinary device string in python.\n\n    Note: this API can only be called once per process. Attempting to change\n    the external backend after it's already been set will result in an error.\n\n    Note(AMP): If you want to support AMP on your device, you can register a custom backend module.\n    The backend must register a custom backend module with ``torch._register_device_module(\"foo\", BackendModule)``.\n    BackendModule needs to have the following API's:\n\n    (1) ``get_amp_supported_dtype() -> List[torch.dtype]``\n        get the supported dtypes on your \"foo\" device in AMP, maybe the \"foo\" device supports one more dtype.\n\n    Note(random): If you want to support to set seed for your device, BackendModule needs to have the following API's:\n\n    (1) ``_is_in_bad_fork() -> bool``\n        Return ``True`` if now it is in bad_fork, else return ``False``.\n\n    (2) ``manual_seed_all(seed int) -> None``\n        Sets the seed for generating random numbers for your devices.\n\n    (3) ``device_count() -> int``\n        Returns the number of \"foo\"s available.\n\n    (4) ``get_rng_state(device: Union[int, str, torch.device] = 'foo') -> Tensor``\n        Returns a list of ByteTensor representing the random number states of all devices.\n\n    (5) ``set_rng_state(new_state: Tensor, device: Union[int, str, torch.device] = 'foo') -> None``\n        Sets the random number generator state of the specified \"foo\" device.\n\n    And there are some common funcs:\n\n    (1) ``is_available() -> bool``\n        Returns a bool indicating if \"foo\" is currently available.\n\n    (2) ``current_device() -> int``\n        Returns the index of a currently selected device.\n\n    For more details, see https://pytorch.org/tutorials/advanced/extend_dispatcher.html#get-a-dispatch-key-for-your-backend\n    For an existing example, see https://github.com/bdhirsh/pytorch_open_registration_example\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"failing\")\n        >>> torch.utils.rename_privateuse1_backend(\"foo\")\n        # This will work, assuming that you've implemented the right C++ kernels\n        # to implement torch.ones.\n        >>> a = torch.ones(2, device=\"foo\")\n\n    ",
      "arguments": [
        "backend_name"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.\n\n    The steps are:\n\n    (1) (In C++) implement kernels for various torch operations, and register them\n        to the PrivateUse1 dispatch key.\n    (2) (In python) call torch.utils.rename_privateuse1_backend(\"foo\")\n\n    You can now use \"foo\" as an ordinary device string in python.\n\n    Note: this API can only be called once per process. Attempting to change\n    the external backend after it's already been set will result in an error.\n\n    Note(AMP): If you want to support AMP on your device, you can register a custom backend module.\n    The backend must register a custom backend module with ``torch._register_device_module(\"foo\", BackendModule)``.\n    BackendModule needs to have the following API's:\n\n    (1) ``get_amp_supported_dtype() -> List[torch.dtype]``\n        get the supported dtypes on your \"foo\" device in AMP, maybe the \"foo\" device supports one more dtype.\n\n    Note(random): If you want to support to set seed for your device, BackendModule needs to have the following API's:\n\n    (1) ``_is_in_bad_fork() -> bool``\n        Return ``True`` if now it is in bad_fork, else return ``False``.\n\n    (2) ``manual_seed_all(seed int) -> None``\n        Sets the seed for generating random numbers for your devices.\n\n    (3) ``device_count() -> int``\n        Returns the number of \"foo\"s available.\n\n    (4) ``get_rng_state(device: Union[int, str, torch.device] = 'foo') -> Tensor``\n        Returns a list of ByteTensor representing the random number states of all devices.\n\n    (5) ``set_rng_state(new_state: Tensor, device: Union[int, str, torch.device] = 'foo') -> None``\n        Sets the random number generator state of the specified \"foo\" device.\n\n    And there are some common funcs:\n\n    (1) ``is_available() -> bool``\n        Returns a bool indicating if \"foo\" is currently available.\n\n    (2) ``current_device() -> int``\n        Returns the index of a currently selected device.\n\n    For more details, see https://pytorch.org/tutorials/advanced/extend_dispatcher.html#get-a-dispatch-key-for-your-backend\n    For an existing example, see https://github.com/bdhirsh/pytorch_open_registration_example\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"failing\")\n        >>> torch.utils.rename_privateuse1_backend(\"foo\")\n        # This will work, assuming that you've implemented the right C++ kernels\n        # to implement torch.ones.\n        >>> a = torch.ones(2, device=\"foo\")\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.utils.set_module",
      "signature": "torch.utils.set_module(obj, mod)",
      "doc": "\n    Set the module attribute on a python object for a given object for nicer printing\n    ",
      "arguments": [
        "obj",
        "mod"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    Set the module attribute on a python object for a given object for nicer printing\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.utils.swap_tensors",
      "signature": "torch.utils.swap_tensors(t1, t2)",
      "doc": "\n    This function swaps the content of the two Tensor objects.\n    At a high level, this will make t1 have the content of t2 while preserving\n    its identity.\n\n    This will not work if t1 and t2 have different slots.\n    ",
      "arguments": [
        "t1",
        "t2"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "\n    This function swaps the content of the two Tensor objects.\n    At a high level, this will make t1 have the content of t2 while preserving\n    its identity.\n\n    This will not work if t1 and t2 have different slots.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ],
  "XPU_OPERATIONS": [
    {
      "function": "torch.xpu.current_device",
      "signature": "torch.xpu.current_device() -> int",
      "doc": "Return the index of a currently selected device.",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the index of a currently selected device.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.current_stream",
      "signature": "torch.xpu.current_stream(device: Union[torch.device, str, int, NoneType] = None) -> torch.xpu.streams.Stream",
      "doc": "Return the currently selected :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the currently selected :class:`Stream` for the current device, given\n            by :func:`~torch.xpu.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.xpu.streams.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the currently selected :class:`Stream` for a given device.\n\n    Args:\n        device (torch.device or int, optional): selected device. Returns\n            the currently selected :class:`Stream` for the current device, given\n            by :func:`~torch.xpu.current_device`, if :attr:`device` is ``None``\n            (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.empty_cache",
      "signature": "torch.xpu.empty_cache() -> None",
      "doc": "Release all unoccupied cached memory currently held by the caching\n    allocator so that those can be used in other XPU application.\n\n    .. note::\n        :func:`~torch.xpu.empty_cache` doesn't increase the amount of XPU\n        memory available for PyTorch. However, it may help reduce fragmentation\n        of XPU memory in certain cases.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Release all unoccupied cached memory currently held by the caching\n    allocator so that those can be used in other XPU application.\n\n    .. note::\n        :func:`~torch.xpu.empty_cache` doesn't increase the amount of XPU\n        memory available for PyTorch. However, it may help reduce fragmentation\n        of XPU memory in certain cases.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.get_arch_list",
      "signature": "torch.xpu.get_arch_list() -> list[str]",
      "doc": "Return list XPU architectures this library was compiled for.",
      "arguments": [],
      "return_type": "list[str]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return list XPU architectures this library was compiled for.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.get_device_name",
      "signature": "torch.xpu.get_device_name(device: Union[torch.device, str, int, NoneType] = None) -> str",
      "doc": "Get the name of a device.\n\n    Args:\n        device (torch.device or int or str, optional): device for which to\n            return the name. This function is a no-op if this argument is a\n            negative integer. It uses the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Returns:\n        str: the name of the device\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get the name of a device.\n\n    Args:\n        device (torch.device or int or str, optional): device for which to\n            return the name. This function is a no-op if this argument is a\n            negative integer. It uses the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Returns:\n        str: the name of the device\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.get_device_properties",
      "signature": "torch.xpu.get_device_properties(device: Union[torch.device, str, int, NoneType] = None) -> torch._utils._XpuDeviceProperties",
      "doc": "Get the properties of a device.\n\n    Args:\n        device (torch.device or int or str): device for which to return the\n            properties of the device.\n\n    Returns:\n        _XpuDeviceProperties: the properties of the device\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch._utils._XpuDeviceProperties'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Get the properties of a device.\n\n    Args:\n        device (torch.device or int or str): device for which to return the\n            properties of the device.\n\n    Returns:\n        _XpuDeviceProperties: the properties of the device\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.get_gencode_flags",
      "signature": "torch.xpu.get_gencode_flags() -> str",
      "doc": "Return XPU AOT(ahead-of-time) build flags this library was compiled with.",
      "arguments": [],
      "return_type": "<class 'str'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return XPU AOT(ahead-of-time) build flags this library was compiled with.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.get_rng_state",
      "signature": "torch.xpu.get_rng_state(device: Union[int, str, torch.device] = 'xpu') -> torch.Tensor",
      "doc": "Return the random number generator state of the specified GPU as a ByteTensor.\n\n    Args:\n        device (torch.device or int, optional): The device to return the RNG state of.\n            Default: ``'xpu'`` (i.e., ``torch.device('xpu')``, the current XPU device).\n\n    .. warning::\n        This function eagerly initializes XPU.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'torch.Tensor'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the random number generator state of the specified GPU as a ByteTensor.\n\n    Args:\n        device (torch.device or int, optional): The device to return the RNG state of.\n            Default: ``'xpu'`` (i.e., ``torch.device('xpu')``, the current XPU device).\n\n    .. warning::\n        This function eagerly initializes XPU.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.get_rng_state_all",
      "signature": "torch.xpu.get_rng_state_all() -> list[torch.Tensor]",
      "doc": "Return a list of ByteTensor representing the random number states of all devices.",
      "arguments": [],
      "return_type": "list[torch.Tensor]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a list of ByteTensor representing the random number states of all devices.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.get_stream_from_external",
      "signature": "torch.xpu.get_stream_from_external(data_ptr: int, device: Union[torch.device, str, int, NoneType] = None) -> torch.xpu.streams.Stream",
      "doc": "Return a :class:`Stream` from an external SYCL queue.\n\n    This function is used to wrap SYCL queue created in other libraries in order\n    to facilitate data exchange and multi-library interactions.\n\n    .. note:: This function doesn't manage the queue life-cycle, it is the user\n       responsibility to keep the referenced queue alive while this returned stream is\n       being used. The different SYCL queue pointers will result in distinct\n       :class:`Stream` objects, even if the SYCL queues they dereference are equivalent.\n\n    Args:\n        data_ptr(int): Integer representation of the `sycl::queue*` value passed externally.\n        device(torch.device or int, optional): the device where the queue was originally created.\n            It is the user responsibility to ensure the device is specified correctly.\n    ",
      "arguments": [
        "data_ptr",
        "device"
      ],
      "return_type": "<class 'torch.xpu.streams.Stream'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a :class:`Stream` from an external SYCL queue.\n\n    This function is used to wrap SYCL queue created in other libraries in order\n    to facilitate data exchange and multi-library interactions.\n\n    .. note:: This function doesn't manage the queue life-cycle, it is the user\n       responsibility to keep the referenced queue alive while this returned stream is\n       being used. The different SYCL queue pointers will result in distinct\n       :class:`Stream` objects, even if the SYCL queues they dereference are equivalent.\n\n    Args:\n        data_ptr(int): Integer representation of the `sycl::queue*` value passed externally.\n        device(torch.device or int, optional): the device where the queue was originally created.\n            It is the user responsibility to ensure the device is specified correctly.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.init",
      "signature": "torch.xpu.init()",
      "doc": "Initialize PyTorch's XPU state.\n    This is a Python API about lazy initialization that avoids initializing\n    XPU until the first time it is accessed. Does nothing if the XPU state is\n    already initialized.\n    ",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Initialize PyTorch's XPU state.\n    This is a Python API about lazy initialization that avoids initializing\n    XPU until the first time it is accessed. Does nothing if the XPU state is\n    already initialized.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.initial_seed",
      "signature": "torch.xpu.initial_seed() -> int",
      "doc": "Return the current random seed of the current GPU.\n\n    .. warning::\n        This function eagerly initializes XPU.\n    ",
      "arguments": [],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the current random seed of the current GPU.\n\n    .. warning::\n        This function eagerly initializes XPU.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.is_available",
      "signature": "torch.xpu.is_available() -> bool",
      "doc": "Return a bool indicating if XPU is currently available.",
      "arguments": [],
      "return_type": "<class 'bool'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a bool indicating if XPU is currently available.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.is_bf16_supported",
      "signature": "torch.xpu.is_bf16_supported()",
      "doc": "Return a bool indicating if the current XPU device supports dtype bfloat16.",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a bool indicating if the current XPU device supports dtype bfloat16.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.is_initialized",
      "signature": "torch.xpu.is_initialized()",
      "doc": "Return whether PyTorch's XPU state has been initialized.",
      "arguments": [],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return whether PyTorch's XPU state has been initialized.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.lru_cache",
      "signature": "torch.xpu.lru_cache(maxsize=128, typed=False)",
      "doc": "Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    ",
      "arguments": [
        "maxsize",
        "typed"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.manual_seed",
      "signature": "torch.xpu.manual_seed(seed: int) -> None",
      "doc": "Set the seed for generating random numbers for the current GPU.\n\n    It's safe to call this function if XPU is not available; in that case, it is silently ignored.\n\n    Args:\n        seed (int): The desired seed.\n\n    .. warning::\n        If you are working with a multi-GPU model, this function is insufficient\n        to get determinism.  To seed all GPUs, use :func:`manual_seed_all`.\n    ",
      "arguments": [
        "seed"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the seed for generating random numbers for the current GPU.\n\n    It's safe to call this function if XPU is not available; in that case, it is silently ignored.\n\n    Args:\n        seed (int): The desired seed.\n\n    .. warning::\n        If you are working with a multi-GPU model, this function is insufficient\n        to get determinism.  To seed all GPUs, use :func:`manual_seed_all`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.manual_seed_all",
      "signature": "torch.xpu.manual_seed_all(seed: int) -> None",
      "doc": "Set the seed for generating random numbers on all GPUs.\n\n    It's safe to call this function if XPU is not available; in that case, it is silently ignored.\n\n    Args:\n        seed (int): The desired seed.\n    ",
      "arguments": [
        "seed"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the seed for generating random numbers on all GPUs.\n\n    It's safe to call this function if XPU is not available; in that case, it is silently ignored.\n\n    Args:\n        seed (int): The desired seed.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.max_memory_allocated",
      "signature": "torch.xpu.max_memory_allocated(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the maximum GPU memory occupied by tensors in bytes for a given device.\n\n    By default, this returns the peak allocated memory since the beginning of\n    this program. :func:`~torch.xpu.reset_peak_memory_stats` can be used to\n    reset the starting point in tracking this metric. For example, these two\n    functions can measure the peak allocated memory usage of each iteration in a\n    training loop.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the maximum GPU memory occupied by tensors in bytes for a given device.\n\n    By default, this returns the peak allocated memory since the beginning of\n    this program. :func:`~torch.xpu.reset_peak_memory_stats` can be used to\n    reset the starting point in tracking this metric. For example, these two\n    functions can measure the peak allocated memory usage of each iteration in a\n    training loop.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.max_memory_reserved",
      "signature": "torch.xpu.max_memory_reserved(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the maximum GPU memory managed by the caching allocator in bytes for a given device.\n\n    By default, this returns the peak cached memory since the beginning of this\n    program. :func:`~torch.xpu.reset_peak_memory_stats` can be used to reset\n    the starting point in tracking this metric. For example, these two functions\n    can measure the peak cached memory amount of each iteration in a training\n    loop.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the maximum GPU memory managed by the caching allocator in bytes for a given device.\n\n    By default, this returns the peak cached memory since the beginning of this\n    program. :func:`~torch.xpu.reset_peak_memory_stats` can be used to reset\n    the starting point in tracking this metric. For example, these two functions\n    can measure the peak cached memory amount of each iteration in a training\n    loop.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.mem_get_info",
      "signature": "torch.xpu.mem_get_info(device: Union[torch.device, str, int, NoneType] = None) -> tuple[int, int]",
      "doc": "Return the global free and total GPU memory for a given device.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Returns:\n        int: the memory available on the device in units of bytes.\n        int: the total memory on the device in units of bytes\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "tuple[int, int]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the global free and total GPU memory for a given device.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    Returns:\n        int: the memory available on the device in units of bytes.\n        int: the total memory on the device in units of bytes\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.memory_allocated",
      "signature": "torch.xpu.memory_allocated(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the current GPU memory occupied by tensors in bytes for a given device.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        This is likely less than the amount shown in `xpu-smi` since some\n        unused memory can be held by the caching allocator and some context\n        needs to be created on GPU.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the current GPU memory occupied by tensors in bytes for a given device.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n\n    .. note::\n        This is likely less than the amount shown in `xpu-smi` since some\n        unused memory can be held by the caching allocator and some context\n        needs to be created on GPU.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.memory_reserved",
      "signature": "torch.xpu.memory_reserved(device: Union[torch.device, str, int, NoneType] = None) -> int",
      "doc": "Return the current GPU memory managed by the caching allocator in bytes for a given device.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "<class 'int'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the current GPU memory managed by the caching allocator in bytes for a given device.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.memory_stats",
      "signature": "torch.xpu.memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> dict[str, typing.Any]",
      "doc": "Return a dictionary of XPU memory allocator statistics for a given device.\n\n    The return value of this function is a dictionary of statistics, each of\n    which is a non-negative integer.\n\n    Core statistics:\n\n    - ``\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of allocated memory.\n    - ``\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of reserved memory.\n    - ``\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of active memory.\n    - ``\"requested_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      memory requested by client code, compare this with allocated_bytes to check if\n      allocation rounding adds too much overhead.\n\n    For these core statistics, values are broken down as follows.\n\n    Pool type:\n\n    - ``all``: combined statistics across all memory pools.\n    - ``large_pool``: statistics for the large allocation pool (for size >= 1MB allocations).\n    - ``small_pool``: statistics for the small allocation pool (for size < 1MB allocations).\n\n    Metric type:\n\n    - ``current``: current value of this metric.\n    - ``peak``: maximum value of this metric.\n    - ``allocated``: historical total increase in this metric.\n    - ``freed``: historical total decrease in this metric.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistics for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return a dictionary of XPU memory allocator statistics for a given device.\n\n    The return value of this function is a dictionary of statistics, each of\n    which is a non-negative integer.\n\n    Core statistics:\n\n    - ``\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of allocated memory.\n    - ``\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of reserved memory.\n    - ``\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      amount of active memory.\n    - ``\"requested_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"``:\n      memory requested by client code, compare this with allocated_bytes to check if\n      allocation rounding adds too much overhead.\n\n    For these core statistics, values are broken down as follows.\n\n    Pool type:\n\n    - ``all``: combined statistics across all memory pools.\n    - ``large_pool``: statistics for the large allocation pool (for size >= 1MB allocations).\n    - ``small_pool``: statistics for the small allocation pool (for size < 1MB allocations).\n\n    Metric type:\n\n    - ``current``: current value of this metric.\n    - ``peak``: maximum value of this metric.\n    - ``allocated``: historical total increase in this metric.\n    - ``freed``: historical total decrease in this metric.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistics for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.memory_stats_as_nested_dict",
      "signature": "torch.xpu.memory_stats_as_nested_dict(device: Union[torch.device, str, int, NoneType] = None) -> dict[str, typing.Any]",
      "doc": "Return the result of :func:`~torch.xpu.memory_stats` as a nested dictionary.",
      "arguments": [
        "device"
      ],
      "return_type": "dict[str, typing.Any]",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Return the result of :func:`~torch.xpu.memory_stats` as a nested dictionary.",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.reset_accumulated_memory_stats",
      "signature": "torch.xpu.reset_accumulated_memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Reset the \"accumulated\" (historical) stats tracked by the XPU memory allocator.\n\n    See :func:`~torch.xpu.memory_stats` for details. Accumulated stats correspond to\n    the `\"allocated\"` and `\"freed\"` keys in each individual stat dict.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the \"accumulated\" (historical) stats tracked by the XPU memory allocator.\n\n    See :func:`~torch.xpu.memory_stats` for details. Accumulated stats correspond to\n    the `\"allocated\"` and `\"freed\"` keys in each individual stat dict.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.reset_peak_memory_stats",
      "signature": "torch.xpu.reset_peak_memory_stats(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Reset the \"peak\" stats tracked by the XPU memory allocator.\n\n    See :func:`~torch.xpu.memory_stats` for details. Peak stats correspond to the\n    `\"peak\"` key in each individual stat dict.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Reset the \"peak\" stats tracked by the XPU memory allocator.\n\n    See :func:`~torch.xpu.memory_stats` for details. Peak stats correspond to the\n    `\"peak\"` key in each individual stat dict.\n\n    Args:\n        device (torch.device or int or str, optional): selected device. Returns\n            statistic for the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.seed",
      "signature": "torch.xpu.seed() -> None",
      "doc": "Set the seed for generating random numbers to a random number for the current GPU.\n\n    It's safe to call this function if XPU is not available; in that case, it is silently ignored.\n\n    .. warning::\n        If you are working with a multi-GPU model, this function will only initialize\n        the seed on one GPU.  To initialize all GPUs, use :func:`seed_all`.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the seed for generating random numbers to a random number for the current GPU.\n\n    It's safe to call this function if XPU is not available; in that case, it is silently ignored.\n\n    .. warning::\n        If you are working with a multi-GPU model, this function will only initialize\n        the seed on one GPU.  To initialize all GPUs, use :func:`seed_all`.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.seed_all",
      "signature": "torch.xpu.seed_all() -> None",
      "doc": "Set the seed for generating random numbers to a random number on all GPUs.\n\n    It's safe to call this function if XPU is not available; in that case, it is silently ignored.\n    ",
      "arguments": [],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the seed for generating random numbers to a random number on all GPUs.\n\n    It's safe to call this function if XPU is not available; in that case, it is silently ignored.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.set_device",
      "signature": "torch.xpu.set_device(device: Union[torch.device, str, int, NoneType]) -> None",
      "doc": "Set the current device.\n\n    Args:\n        device (torch.device or int or str): selected device. This function is a\n            no-op if this argument is negative.\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current device.\n\n    Args:\n        device (torch.device or int or str): selected device. This function is a\n            no-op if this argument is negative.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.set_rng_state",
      "signature": "torch.xpu.set_rng_state(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'xpu') -> None",
      "doc": "Set the random number generator state of the specified GPU.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n        device (torch.device or int, optional): The device to set the RNG state.\n            Default: ``'xpu'`` (i.e., ``torch.device('xpu')``, the current XPU device).\n    ",
      "arguments": [
        "new_state",
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the random number generator state of the specified GPU.\n\n    Args:\n        new_state (torch.ByteTensor): The desired state\n        device (torch.device or int, optional): The device to set the RNG state.\n            Default: ``'xpu'`` (i.e., ``torch.device('xpu')``, the current XPU device).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.set_rng_state_all",
      "signature": "torch.xpu.set_rng_state_all(new_states: collections.abc.Iterable[torch.Tensor]) -> None",
      "doc": "Set the random number generator state of all devices.\n\n    Args:\n        new_states (Iterable of torch.ByteTensor): The desired state for each device.\n    ",
      "arguments": [
        "new_states"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the random number generator state of all devices.\n\n    Args:\n        new_states (Iterable of torch.ByteTensor): The desired state for each device.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.set_stream",
      "signature": "torch.xpu.set_stream(stream: torch.xpu.streams.Stream)",
      "doc": "Set the current stream.This is a wrapper API to set the stream.\n        Usage of this function is discouraged in favor of the ``stream``\n        context manager.\n\n    Args:\n        stream (Stream): selected stream. This function is a no-op\n            if this argument is ``None``.\n    ",
      "arguments": [
        "stream"
      ],
      "return_type": "Any",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Set the current stream.This is a wrapper API to set the stream.\n        Usage of this function is discouraged in favor of the ``stream``\n        context manager.\n\n    Args:\n        stream (Stream): selected stream. This function is a no-op\n            if this argument is ``None``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.stream",
      "signature": "torch.xpu.stream(stream: Optional[ForwardRef('torch.xpu.Stream')]) -> torch.xpu.StreamContext",
      "doc": "Wrap around the Context-manager StreamContext that selects a given stream.\n\n    Arguments:\n        stream (Stream): selected stream. This manager is a no-op if it's ``None``.\n    ",
      "arguments": [
        "stream"
      ],
      "return_type": "<class 'torch.xpu.StreamContext'>",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Wrap around the Context-manager StreamContext that selects a given stream.\n\n    Arguments:\n        stream (Stream): selected stream. This manager is a no-op if it's ``None``.\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    },
    {
      "function": "torch.xpu.synchronize",
      "signature": "torch.xpu.synchronize(device: Union[torch.device, str, int, NoneType] = None) -> None",
      "doc": "Wait for all kernels in all streams on a XPU device to complete.\n\n    Args:\n        device (torch.device or int, optional): device for which to synchronize.\n            It uses the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
      "arguments": [
        "device"
      ],
      "return_type": "None",
      "implementation_status": "not_started",
      "implementation_notes": null,
      "status_updated": null,
      "versions": [
        {
          "version": "unknown",
          "status": "current",
          "signature": "",
          "doc": "Wait for all kernels in all streams on a XPU device to complete.\n\n    Args:\n        device (torch.device or int, optional): device for which to synchronize.\n            It uses the current device, given by :func:`~torch.xpu.current_device`,\n            if :attr:`device` is ``None`` (default).\n    ",
          "device_support": null,
          "added_in": null,
          "deprecated_in": null,
          "changed_in": null,
          "added_date": "2025-07-10"
        }
      ],
      "current_version": "unknown",
      "last_updated": "2025-07-10",
      "device_support": null,
      "mlx_mapping": null,
      "deprecation_warnings": [],
      "added_date": "2025-07-10"
    }
  ]
}